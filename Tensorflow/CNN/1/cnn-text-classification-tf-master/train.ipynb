{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "WARNING:tensorflow:From <ipython-input-1-7eeaec4464e8>:54: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From C:\\professionalsoft\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\preprocessing\\text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From C:\\professionalsoft\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\preprocessing\\text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "Vocabulary Size: 18758\n",
      "Train/Dev split: 9596/1066\n",
      "WARNING:tensorflow:From C:\\professionalsoft\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\mao\\Desktop\\Tensorflow\\CNN\\1\\cnn-text-classification-tf-master\\text_cnn.py:62: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\mao\\Desktop\\Tensorflow\\CNN\\1\\cnn-text-classification-tf-master\\text_cnn.py:78: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "WARNING:tensorflow:From C:\\professionalsoft\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\tensorflow\\python\\ops\\array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "Writing to C:\\Users\\mao\\Desktop\\Tensorflow\\CNN\\1\\cnn-text-classification-tf-master\\runs\\1561796526\n",
      "\n",
      "2019-06-29T16:22:10.097809: step 1, loss 3.39312, acc 0.4375\n",
      "2019-06-29T16:22:10.379135: step 2, loss 2.18179, acc 0.53125\n",
      "2019-06-29T16:22:10.693831: step 3, loss 1.93244, acc 0.5\n",
      "2019-06-29T16:22:10.994430: step 4, loss 2.46774, acc 0.359375\n",
      "2019-06-29T16:22:11.305345: step 5, loss 2.11874, acc 0.5\n",
      "2019-06-29T16:22:11.588263: step 6, loss 2.39137, acc 0.515625\n",
      "2019-06-29T16:22:11.862451: step 7, loss 2.23308, acc 0.5\n",
      "2019-06-29T16:22:12.165874: step 8, loss 2.60928, acc 0.453125\n",
      "2019-06-29T16:22:12.468023: step 9, loss 1.93512, acc 0.453125\n",
      "2019-06-29T16:22:12.857229: step 10, loss 2.14329, acc 0.46875\n",
      "2019-06-29T16:22:13.137648: step 11, loss 1.78043, acc 0.5\n",
      "2019-06-29T16:22:13.412411: step 12, loss 2.23263, acc 0.484375\n",
      "2019-06-29T16:22:13.711117: step 13, loss 2.16316, acc 0.484375\n",
      "2019-06-29T16:22:14.008773: step 14, loss 2.31359, acc 0.46875\n",
      "2019-06-29T16:22:14.278685: step 15, loss 2.24318, acc 0.515625\n",
      "2019-06-29T16:22:14.561871: step 16, loss 1.85412, acc 0.578125\n",
      "2019-06-29T16:22:14.849888: step 17, loss 2.01103, acc 0.53125\n",
      "2019-06-29T16:22:15.136027: step 18, loss 1.78245, acc 0.484375\n",
      "2019-06-29T16:22:15.423787: step 19, loss 2.37187, acc 0.46875\n",
      "2019-06-29T16:22:15.726806: step 20, loss 1.84949, acc 0.453125\n",
      "2019-06-29T16:22:16.007703: step 21, loss 1.81347, acc 0.453125\n",
      "2019-06-29T16:22:16.294262: step 22, loss 1.65786, acc 0.5625\n",
      "2019-06-29T16:22:16.577089: step 23, loss 1.67186, acc 0.578125\n",
      "2019-06-29T16:22:16.856080: step 24, loss 2.10991, acc 0.4375\n",
      "2019-06-29T16:22:17.172305: step 25, loss 1.51587, acc 0.578125\n",
      "2019-06-29T16:22:17.461313: step 26, loss 1.46738, acc 0.578125\n",
      "2019-06-29T16:22:17.745398: step 27, loss 1.67845, acc 0.484375\n",
      "2019-06-29T16:22:18.029772: step 28, loss 2.27405, acc 0.4375\n",
      "2019-06-29T16:22:18.326357: step 29, loss 1.48809, acc 0.546875\n",
      "2019-06-29T16:22:18.611570: step 30, loss 1.81271, acc 0.4375\n",
      "2019-06-29T16:22:18.891519: step 31, loss 1.62537, acc 0.578125\n",
      "2019-06-29T16:22:19.163106: step 32, loss 1.42836, acc 0.625\n",
      "2019-06-29T16:22:19.442060: step 33, loss 1.72804, acc 0.515625\n",
      "2019-06-29T16:22:19.728705: step 34, loss 1.80965, acc 0.53125\n",
      "2019-06-29T16:22:19.994998: step 35, loss 1.98646, acc 0.5625\n",
      "2019-06-29T16:22:20.272638: step 36, loss 1.27006, acc 0.5625\n",
      "2019-06-29T16:22:20.673220: step 37, loss 1.72283, acc 0.421875\n",
      "2019-06-29T16:22:20.957471: step 38, loss 1.79395, acc 0.53125\n",
      "2019-06-29T16:22:21.238205: step 39, loss 1.54664, acc 0.625\n",
      "2019-06-29T16:22:21.511664: step 40, loss 1.50946, acc 0.53125\n",
      "2019-06-29T16:22:21.781214: step 41, loss 1.58082, acc 0.515625\n",
      "2019-06-29T16:22:22.054714: step 42, loss 1.92216, acc 0.484375\n",
      "2019-06-29T16:22:22.420975: step 43, loss 1.52608, acc 0.515625\n",
      "2019-06-29T16:22:22.776691: step 44, loss 1.60168, acc 0.59375\n",
      "2019-06-29T16:22:23.049116: step 45, loss 1.63302, acc 0.5\n",
      "2019-06-29T16:22:23.414377: step 46, loss 1.25683, acc 0.734375\n",
      "2019-06-29T16:22:23.706264: step 47, loss 1.49222, acc 0.515625\n",
      "2019-06-29T16:22:23.977226: step 48, loss 1.32703, acc 0.546875\n",
      "2019-06-29T16:22:24.334482: step 49, loss 1.58552, acc 0.5625\n",
      "2019-06-29T16:22:24.603217: step 50, loss 1.54309, acc 0.59375\n",
      "2019-06-29T16:22:24.871985: step 51, loss 1.70406, acc 0.53125\n",
      "2019-06-29T16:22:25.141508: step 52, loss 1.89221, acc 0.453125\n",
      "2019-06-29T16:22:25.411217: step 53, loss 1.65212, acc 0.546875\n",
      "2019-06-29T16:22:25.683994: step 54, loss 1.48888, acc 0.5625\n",
      "2019-06-29T16:22:25.970967: step 55, loss 1.26411, acc 0.5625\n",
      "2019-06-29T16:22:26.236719: step 56, loss 1.84937, acc 0.46875\n",
      "2019-06-29T16:22:26.510553: step 57, loss 1.81335, acc 0.40625\n",
      "2019-06-29T16:22:26.798574: step 58, loss 1.8905, acc 0.4375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-29T16:22:27.110684: step 59, loss 1.68814, acc 0.546875\n",
      "2019-06-29T16:22:27.418463: step 60, loss 1.52971, acc 0.546875\n",
      "2019-06-29T16:22:27.678130: step 61, loss 1.9034, acc 0.4375\n",
      "2019-06-29T16:22:27.945461: step 62, loss 1.7403, acc 0.453125\n",
      "2019-06-29T16:22:28.245964: step 63, loss 1.95955, acc 0.4375\n",
      "2019-06-29T16:22:28.527128: step 64, loss 1.56371, acc 0.546875\n",
      "2019-06-29T16:22:28.794866: step 65, loss 1.8479, acc 0.484375\n",
      "2019-06-29T16:22:29.078728: step 66, loss 1.82928, acc 0.484375\n",
      "2019-06-29T16:22:29.360861: step 67, loss 2.09852, acc 0.46875\n",
      "2019-06-29T16:22:29.633434: step 68, loss 1.52135, acc 0.640625\n",
      "2019-06-29T16:22:29.909255: step 69, loss 1.47703, acc 0.609375\n",
      "2019-06-29T16:22:30.208958: step 70, loss 1.3347, acc 0.578125\n",
      "2019-06-29T16:22:30.492395: step 71, loss 1.45748, acc 0.546875\n",
      "2019-06-29T16:22:30.762065: step 72, loss 1.51223, acc 0.5\n",
      "2019-06-29T16:22:31.044005: step 73, loss 1.9417, acc 0.421875\n",
      "2019-06-29T16:22:31.326762: step 74, loss 1.57385, acc 0.546875\n",
      "2019-06-29T16:22:31.609738: step 75, loss 1.73783, acc 0.484375\n",
      "2019-06-29T16:22:31.905555: step 76, loss 1.43682, acc 0.546875\n",
      "2019-06-29T16:22:32.177478: step 77, loss 1.45518, acc 0.5625\n",
      "2019-06-29T16:22:32.466143: step 78, loss 1.53594, acc 0.5625\n",
      "2019-06-29T16:22:32.750137: step 79, loss 1.58021, acc 0.53125\n",
      "2019-06-29T16:22:33.029776: step 80, loss 1.2284, acc 0.5625\n",
      "2019-06-29T16:22:33.311392: step 81, loss 1.21529, acc 0.625\n",
      "2019-06-29T16:22:33.610254: step 82, loss 1.51877, acc 0.578125\n",
      "2019-06-29T16:22:33.893060: step 83, loss 1.54227, acc 0.546875\n",
      "2019-06-29T16:22:34.198085: step 84, loss 1.63916, acc 0.4375\n",
      "2019-06-29T16:22:34.487291: step 85, loss 1.11802, acc 0.515625\n",
      "2019-06-29T16:22:34.762488: step 86, loss 1.22346, acc 0.59375\n",
      "2019-06-29T16:22:35.031177: step 87, loss 2.11322, acc 0.296875\n",
      "2019-06-29T16:22:35.301496: step 88, loss 1.59938, acc 0.484375\n",
      "2019-06-29T16:22:35.595707: step 89, loss 1.07911, acc 0.59375\n",
      "2019-06-29T16:22:35.863897: step 90, loss 1.46499, acc 0.484375\n",
      "2019-06-29T16:22:36.127868: step 91, loss 1.31108, acc 0.5\n",
      "2019-06-29T16:22:36.403499: step 92, loss 1.25557, acc 0.53125\n",
      "2019-06-29T16:22:36.669078: step 93, loss 1.4054, acc 0.484375\n",
      "2019-06-29T16:22:36.965091: step 94, loss 1.03545, acc 0.609375\n",
      "2019-06-29T16:22:37.234283: step 95, loss 1.34251, acc 0.546875\n",
      "2019-06-29T16:22:37.507702: step 96, loss 1.52198, acc 0.546875\n",
      "2019-06-29T16:22:37.780895: step 97, loss 1.08258, acc 0.625\n",
      "2019-06-29T16:22:38.064097: step 98, loss 1.35916, acc 0.5625\n",
      "2019-06-29T16:22:38.326435: step 99, loss 1.53417, acc 0.484375\n",
      "2019-06-29T16:22:38.594555: step 100, loss 1.65233, acc 0.40625\n",
      "\n",
      "Evaluation:\n",
      "2019-06-29T16:22:39.350688: step 100, loss 0.856834, acc 0.551595\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\mao\\Desktop\\Tensorflow\\CNN\\1\\cnn-text-classification-tf-master\\runs\\1561796526\\checkpoints\\model-100\n",
      "\n",
      "2019-06-29T16:22:40.552035: step 101, loss 1.23343, acc 0.609375\n",
      "2019-06-29T16:22:41.021237: step 102, loss 1.16389, acc 0.640625\n",
      "2019-06-29T16:22:41.308403: step 103, loss 1.01042, acc 0.671875\n",
      "2019-06-29T16:22:41.560288: step 104, loss 1.16715, acc 0.53125\n",
      "2019-06-29T16:22:41.844796: step 105, loss 1.68943, acc 0.453125\n",
      "2019-06-29T16:22:42.117208: step 106, loss 1.32958, acc 0.453125\n",
      "2019-06-29T16:22:42.445392: step 107, loss 1.4782, acc 0.5\n",
      "2019-06-29T16:22:42.919317: step 108, loss 1.34751, acc 0.53125\n",
      "2019-06-29T16:22:43.319602: step 109, loss 1.34396, acc 0.5\n",
      "2019-06-29T16:22:43.720945: step 110, loss 1.59789, acc 0.5\n",
      "2019-06-29T16:22:44.040671: step 111, loss 1.21052, acc 0.5625\n",
      "2019-06-29T16:22:44.346254: step 112, loss 1.2872, acc 0.53125\n",
      "2019-06-29T16:22:44.756176: step 113, loss 0.837005, acc 0.609375\n",
      "2019-06-29T16:22:45.071532: step 114, loss 1.2134, acc 0.59375\n",
      "2019-06-29T16:22:45.385460: step 115, loss 1.76706, acc 0.5\n",
      "2019-06-29T16:22:45.720910: step 116, loss 1.47085, acc 0.46875\n",
      "2019-06-29T16:22:46.071159: step 117, loss 1.70913, acc 0.53125\n",
      "2019-06-29T16:22:46.368369: step 118, loss 1.07037, acc 0.609375\n",
      "2019-06-29T16:22:46.739635: step 119, loss 1.24814, acc 0.5625\n",
      "2019-06-29T16:22:47.085439: step 120, loss 1.29069, acc 0.515625\n",
      "2019-06-29T16:22:47.440677: step 121, loss 1.33184, acc 0.484375\n",
      "2019-06-29T16:22:47.743171: step 122, loss 1.72934, acc 0.375\n",
      "2019-06-29T16:22:48.067402: step 123, loss 1.38584, acc 0.546875\n",
      "2019-06-29T16:22:48.343360: step 124, loss 1.3992, acc 0.5\n",
      "2019-06-29T16:22:48.634438: step 125, loss 1.35289, acc 0.5\n",
      "2019-06-29T16:22:49.008705: step 126, loss 1.46039, acc 0.515625\n",
      "2019-06-29T16:22:49.293809: step 127, loss 1.15998, acc 0.5\n",
      "2019-06-29T16:22:49.578270: step 128, loss 1.42992, acc 0.578125\n",
      "2019-06-29T16:22:49.880110: step 129, loss 1.35604, acc 0.53125\n",
      "2019-06-29T16:22:50.214235: step 130, loss 1.52946, acc 0.59375\n",
      "2019-06-29T16:22:50.511114: step 131, loss 1.49266, acc 0.59375\n",
      "2019-06-29T16:22:50.870495: step 132, loss 1.63279, acc 0.5\n",
      "2019-06-29T16:22:51.226309: step 133, loss 1.03611, acc 0.609375\n",
      "2019-06-29T16:22:51.523189: step 134, loss 1.55623, acc 0.484375\n",
      "2019-06-29T16:22:51.804442: step 135, loss 1.18365, acc 0.59375\n",
      "2019-06-29T16:22:52.069939: step 136, loss 0.960962, acc 0.625\n",
      "2019-06-29T16:22:52.335568: step 137, loss 1.17643, acc 0.5625\n",
      "2019-06-29T16:22:52.601196: step 138, loss 1.26955, acc 0.484375\n",
      "2019-06-29T16:22:52.854623: step 139, loss 1.24892, acc 0.5625\n",
      "2019-06-29T16:22:53.127625: step 140, loss 1.38224, acc 0.59375\n",
      "2019-06-29T16:22:53.393254: step 141, loss 1.16975, acc 0.546875\n",
      "2019-06-29T16:22:53.658882: step 142, loss 1.30353, acc 0.484375\n",
      "2019-06-29T16:22:53.929811: step 143, loss 1.16532, acc 0.609375\n",
      "2019-06-29T16:22:54.179815: step 144, loss 1.01592, acc 0.5625\n",
      "2019-06-29T16:22:54.461069: step 145, loss 1.13174, acc 0.515625\n",
      "2019-06-29T16:22:54.742324: step 146, loss 1.06672, acc 0.65625\n",
      "2019-06-29T16:22:55.171847: step 147, loss 1.42002, acc 0.4375\n",
      "2019-06-29T16:22:55.453101: step 148, loss 1.49555, acc 0.578125\n",
      "2019-06-29T16:22:55.718728: step 149, loss 1.33933, acc 0.46875\n",
      "2019-06-29T16:22:55.974422: step 150, loss 1.14255, acc 0.583333\n",
      "2019-06-29T16:22:56.271303: step 151, loss 0.932525, acc 0.578125\n",
      "2019-06-29T16:22:56.536931: step 152, loss 0.769313, acc 0.65625\n",
      "2019-06-29T16:22:56.802560: step 153, loss 0.836272, acc 0.609375\n",
      "2019-06-29T16:22:57.073504: step 154, loss 0.979788, acc 0.59375\n",
      "2019-06-29T16:22:57.323508: step 155, loss 1.08275, acc 0.546875\n",
      "2019-06-29T16:22:57.604764: step 156, loss 1.01139, acc 0.59375\n",
      "2019-06-29T16:22:57.907239: step 157, loss 1.35758, acc 0.5625\n",
      "2019-06-29T16:22:58.172867: step 158, loss 0.899543, acc 0.59375\n",
      "2019-06-29T16:22:58.453405: step 159, loss 0.793383, acc 0.65625\n",
      "2019-06-29T16:22:58.750284: step 160, loss 0.836465, acc 0.609375\n",
      "2019-06-29T16:22:59.089853: step 161, loss 1.04307, acc 0.515625\n",
      "2019-06-29T16:22:59.402358: step 162, loss 1.0577, acc 0.609375\n",
      "2019-06-29T16:22:59.676752: step 163, loss 1.19632, acc 0.546875\n",
      "2019-06-29T16:23:00.042831: step 164, loss 1.08148, acc 0.640625\n",
      "2019-06-29T16:23:00.312500: step 165, loss 0.831577, acc 0.609375\n",
      "2019-06-29T16:23:00.593754: step 166, loss 0.846186, acc 0.65625\n",
      "2019-06-29T16:23:00.880541: step 167, loss 0.876928, acc 0.59375\n",
      "2019-06-29T16:23:01.161793: step 168, loss 1.10987, acc 0.53125\n",
      "2019-06-29T16:23:01.443046: step 169, loss 0.993726, acc 0.609375\n",
      "2019-06-29T16:23:01.708676: step 170, loss 0.958298, acc 0.609375\n",
      "2019-06-29T16:23:01.979778: step 171, loss 1.07827, acc 0.546875\n",
      "2019-06-29T16:23:02.276657: step 172, loss 0.821143, acc 0.625\n",
      "2019-06-29T16:23:02.604787: step 173, loss 0.62546, acc 0.6875\n",
      "2019-06-29T16:23:02.972957: step 174, loss 0.81779, acc 0.703125\n",
      "2019-06-29T16:23:03.427532: step 175, loss 0.90596, acc 0.625\n",
      "2019-06-29T16:23:03.771286: step 176, loss 0.752999, acc 0.640625\n",
      "2019-06-29T16:23:04.042593: step 177, loss 0.915824, acc 0.578125\n",
      "2019-06-29T16:23:04.339472: step 178, loss 0.905491, acc 0.5625\n",
      "2019-06-29T16:23:04.620728: step 179, loss 0.968031, acc 0.578125\n",
      "2019-06-29T16:23:04.932204: step 180, loss 1.00686, acc 0.578125\n",
      "2019-06-29T16:23:05.260334: step 181, loss 0.959208, acc 0.546875\n",
      "2019-06-29T16:23:05.588463: step 182, loss 1.04818, acc 0.578125\n",
      "2019-06-29T16:23:05.914369: step 183, loss 0.958067, acc 0.59375\n",
      "2019-06-29T16:23:06.356697: step 184, loss 0.887846, acc 0.609375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-29T16:23:06.841079: step 185, loss 0.967461, acc 0.578125\n",
      "2019-06-29T16:23:07.128346: step 186, loss 0.892052, acc 0.625\n",
      "2019-06-29T16:23:07.425225: step 187, loss 1.14942, acc 0.53125\n",
      "2019-06-29T16:23:07.706480: step 188, loss 1.08552, acc 0.5\n",
      "2019-06-29T16:23:07.962037: step 189, loss 0.945184, acc 0.5625\n",
      "2019-06-29T16:23:08.243291: step 190, loss 0.889716, acc 0.625\n",
      "2019-06-29T16:23:08.508920: step 191, loss 0.944595, acc 0.609375\n",
      "2019-06-29T16:23:08.774549: step 192, loss 1.23426, acc 0.609375\n",
      "2019-06-29T16:23:09.045307: step 193, loss 0.858688, acc 0.671875\n",
      "2019-06-29T16:23:09.310936: step 194, loss 0.898139, acc 0.625\n",
      "2019-06-29T16:23:09.576565: step 195, loss 0.797731, acc 0.71875\n",
      "2019-06-29T16:23:09.842195: step 196, loss 0.761984, acc 0.703125\n",
      "2019-06-29T16:23:10.140257: step 197, loss 1.26495, acc 0.515625\n",
      "2019-06-29T16:23:10.421510: step 198, loss 0.851109, acc 0.71875\n",
      "2019-06-29T16:23:10.687138: step 199, loss 0.951732, acc 0.546875\n",
      "2019-06-29T16:23:10.958281: step 200, loss 0.871607, acc 0.625\n",
      "\n",
      "Evaluation:\n",
      "2019-06-29T16:23:11.598915: step 200, loss 0.691203, acc 0.603189\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\mao\\Desktop\\Tensorflow\\CNN\\1\\cnn-text-classification-tf-master\\runs\\1561796526\\checkpoints\\model-200\n",
      "\n",
      "2019-06-29T16:23:13.104000: step 201, loss 0.945349, acc 0.546875\n",
      "2019-06-29T16:23:13.429739: step 202, loss 0.862768, acc 0.5625\n",
      "2019-06-29T16:23:13.679742: step 203, loss 1.13862, acc 0.515625\n",
      "2019-06-29T16:23:13.950741: step 204, loss 0.784498, acc 0.609375\n",
      "2019-06-29T16:23:14.200744: step 205, loss 1.03375, acc 0.625\n",
      "2019-06-29T16:23:14.481999: step 206, loss 0.714424, acc 0.671875\n",
      "2019-06-29T16:23:14.763253: step 207, loss 0.88454, acc 0.59375\n",
      "2019-06-29T16:23:15.051677: step 208, loss 0.83129, acc 0.640625\n",
      "2019-06-29T16:23:15.407028: step 209, loss 0.850086, acc 0.59375\n",
      "2019-06-29T16:23:15.676504: step 210, loss 0.710526, acc 0.6875\n",
      "2019-06-29T16:23:15.963388: step 211, loss 0.811212, acc 0.640625\n",
      "2019-06-29T16:23:16.240319: step 212, loss 0.873304, acc 0.625\n",
      "2019-06-29T16:23:16.537198: step 213, loss 0.90144, acc 0.625\n",
      "2019-06-29T16:23:16.818452: step 214, loss 0.647095, acc 0.703125\n",
      "2019-06-29T16:23:17.105089: step 215, loss 0.869296, acc 0.65625\n",
      "2019-06-29T16:23:17.370717: step 216, loss 0.579775, acc 0.71875\n",
      "2019-06-29T16:23:17.651971: step 217, loss 0.837361, acc 0.5625\n",
      "2019-06-29T16:23:17.938501: step 218, loss 0.788807, acc 0.59375\n",
      "2019-06-29T16:23:18.219757: step 219, loss 1.01295, acc 0.578125\n",
      "2019-06-29T16:23:18.485385: step 220, loss 0.575695, acc 0.765625\n",
      "2019-06-29T16:23:18.766639: step 221, loss 0.611462, acc 0.703125\n",
      "2019-06-29T16:23:19.053880: step 222, loss 0.867007, acc 0.59375\n",
      "2019-06-29T16:23:19.335134: step 223, loss 0.865386, acc 0.625\n",
      "2019-06-29T16:23:19.663264: step 224, loss 1.02668, acc 0.640625\n",
      "2019-06-29T16:23:20.125588: step 225, loss 0.977513, acc 0.578125\n",
      "2019-06-29T16:23:20.406843: step 226, loss 0.814142, acc 0.609375\n",
      "2019-06-29T16:23:20.688096: step 227, loss 0.803224, acc 0.6875\n",
      "2019-06-29T16:23:20.960324: step 228, loss 0.752877, acc 0.59375\n",
      "2019-06-29T16:23:21.237469: step 229, loss 0.879095, acc 0.609375\n",
      "2019-06-29T16:23:21.565599: step 230, loss 0.7749, acc 0.640625\n",
      "2019-06-29T16:23:21.846853: step 231, loss 0.979264, acc 0.609375\n",
      "2019-06-29T16:23:22.150820: step 232, loss 0.811812, acc 0.546875\n",
      "2019-06-29T16:23:22.432073: step 233, loss 0.952302, acc 0.53125\n",
      "2019-06-29T16:23:22.713328: step 234, loss 0.902089, acc 0.5625\n",
      "2019-06-29T16:23:22.984240: step 235, loss 0.930085, acc 0.59375\n",
      "2019-06-29T16:23:23.265495: step 236, loss 0.880768, acc 0.625\n",
      "2019-06-29T16:23:23.531123: step 237, loss 0.968371, acc 0.609375\n",
      "2019-06-29T16:23:23.796752: step 238, loss 0.832278, acc 0.609375\n",
      "2019-06-29T16:23:24.052030: step 239, loss 0.768425, acc 0.65625\n",
      "2019-06-29T16:23:24.333284: step 240, loss 0.814814, acc 0.609375\n",
      "2019-06-29T16:23:24.598913: step 241, loss 0.918377, acc 0.59375\n",
      "2019-06-29T16:23:24.869936: step 242, loss 0.78353, acc 0.671875\n",
      "2019-06-29T16:23:25.135565: step 243, loss 0.959749, acc 0.515625\n",
      "2019-06-29T16:23:25.416819: step 244, loss 1.07497, acc 0.5625\n",
      "2019-06-29T16:23:25.666822: step 245, loss 0.916266, acc 0.578125\n",
      "2019-06-29T16:23:25.937904: step 246, loss 0.836293, acc 0.609375\n",
      "2019-06-29T16:23:26.203534: step 247, loss 0.660494, acc 0.640625\n",
      "2019-06-29T16:23:26.484787: step 248, loss 0.857098, acc 0.578125\n",
      "2019-06-29T16:23:26.750416: step 249, loss 0.76163, acc 0.578125\n",
      "2019-06-29T16:23:27.022164: step 250, loss 0.892857, acc 0.546875\n",
      "2019-06-29T16:23:27.287792: step 251, loss 0.927035, acc 0.53125\n",
      "2019-06-29T16:23:27.553420: step 252, loss 0.800638, acc 0.640625\n",
      "2019-06-29T16:23:27.834674: step 253, loss 0.666495, acc 0.6875\n",
      "2019-06-29T16:23:28.105695: step 254, loss 0.994454, acc 0.5\n",
      "2019-06-29T16:23:28.371323: step 255, loss 0.917256, acc 0.53125\n",
      "2019-06-29T16:23:28.636952: step 256, loss 0.869445, acc 0.546875\n",
      "2019-06-29T16:23:28.893751: step 257, loss 0.749487, acc 0.625\n",
      "2019-06-29T16:23:29.159380: step 258, loss 0.722604, acc 0.640625\n",
      "2019-06-29T16:23:29.425008: step 259, loss 0.753215, acc 0.625\n",
      "2019-06-29T16:23:29.710124: step 260, loss 0.710444, acc 0.671875\n",
      "2019-06-29T16:23:29.965444: step 261, loss 0.664247, acc 0.671875\n",
      "2019-06-29T16:23:30.231073: step 262, loss 0.858313, acc 0.546875\n",
      "2019-06-29T16:23:30.512328: step 263, loss 0.858793, acc 0.6875\n",
      "2019-06-29T16:23:30.809206: step 264, loss 0.804785, acc 0.65625\n",
      "2019-06-29T16:23:31.118786: step 265, loss 0.846648, acc 0.609375\n",
      "2019-06-29T16:23:31.422560: step 266, loss 0.794602, acc 0.5625\n",
      "2019-06-29T16:23:31.675995: step 267, loss 0.783287, acc 0.59375\n",
      "2019-06-29T16:23:31.995896: step 268, loss 0.692322, acc 0.640625\n",
      "2019-06-29T16:23:32.277152: step 269, loss 0.715587, acc 0.640625\n",
      "2019-06-29T16:23:32.574030: step 270, loss 0.655452, acc 0.6875\n",
      "2019-06-29T16:23:32.855284: step 271, loss 0.795633, acc 0.640625\n",
      "2019-06-29T16:23:33.126297: step 272, loss 0.668126, acc 0.6875\n",
      "2019-06-29T16:23:33.407552: step 273, loss 0.714992, acc 0.65625\n",
      "2019-06-29T16:23:33.688805: step 274, loss 0.846159, acc 0.625\n",
      "2019-06-29T16:23:33.960053: step 275, loss 0.665503, acc 0.65625\n",
      "2019-06-29T16:23:34.252166: step 276, loss 0.947586, acc 0.546875\n",
      "2019-06-29T16:23:34.517796: step 277, loss 0.789025, acc 0.59375\n",
      "2019-06-29T16:23:34.799050: step 278, loss 0.689327, acc 0.703125\n",
      "2019-06-29T16:23:35.070355: step 279, loss 0.763046, acc 0.625\n",
      "2019-06-29T16:23:35.382858: step 280, loss 0.749754, acc 0.6875\n",
      "2019-06-29T16:23:35.664112: step 281, loss 0.823604, acc 0.59375\n",
      "2019-06-29T16:23:35.950918: step 282, loss 0.819713, acc 0.578125\n",
      "2019-06-29T16:23:36.232172: step 283, loss 0.75282, acc 0.59375\n",
      "2019-06-29T16:23:36.529051: step 284, loss 1.00066, acc 0.53125\n",
      "2019-06-29T16:23:36.810305: step 285, loss 0.649607, acc 0.6875\n",
      "2019-06-29T16:23:37.097294: step 286, loss 0.850372, acc 0.578125\n",
      "2019-06-29T16:23:37.378549: step 287, loss 0.825649, acc 0.5625\n",
      "2019-06-29T16:23:37.659801: step 288, loss 0.779857, acc 0.59375\n",
      "2019-06-29T16:23:37.946504: step 289, loss 0.792419, acc 0.546875\n",
      "2019-06-29T16:23:38.227758: step 290, loss 0.740255, acc 0.640625\n",
      "2019-06-29T16:23:38.493389: step 291, loss 0.830799, acc 0.5625\n",
      "2019-06-29T16:23:38.759015: step 292, loss 0.719062, acc 0.703125\n",
      "2019-06-29T16:23:39.014492: step 293, loss 0.732627, acc 0.578125\n",
      "2019-06-29T16:23:39.280120: step 294, loss 0.714338, acc 0.671875\n",
      "2019-06-29T16:23:39.545749: step 295, loss 0.819928, acc 0.578125\n",
      "2019-06-29T16:23:39.811379: step 296, loss 0.896018, acc 0.515625\n",
      "2019-06-29T16:23:40.082441: step 297, loss 0.838079, acc 0.59375\n",
      "2019-06-29T16:23:40.348070: step 298, loss 0.715158, acc 0.6875\n",
      "2019-06-29T16:23:40.613699: step 299, loss 0.922766, acc 0.5625\n",
      "2019-06-29T16:23:40.884831: step 300, loss 0.865177, acc 0.516667\n",
      "\n",
      "Evaluation:\n",
      "2019-06-29T16:23:41.494216: step 300, loss 0.691305, acc 0.599437\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\mao\\Desktop\\Tensorflow\\CNN\\1\\cnn-text-classification-tf-master\\runs\\1561796526\\checkpoints\\model-300\n",
      "\n",
      "2019-06-29T16:23:42.451105: step 301, loss 0.725625, acc 0.671875\n",
      "2019-06-29T16:23:42.844435: step 302, loss 0.619709, acc 0.703125\n",
      "2019-06-29T16:23:43.194427: step 303, loss 0.685708, acc 0.65625\n",
      "2019-06-29T16:23:43.460055: step 304, loss 0.528243, acc 0.734375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-29T16:23:43.710058: step 305, loss 0.950459, acc 0.53125\n",
      "2019-06-29T16:23:43.981537: step 306, loss 0.846425, acc 0.625\n",
      "2019-06-29T16:23:44.257856: step 307, loss 0.662081, acc 0.640625\n",
      "2019-06-29T16:23:44.523484: step 308, loss 0.642837, acc 0.640625\n",
      "2019-06-29T16:23:44.773488: step 309, loss 0.752559, acc 0.6875\n",
      "2019-06-29T16:23:45.044447: step 310, loss 0.728639, acc 0.625\n",
      "2019-06-29T16:23:45.341327: step 311, loss 0.761337, acc 0.609375\n",
      "2019-06-29T16:23:45.614473: step 312, loss 0.532364, acc 0.8125\n",
      "2019-06-29T16:23:45.871420: step 313, loss 0.857393, acc 0.578125\n",
      "2019-06-29T16:23:46.137050: step 314, loss 0.589154, acc 0.6875\n",
      "2019-06-29T16:23:46.418306: step 315, loss 0.722751, acc 0.625\n",
      "2019-06-29T16:23:46.715184: step 316, loss 0.631547, acc 0.65625\n",
      "2019-06-29T16:23:47.019067: step 317, loss 0.424247, acc 0.78125\n",
      "2019-06-29T16:23:47.300320: step 318, loss 0.745386, acc 0.609375\n",
      "2019-06-29T16:23:47.605159: step 319, loss 0.741354, acc 0.546875\n",
      "2019-06-29T16:23:47.908827: step 320, loss 0.717168, acc 0.65625\n",
      "2019-06-29T16:23:48.236956: step 321, loss 0.63199, acc 0.6875\n",
      "2019-06-29T16:23:48.705712: step 322, loss 0.602069, acc 0.734375\n",
      "2019-06-29T16:23:49.454331: step 323, loss 0.655279, acc 0.625\n",
      "2019-06-29T16:23:50.035300: step 324, loss 0.721194, acc 0.625\n",
      "2019-06-29T16:23:51.053850: step 325, loss 0.738196, acc 0.671875\n",
      "2019-06-29T16:23:51.403099: step 326, loss 0.538322, acc 0.796875\n",
      "2019-06-29T16:23:51.714321: step 327, loss 0.620874, acc 0.71875\n",
      "2019-06-29T16:23:52.154633: step 328, loss 0.707248, acc 0.671875\n",
      "2019-06-29T16:23:52.497878: step 329, loss 0.657686, acc 0.671875\n",
      "2019-06-29T16:23:52.964211: step 330, loss 0.626655, acc 0.703125\n",
      "2019-06-29T16:23:53.314203: step 331, loss 0.653877, acc 0.703125\n",
      "2019-06-29T16:23:53.595458: step 332, loss 0.712954, acc 0.671875\n",
      "2019-06-29T16:23:53.902524: step 333, loss 0.589774, acc 0.71875\n",
      "2019-06-29T16:23:54.267218: step 334, loss 0.562535, acc 0.671875\n",
      "2019-06-29T16:23:54.548500: step 335, loss 0.767935, acc 0.625\n",
      "2019-06-29T16:23:54.846185: step 336, loss 0.689263, acc 0.65625\n",
      "2019-06-29T16:23:55.421318: step 337, loss 0.604457, acc 0.78125\n",
      "2019-06-29T16:23:55.776570: step 338, loss 0.651444, acc 0.65625\n",
      "2019-06-29T16:23:56.077785: step 339, loss 0.549328, acc 0.765625\n",
      "2019-06-29T16:23:56.388006: step 340, loss 0.544718, acc 0.78125\n",
      "2019-06-29T16:23:56.692222: step 341, loss 0.725891, acc 0.640625\n",
      "2019-06-29T16:23:56.986432: step 342, loss 0.773324, acc 0.65625\n",
      "2019-06-29T16:23:57.316667: step 343, loss 0.748561, acc 0.65625\n",
      "2019-06-29T16:23:57.624886: step 344, loss 0.624491, acc 0.65625\n",
      "2019-06-29T16:23:57.966130: step 345, loss 0.613951, acc 0.734375\n",
      "2019-06-29T16:23:58.307372: step 346, loss 0.701381, acc 0.71875\n",
      "2019-06-29T16:23:58.621596: step 347, loss 0.703351, acc 0.671875\n",
      "2019-06-29T16:23:58.943825: step 348, loss 0.606064, acc 0.71875\n",
      "2019-06-29T16:23:59.276063: step 349, loss 0.479634, acc 0.75\n",
      "2019-06-29T16:23:59.611300: step 350, loss 0.623476, acc 0.703125\n",
      "2019-06-29T16:23:59.955546: step 351, loss 0.658796, acc 0.703125\n",
      "2019-06-29T16:24:00.301827: step 352, loss 0.548297, acc 0.75\n",
      "2019-06-29T16:24:00.650071: step 353, loss 0.559252, acc 0.671875\n",
      "2019-06-29T16:24:00.948284: step 354, loss 0.723826, acc 0.65625\n",
      "2019-06-29T16:24:01.261508: step 355, loss 0.678317, acc 0.609375\n",
      "2019-06-29T16:24:01.575731: step 356, loss 0.715496, acc 0.65625\n",
      "2019-06-29T16:24:01.942993: step 357, loss 0.69341, acc 0.703125\n",
      "2019-06-29T16:24:02.242484: step 358, loss 0.792942, acc 0.625\n",
      "2019-06-29T16:24:02.525197: step 359, loss 0.725103, acc 0.625\n",
      "2019-06-29T16:24:02.849428: step 360, loss 0.770232, acc 0.6875\n",
      "2019-06-29T16:24:03.188671: step 361, loss 0.664971, acc 0.671875\n",
      "2019-06-29T16:24:03.560935: step 362, loss 0.771403, acc 0.65625\n",
      "2019-06-29T16:24:03.859071: step 363, loss 0.748902, acc 0.65625\n",
      "2019-06-29T16:24:04.260356: step 364, loss 0.663013, acc 0.671875\n",
      "2019-06-29T16:24:04.558568: step 365, loss 0.505643, acc 0.734375\n",
      "2019-06-29T16:24:04.953851: step 366, loss 0.609628, acc 0.71875\n",
      "2019-06-29T16:24:05.399167: step 367, loss 0.615177, acc 0.703125\n",
      "2019-06-29T16:24:05.725398: step 368, loss 0.561122, acc 0.734375\n",
      "2019-06-29T16:24:06.027613: step 369, loss 0.532886, acc 0.75\n",
      "2019-06-29T16:24:06.338836: step 370, loss 0.602875, acc 0.671875\n",
      "2019-06-29T16:24:06.666068: step 371, loss 0.634418, acc 0.734375\n",
      "2019-06-29T16:24:06.964280: step 372, loss 0.609349, acc 0.6875\n",
      "2019-06-29T16:24:07.275503: step 373, loss 0.765298, acc 0.578125\n",
      "2019-06-29T16:24:07.588726: step 374, loss 0.604345, acc 0.75\n",
      "2019-06-29T16:24:07.907954: step 375, loss 0.563202, acc 0.703125\n",
      "2019-06-29T16:24:08.225178: step 376, loss 0.660585, acc 0.6875\n",
      "2019-06-29T16:24:08.546407: step 377, loss 0.556131, acc 0.734375\n",
      "2019-06-29T16:24:08.896656: step 378, loss 0.773592, acc 0.640625\n",
      "2019-06-29T16:24:09.204876: step 379, loss 0.661231, acc 0.71875\n",
      "2019-06-29T16:24:09.503088: step 380, loss 0.622377, acc 0.734375\n",
      "2019-06-29T16:24:09.819314: step 381, loss 0.757371, acc 0.609375\n",
      "2019-06-29T16:24:10.176568: step 382, loss 0.677874, acc 0.6875\n",
      "2019-06-29T16:24:10.482785: step 383, loss 0.624158, acc 0.734375\n",
      "2019-06-29T16:24:10.801012: step 384, loss 0.815076, acc 0.59375\n",
      "2019-06-29T16:24:11.146258: step 385, loss 0.697781, acc 0.65625\n",
      "2019-06-29T16:24:11.453476: step 386, loss 0.773694, acc 0.640625\n",
      "2019-06-29T16:24:11.740682: step 387, loss 0.535452, acc 0.640625\n",
      "2019-06-29T16:24:12.054906: step 388, loss 0.822244, acc 0.546875\n",
      "2019-06-29T16:24:12.364126: step 389, loss 0.717054, acc 0.65625\n",
      "2019-06-29T16:24:12.662337: step 390, loss 0.777347, acc 0.625\n",
      "2019-06-29T16:24:13.003582: step 391, loss 0.685118, acc 0.640625\n",
      "2019-06-29T16:24:13.334816: step 392, loss 0.71924, acc 0.6875\n",
      "2019-06-29T16:24:13.632311: step 393, loss 0.60555, acc 0.703125\n",
      "2019-06-29T16:24:13.918515: step 394, loss 0.635149, acc 0.734375\n",
      "2019-06-29T16:24:14.204824: step 395, loss 0.66904, acc 0.65625\n",
      "2019-06-29T16:24:14.495387: step 396, loss 0.52315, acc 0.6875\n",
      "2019-06-29T16:24:14.766577: step 397, loss 0.636981, acc 0.671875\n",
      "2019-06-29T16:24:15.113478: step 398, loss 0.550267, acc 0.671875\n",
      "2019-06-29T16:24:15.438709: step 399, loss 0.571985, acc 0.703125\n",
      "2019-06-29T16:24:15.745926: step 400, loss 0.392316, acc 0.859375\n",
      "\n",
      "Evaluation:\n",
      "2019-06-29T16:24:16.471446: step 400, loss 0.685745, acc 0.575985\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\mao\\Desktop\\Tensorflow\\CNN\\1\\cnn-text-classification-tf-master\\runs\\1561796526\\checkpoints\\model-400\n",
      "\n",
      "2019-06-29T16:24:17.568261: step 401, loss 0.735752, acc 0.640625\n",
      "2019-06-29T16:24:18.010576: step 402, loss 0.842196, acc 0.609375\n",
      "2019-06-29T16:24:18.281474: step 403, loss 0.678863, acc 0.65625\n",
      "2019-06-29T16:24:18.558672: step 404, loss 0.553483, acc 0.78125\n",
      "2019-06-29T16:24:18.868893: step 405, loss 0.68304, acc 0.6875\n",
      "2019-06-29T16:24:19.161102: step 406, loss 0.631968, acc 0.71875\n",
      "2019-06-29T16:24:19.458100: step 407, loss 0.851641, acc 0.578125\n",
      "2019-06-29T16:24:19.740646: step 408, loss 0.748092, acc 0.671875\n",
      "2019-06-29T16:24:20.018844: step 409, loss 0.484027, acc 0.75\n",
      "2019-06-29T16:24:20.316055: step 410, loss 0.58063, acc 0.671875\n",
      "2019-06-29T16:24:20.592251: step 411, loss 0.715362, acc 0.671875\n",
      "2019-06-29T16:24:20.906475: step 412, loss 0.649774, acc 0.65625\n",
      "2019-06-29T16:24:21.200685: step 413, loss 0.619476, acc 0.71875\n",
      "2019-06-29T16:24:21.476881: step 414, loss 0.72463, acc 0.65625\n",
      "2019-06-29T16:24:21.756080: step 415, loss 0.664907, acc 0.6875\n",
      "2019-06-29T16:24:22.044285: step 416, loss 0.764714, acc 0.578125\n",
      "2019-06-29T16:24:22.350503: step 417, loss 0.650778, acc 0.578125\n",
      "2019-06-29T16:24:22.631704: step 418, loss 0.549193, acc 0.734375\n",
      "2019-06-29T16:24:22.906900: step 419, loss 0.742573, acc 0.5625\n",
      "2019-06-29T16:24:23.223125: step 420, loss 0.743146, acc 0.671875\n",
      "2019-06-29T16:24:23.512330: step 421, loss 0.757036, acc 0.640625\n",
      "2019-06-29T16:24:23.800536: step 422, loss 0.677099, acc 0.640625\n",
      "2019-06-29T16:24:24.073730: step 423, loss 0.654581, acc 0.640625\n",
      "2019-06-29T16:24:24.386954: step 424, loss 0.443015, acc 0.796875\n",
      "2019-06-29T16:24:24.673157: step 425, loss 0.658613, acc 0.65625\n",
      "2019-06-29T16:24:24.958360: step 426, loss 0.668334, acc 0.671875\n",
      "2019-06-29T16:24:25.234557: step 427, loss 0.746553, acc 0.578125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-29T16:24:25.517759: step 428, loss 0.782794, acc 0.484375\n",
      "2019-06-29T16:24:25.793955: step 429, loss 0.656094, acc 0.703125\n",
      "2019-06-29T16:24:26.077156: step 430, loss 0.591494, acc 0.71875\n",
      "2019-06-29T16:24:26.354354: step 431, loss 0.733316, acc 0.65625\n",
      "2019-06-29T16:24:26.639557: step 432, loss 0.597744, acc 0.671875\n",
      "2019-06-29T16:24:26.936768: step 433, loss 0.473366, acc 0.734375\n",
      "2019-06-29T16:24:27.207963: step 434, loss 0.660236, acc 0.6875\n",
      "2019-06-29T16:24:27.475152: step 435, loss 0.668944, acc 0.640625\n",
      "2019-06-29T16:24:27.745344: step 436, loss 0.625865, acc 0.65625\n",
      "2019-06-29T16:24:28.013535: step 437, loss 0.629141, acc 0.765625\n",
      "2019-06-29T16:24:28.282727: step 438, loss 0.627616, acc 0.6875\n",
      "2019-06-29T16:24:28.551918: step 439, loss 0.738822, acc 0.578125\n",
      "2019-06-29T16:24:28.819108: step 440, loss 0.666156, acc 0.609375\n",
      "2019-06-29T16:24:29.084298: step 441, loss 0.674604, acc 0.6875\n",
      "2019-06-29T16:24:29.384511: step 442, loss 0.647496, acc 0.65625\n",
      "2019-06-29T16:24:29.656705: step 443, loss 0.609652, acc 0.703125\n",
      "2019-06-29T16:24:29.921894: step 444, loss 0.422067, acc 0.828125\n",
      "2019-06-29T16:24:30.187083: step 445, loss 0.479041, acc 0.765625\n",
      "2019-06-29T16:24:30.453273: step 446, loss 0.687734, acc 0.6875\n",
      "2019-06-29T16:24:30.721464: step 447, loss 0.645723, acc 0.65625\n",
      "2019-06-29T16:24:30.990655: step 448, loss 0.726916, acc 0.625\n",
      "2019-06-29T16:24:31.255844: step 449, loss 0.762495, acc 0.59375\n",
      "2019-06-29T16:24:31.535042: step 450, loss 0.876352, acc 0.516667\n",
      "2019-06-29T16:24:31.808237: step 451, loss 0.79047, acc 0.625\n",
      "2019-06-29T16:24:32.076428: step 452, loss 0.590352, acc 0.65625\n",
      "2019-06-29T16:24:32.357628: step 453, loss 0.600734, acc 0.6875\n",
      "2019-06-29T16:24:32.627821: step 454, loss 0.549537, acc 0.6875\n",
      "2019-06-29T16:24:32.895010: step 455, loss 0.51878, acc 0.765625\n",
      "2019-06-29T16:24:33.160199: step 456, loss 0.553383, acc 0.71875\n",
      "2019-06-29T16:24:33.429391: step 457, loss 0.665179, acc 0.671875\n",
      "2019-06-29T16:24:33.699330: step 458, loss 0.48975, acc 0.75\n",
      "2019-06-29T16:24:33.963025: step 459, loss 0.557545, acc 0.6875\n",
      "2019-06-29T16:24:34.231217: step 460, loss 0.581014, acc 0.703125\n",
      "2019-06-29T16:24:34.527427: step 461, loss 0.547979, acc 0.703125\n",
      "2019-06-29T16:24:34.858664: step 462, loss 0.509176, acc 0.71875\n",
      "2019-06-29T16:24:35.252945: step 463, loss 0.632627, acc 0.703125\n",
      "2019-06-29T16:24:35.614201: step 464, loss 0.437045, acc 0.78125\n",
      "2019-06-29T16:24:35.875303: step 465, loss 0.59546, acc 0.71875\n",
      "2019-06-29T16:24:36.217546: step 466, loss 0.507582, acc 0.78125\n",
      "2019-06-29T16:24:36.493743: step 467, loss 0.434041, acc 0.796875\n",
      "2019-06-29T16:24:36.772941: step 468, loss 0.546627, acc 0.734375\n",
      "2019-06-29T16:24:37.056143: step 469, loss 0.579769, acc 0.6875\n",
      "2019-06-29T16:24:37.356356: step 470, loss 0.499882, acc 0.734375\n",
      "2019-06-29T16:24:37.634554: step 471, loss 0.602452, acc 0.6875\n",
      "2019-06-29T16:24:37.908751: step 472, loss 0.531503, acc 0.75\n",
      "2019-06-29T16:24:38.183946: step 473, loss 0.623235, acc 0.734375\n",
      "2019-06-29T16:24:38.489163: step 474, loss 0.506652, acc 0.78125\n",
      "2019-06-29T16:24:38.766361: step 475, loss 0.597592, acc 0.6875\n",
      "2019-06-29T16:24:39.047561: step 476, loss 0.627478, acc 0.671875\n",
      "2019-06-29T16:24:39.336767: step 477, loss 0.604882, acc 0.65625\n",
      "2019-06-29T16:24:39.642986: step 478, loss 0.635665, acc 0.71875\n",
      "2019-06-29T16:24:39.928188: step 479, loss 0.553531, acc 0.703125\n",
      "2019-06-29T16:24:40.203383: step 480, loss 0.650296, acc 0.6875\n",
      "2019-06-29T16:24:40.485585: step 481, loss 0.668224, acc 0.6875\n",
      "2019-06-29T16:24:40.773790: step 482, loss 0.647906, acc 0.703125\n",
      "2019-06-29T16:24:41.052989: step 483, loss 0.533428, acc 0.71875\n",
      "2019-06-29T16:24:41.330187: step 484, loss 0.490694, acc 0.78125\n",
      "2019-06-29T16:24:41.616390: step 485, loss 0.717896, acc 0.703125\n",
      "2019-06-29T16:24:41.903595: step 486, loss 0.671954, acc 0.6875\n",
      "2019-06-29T16:24:42.176789: step 487, loss 0.524186, acc 0.640625\n",
      "2019-06-29T16:24:42.451985: step 488, loss 0.6208, acc 0.671875\n",
      "2019-06-29T16:24:42.736189: step 489, loss 0.487872, acc 0.71875\n",
      "2019-06-29T16:24:43.012384: step 490, loss 0.571715, acc 0.6875\n",
      "2019-06-29T16:24:43.272569: step 491, loss 0.512904, acc 0.765625\n",
      "2019-06-29T16:24:43.553769: step 492, loss 0.708444, acc 0.578125\n",
      "2019-06-29T16:24:43.816957: step 493, loss 0.609633, acc 0.625\n",
      "2019-06-29T16:24:44.088150: step 494, loss 0.520505, acc 0.71875\n",
      "2019-06-29T16:24:44.350336: step 495, loss 0.522602, acc 0.71875\n",
      "2019-06-29T16:24:44.617526: step 496, loss 0.540178, acc 0.71875\n",
      "2019-06-29T16:24:44.881715: step 497, loss 0.495526, acc 0.78125\n",
      "2019-06-29T16:24:45.158912: step 498, loss 0.43434, acc 0.796875\n",
      "2019-06-29T16:24:45.416095: step 499, loss 0.486086, acc 0.796875\n",
      "2019-06-29T16:24:45.681284: step 500, loss 0.599204, acc 0.6875\n",
      "\n",
      "Evaluation:\n",
      "2019-06-29T16:24:46.346760: step 500, loss 0.63078, acc 0.63227\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\mao\\Desktop\\Tensorflow\\CNN\\1\\cnn-text-classification-tf-master\\runs\\1561796526\\checkpoints\\model-500\n",
      "\n",
      "2019-06-29T16:24:47.459882: step 501, loss 0.658569, acc 0.703125\n",
      "2019-06-29T16:24:47.909202: step 502, loss 0.557245, acc 0.71875\n",
      "2019-06-29T16:24:48.186400: step 503, loss 0.531103, acc 0.796875\n",
      "2019-06-29T16:24:48.452589: step 504, loss 0.583757, acc 0.703125\n",
      "2019-06-29T16:24:48.719780: step 505, loss 0.599778, acc 0.703125\n",
      "2019-06-29T16:24:48.982967: step 506, loss 0.711304, acc 0.5625\n",
      "2019-06-29T16:24:49.240149: step 507, loss 0.593926, acc 0.734375\n",
      "2019-06-29T16:24:49.511342: step 508, loss 0.528396, acc 0.765625\n",
      "2019-06-29T16:24:49.778533: step 509, loss 0.532757, acc 0.78125\n",
      "2019-06-29T16:24:50.044764: step 510, loss 0.665492, acc 0.640625\n",
      "2019-06-29T16:24:50.311955: step 511, loss 0.53444, acc 0.75\n",
      "2019-06-29T16:24:50.653198: step 512, loss 0.532967, acc 0.78125\n",
      "2019-06-29T16:24:50.987436: step 513, loss 0.549266, acc 0.75\n",
      "2019-06-29T16:24:51.330681: step 514, loss 0.620806, acc 0.703125\n",
      "2019-06-29T16:24:51.635770: step 515, loss 0.657977, acc 0.671875\n",
      "2019-06-29T16:24:51.923975: step 516, loss 0.51392, acc 0.71875\n",
      "2019-06-29T16:24:52.196169: step 517, loss 0.583207, acc 0.734375\n",
      "2019-06-29T16:24:52.478370: step 518, loss 0.536477, acc 0.75\n",
      "2019-06-29T16:24:52.783587: step 519, loss 0.599729, acc 0.75\n",
      "2019-06-29T16:24:53.061785: step 520, loss 0.642575, acc 0.6875\n",
      "2019-06-29T16:24:53.339983: step 521, loss 0.589821, acc 0.75\n",
      "2019-06-29T16:24:53.618181: step 522, loss 0.572527, acc 0.75\n",
      "2019-06-29T16:24:53.926401: step 523, loss 0.644904, acc 0.609375\n",
      "2019-06-29T16:24:54.205600: step 524, loss 0.610621, acc 0.671875\n",
      "2019-06-29T16:24:54.476793: step 525, loss 0.499032, acc 0.828125\n",
      "2019-06-29T16:24:54.755991: step 526, loss 0.504009, acc 0.734375\n",
      "2019-06-29T16:24:55.060208: step 527, loss 0.595389, acc 0.625\n",
      "2019-06-29T16:24:55.342411: step 528, loss 0.516948, acc 0.71875\n",
      "2019-06-29T16:24:55.623609: step 529, loss 0.489829, acc 0.75\n",
      "2019-06-29T16:24:55.905810: step 530, loss 0.556665, acc 0.734375\n",
      "2019-06-29T16:24:56.198018: step 531, loss 0.621301, acc 0.640625\n",
      "2019-06-29T16:24:56.472213: step 532, loss 0.509394, acc 0.703125\n",
      "2019-06-29T16:24:56.748410: step 533, loss 0.539788, acc 0.65625\n",
      "2019-06-29T16:24:57.031612: step 534, loss 0.461488, acc 0.765625\n",
      "2019-06-29T16:24:57.325821: step 535, loss 0.56574, acc 0.671875\n",
      "2019-06-29T16:24:57.610024: step 536, loss 0.647384, acc 0.65625\n",
      "2019-06-29T16:24:57.885219: step 537, loss 0.533239, acc 0.71875\n",
      "2019-06-29T16:24:58.168421: step 538, loss 0.648266, acc 0.625\n",
      "2019-06-29T16:24:58.451622: step 539, loss 0.709638, acc 0.578125\n",
      "2019-06-29T16:24:58.720815: step 540, loss 0.561267, acc 0.75\n",
      "2019-06-29T16:24:58.991006: step 541, loss 0.479233, acc 0.796875\n",
      "2019-06-29T16:24:59.258197: step 542, loss 0.532078, acc 0.75\n",
      "2019-06-29T16:24:59.524386: step 543, loss 0.591262, acc 0.734375\n",
      "2019-06-29T16:24:59.792577: step 544, loss 0.526007, acc 0.734375\n",
      "2019-06-29T16:25:00.050269: step 545, loss 0.562186, acc 0.78125\n",
      "2019-06-29T16:25:00.311973: step 546, loss 0.556889, acc 0.703125\n",
      "2019-06-29T16:25:00.583166: step 547, loss 0.597941, acc 0.703125\n",
      "2019-06-29T16:25:00.845352: step 548, loss 0.516893, acc 0.734375\n",
      "2019-06-29T16:25:01.109541: step 549, loss 0.531942, acc 0.71875\n",
      "2019-06-29T16:25:01.372728: step 550, loss 0.633402, acc 0.6875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-29T16:25:01.652928: step 551, loss 0.576931, acc 0.734375\n",
      "2019-06-29T16:25:01.916115: step 552, loss 0.57581, acc 0.71875\n",
      "2019-06-29T16:25:02.174299: step 553, loss 0.556434, acc 0.703125\n",
      "2019-06-29T16:25:02.433483: step 554, loss 0.530693, acc 0.65625\n",
      "2019-06-29T16:25:02.707679: step 555, loss 0.60769, acc 0.71875\n",
      "2019-06-29T16:25:02.976870: step 556, loss 0.643222, acc 0.703125\n",
      "2019-06-29T16:25:03.239057: step 557, loss 0.517827, acc 0.75\n",
      "2019-06-29T16:25:03.507248: step 558, loss 0.587117, acc 0.671875\n",
      "2019-06-29T16:25:03.778441: step 559, loss 0.564643, acc 0.671875\n",
      "2019-06-29T16:25:04.040627: step 560, loss 0.513867, acc 0.75\n",
      "2019-06-29T16:25:04.297810: step 561, loss 0.420053, acc 0.765625\n",
      "2019-06-29T16:25:04.557996: step 562, loss 0.541186, acc 0.75\n",
      "2019-06-29T16:25:04.823184: step 563, loss 0.553115, acc 0.71875\n",
      "2019-06-29T16:25:05.083370: step 564, loss 0.437341, acc 0.8125\n",
      "2019-06-29T16:25:05.346557: step 565, loss 0.557042, acc 0.75\n",
      "2019-06-29T16:25:05.621915: step 566, loss 0.534117, acc 0.734375\n",
      "2019-06-29T16:25:05.889107: step 567, loss 0.58484, acc 0.703125\n",
      "2019-06-29T16:25:06.150294: step 568, loss 0.500959, acc 0.765625\n",
      "2019-06-29T16:25:06.416483: step 569, loss 0.596845, acc 0.703125\n",
      "2019-06-29T16:25:06.710693: step 570, loss 0.530958, acc 0.71875\n",
      "2019-06-29T16:25:07.007904: step 571, loss 0.53809, acc 0.734375\n",
      "2019-06-29T16:25:07.293107: step 572, loss 0.588467, acc 0.703125\n",
      "2019-06-29T16:25:07.572427: step 573, loss 0.635478, acc 0.65625\n",
      "2019-06-29T16:25:07.862634: step 574, loss 0.726171, acc 0.609375\n",
      "2019-06-29T16:25:08.157845: step 575, loss 0.536665, acc 0.765625\n",
      "2019-06-29T16:25:08.428037: step 576, loss 0.479945, acc 0.796875\n",
      "2019-06-29T16:25:08.709237: step 577, loss 0.513787, acc 0.703125\n",
      "2019-06-29T16:25:08.985433: step 578, loss 0.724433, acc 0.671875\n",
      "2019-06-29T16:25:09.294654: step 579, loss 0.626964, acc 0.671875\n",
      "2019-06-29T16:25:09.569849: step 580, loss 0.607701, acc 0.640625\n",
      "2019-06-29T16:25:09.849049: step 581, loss 0.583613, acc 0.65625\n",
      "2019-06-29T16:25:10.126246: step 582, loss 0.544615, acc 0.75\n",
      "2019-06-29T16:25:10.428461: step 583, loss 0.551585, acc 0.6875\n",
      "2019-06-29T16:25:10.706659: step 584, loss 0.473924, acc 0.796875\n",
      "2019-06-29T16:25:10.984858: step 585, loss 0.555178, acc 0.6875\n",
      "2019-06-29T16:25:11.271061: step 586, loss 0.508881, acc 0.75\n",
      "2019-06-29T16:25:11.568272: step 587, loss 0.522646, acc 0.8125\n",
      "2019-06-29T16:25:11.844469: step 588, loss 0.415345, acc 0.796875\n",
      "2019-06-29T16:25:12.124669: step 589, loss 0.609711, acc 0.578125\n",
      "2019-06-29T16:25:12.403868: step 590, loss 0.729277, acc 0.640625\n",
      "2019-06-29T16:25:12.692072: step 591, loss 0.578382, acc 0.71875\n",
      "2019-06-29T16:25:12.974273: step 592, loss 0.484113, acc 0.8125\n",
      "2019-06-29T16:25:13.256474: step 593, loss 0.556166, acc 0.671875\n",
      "2019-06-29T16:25:13.535674: step 594, loss 0.538606, acc 0.796875\n",
      "2019-06-29T16:25:13.813871: step 595, loss 0.435977, acc 0.8125\n",
      "2019-06-29T16:25:14.094071: step 596, loss 0.755278, acc 0.65625\n",
      "2019-06-29T16:25:14.372269: step 597, loss 0.587736, acc 0.6875\n",
      "2019-06-29T16:25:14.634456: step 598, loss 0.53599, acc 0.71875\n",
      "2019-06-29T16:25:14.897643: step 599, loss 0.552736, acc 0.609375\n",
      "2019-06-29T16:25:15.158829: step 600, loss 0.606372, acc 0.666667\n",
      "\n",
      "Evaluation:\n",
      "2019-06-29T16:25:15.822303: step 600, loss 0.687963, acc 0.587242\n",
      "\n",
      "WARNING:tensorflow:From C:\\professionalsoft\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "Saved model checkpoint to C:\\Users\\mao\\Desktop\\Tensorflow\\CNN\\1\\cnn-text-classification-tf-master\\runs\\1561796526\\checkpoints\\model-600\n",
      "\n",
      "2019-06-29T16:25:16.926312: step 601, loss 0.473816, acc 0.765625\n",
      "2019-06-29T16:25:17.353616: step 602, loss 0.75054, acc 0.65625\n",
      "2019-06-29T16:25:17.612800: step 603, loss 0.578723, acc 0.6875\n",
      "2019-06-29T16:25:17.879992: step 604, loss 0.446255, acc 0.828125\n",
      "2019-06-29T16:25:18.141177: step 605, loss 0.487349, acc 0.765625\n",
      "2019-06-29T16:25:18.406366: step 606, loss 0.528758, acc 0.734375\n",
      "2019-06-29T16:25:18.667552: step 607, loss 0.536152, acc 0.703125\n",
      "2019-06-29T16:25:18.931740: step 608, loss 0.648592, acc 0.65625\n",
      "2019-06-29T16:25:19.204934: step 609, loss 0.64713, acc 0.6875\n",
      "2019-06-29T16:25:19.464119: step 610, loss 0.537404, acc 0.734375\n",
      "2019-06-29T16:25:19.727306: step 611, loss 0.645071, acc 0.640625\n",
      "2019-06-29T16:25:19.992495: step 612, loss 0.435996, acc 0.828125\n",
      "2019-06-29T16:25:20.275697: step 613, loss 0.555812, acc 0.75\n",
      "2019-06-29T16:25:20.537883: step 614, loss 0.45513, acc 0.78125\n",
      "2019-06-29T16:25:20.795067: step 615, loss 0.527211, acc 0.75\n",
      "2019-06-29T16:25:21.051249: step 616, loss 0.526253, acc 0.78125\n",
      "2019-06-29T16:25:21.334450: step 617, loss 0.645692, acc 0.71875\n",
      "2019-06-29T16:25:21.600883: step 618, loss 0.469367, acc 0.765625\n",
      "2019-06-29T16:25:21.860068: step 619, loss 0.435504, acc 0.765625\n",
      "2019-06-29T16:25:22.117251: step 620, loss 0.494739, acc 0.703125\n",
      "2019-06-29T16:25:22.386442: step 621, loss 0.579483, acc 0.703125\n",
      "2019-06-29T16:25:22.679651: step 622, loss 0.438548, acc 0.8125\n",
      "2019-06-29T16:25:22.964854: step 623, loss 0.409281, acc 0.828125\n",
      "2019-06-29T16:25:23.259064: step 624, loss 0.450105, acc 0.796875\n",
      "2019-06-29T16:25:23.573855: step 625, loss 0.525042, acc 0.71875\n",
      "2019-06-29T16:25:23.861059: step 626, loss 0.476646, acc 0.765625\n",
      "2019-06-29T16:25:24.133253: step 627, loss 0.387029, acc 0.890625\n",
      "2019-06-29T16:25:24.407449: step 628, loss 0.490874, acc 0.6875\n",
      "2019-06-29T16:25:24.708663: step 629, loss 0.495835, acc 0.796875\n",
      "2019-06-29T16:25:24.984859: step 630, loss 0.457303, acc 0.765625\n",
      "2019-06-29T16:25:25.263057: step 631, loss 0.591444, acc 0.734375\n",
      "2019-06-29T16:25:25.542256: step 632, loss 0.576824, acc 0.71875\n",
      "2019-06-29T16:25:25.843471: step 633, loss 0.581029, acc 0.703125\n",
      "2019-06-29T16:25:26.120668: step 634, loss 0.496185, acc 0.78125\n",
      "2019-06-29T16:25:26.398866: step 635, loss 0.598891, acc 0.75\n",
      "2019-06-29T16:25:26.672061: step 636, loss 0.500939, acc 0.796875\n",
      "2019-06-29T16:25:26.970273: step 637, loss 0.511646, acc 0.765625\n",
      "2019-06-29T16:25:27.249472: step 638, loss 0.429896, acc 0.828125\n",
      "2019-06-29T16:25:27.536676: step 639, loss 0.486418, acc 0.828125\n",
      "2019-06-29T16:25:27.821879: step 640, loss 0.475944, acc 0.765625\n",
      "2019-06-29T16:25:28.109084: step 641, loss 0.412016, acc 0.796875\n",
      "2019-06-29T16:25:28.397289: step 642, loss 0.355527, acc 0.84375\n",
      "2019-06-29T16:25:28.675487: step 643, loss 0.506657, acc 0.828125\n",
      "2019-06-29T16:25:28.958689: step 644, loss 0.557589, acc 0.71875\n",
      "2019-06-29T16:25:29.243892: step 645, loss 0.53455, acc 0.765625\n",
      "2019-06-29T16:25:29.522089: step 646, loss 0.44754, acc 0.796875\n",
      "2019-06-29T16:25:29.797286: step 647, loss 0.555985, acc 0.75\n",
      "2019-06-29T16:25:30.075484: step 648, loss 0.415333, acc 0.796875\n",
      "2019-06-29T16:25:30.354683: step 649, loss 0.446214, acc 0.828125\n",
      "2019-06-29T16:25:30.618871: step 650, loss 0.533098, acc 0.75\n",
      "2019-06-29T16:25:30.879056: step 651, loss 0.544898, acc 0.71875\n",
      "2019-06-29T16:25:31.144246: step 652, loss 0.437981, acc 0.8125\n",
      "2019-06-29T16:25:31.414437: step 653, loss 0.482922, acc 0.8125\n",
      "2019-06-29T16:25:31.681627: step 654, loss 0.478214, acc 0.78125\n",
      "2019-06-29T16:25:31.947817: step 655, loss 0.468662, acc 0.75\n",
      "2019-06-29T16:25:32.205000: step 656, loss 0.536874, acc 0.71875\n",
      "2019-06-29T16:25:32.480196: step 657, loss 0.503763, acc 0.8125\n",
      "2019-06-29T16:25:32.738380: step 658, loss 0.459847, acc 0.734375\n",
      "2019-06-29T16:25:32.998566: step 659, loss 0.502509, acc 0.71875\n",
      "2019-06-29T16:25:33.263754: step 660, loss 0.470411, acc 0.8125\n",
      "2019-06-29T16:25:33.546955: step 661, loss 0.400862, acc 0.84375\n",
      "2019-06-29T16:25:33.810143: step 662, loss 0.502284, acc 0.78125\n",
      "2019-06-29T16:25:34.075331: step 663, loss 0.608699, acc 0.71875\n",
      "2019-06-29T16:25:34.337518: step 664, loss 0.45295, acc 0.796875\n",
      "2019-06-29T16:25:34.603708: step 665, loss 0.49954, acc 0.71875\n",
      "2019-06-29T16:25:34.869897: step 666, loss 0.522473, acc 0.78125\n",
      "2019-06-29T16:25:35.131083: step 667, loss 0.508071, acc 0.734375\n",
      "2019-06-29T16:25:35.398273: step 668, loss 0.515848, acc 0.71875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-29T16:25:35.660460: step 669, loss 0.498156, acc 0.8125\n",
      "2019-06-29T16:25:35.918644: step 670, loss 0.513308, acc 0.71875\n",
      "2019-06-29T16:25:36.183833: step 671, loss 0.550094, acc 0.734375\n",
      "2019-06-29T16:25:36.449021: step 672, loss 0.473231, acc 0.828125\n",
      "2019-06-29T16:25:36.710207: step 673, loss 0.578653, acc 0.703125\n",
      "2019-06-29T16:25:36.975397: step 674, loss 0.52902, acc 0.765625\n",
      "2019-06-29T16:25:37.237583: step 675, loss 0.583887, acc 0.6875\n",
      "2019-06-29T16:25:37.507775: step 676, loss 0.63679, acc 0.71875\n",
      "2019-06-29T16:25:37.771054: step 677, loss 0.36247, acc 0.828125\n",
      "2019-06-29T16:25:38.029238: step 678, loss 0.509373, acc 0.765625\n",
      "2019-06-29T16:25:38.294426: step 679, loss 0.483157, acc 0.78125\n",
      "2019-06-29T16:25:38.582632: step 680, loss 0.561959, acc 0.703125\n",
      "2019-06-29T16:25:38.876842: step 681, loss 0.629438, acc 0.6875\n",
      "2019-06-29T16:25:39.170052: step 682, loss 0.498447, acc 0.75\n",
      "2019-06-29T16:25:39.461213: step 683, loss 0.527913, acc 0.703125\n",
      "2019-06-29T16:25:39.729487: step 684, loss 0.607499, acc 0.703125\n",
      "2019-06-29T16:25:40.016691: step 685, loss 0.44858, acc 0.796875\n",
      "2019-06-29T16:25:40.292888: step 686, loss 0.560931, acc 0.6875\n",
      "2019-06-29T16:25:40.571086: step 687, loss 0.602367, acc 0.65625\n",
      "2019-06-29T16:25:40.842279: step 688, loss 0.588193, acc 0.71875\n",
      "2019-06-29T16:25:41.133487: step 689, loss 0.464563, acc 0.796875\n",
      "2019-06-29T16:25:41.410684: step 690, loss 0.492588, acc 0.796875\n",
      "2019-06-29T16:25:41.684879: step 691, loss 0.373927, acc 0.8125\n",
      "2019-06-29T16:25:41.961075: step 692, loss 0.51238, acc 0.765625\n",
      "2019-06-29T16:25:42.268295: step 693, loss 0.474244, acc 0.78125\n",
      "2019-06-29T16:25:42.542489: step 694, loss 0.369144, acc 0.84375\n",
      "2019-06-29T16:25:42.824691: step 695, loss 0.518306, acc 0.765625\n",
      "2019-06-29T16:25:43.108893: step 696, loss 0.454563, acc 0.796875\n",
      "2019-06-29T16:25:43.428120: step 697, loss 0.412803, acc 0.796875\n",
      "2019-06-29T16:25:43.719328: step 698, loss 0.670107, acc 0.6875\n",
      "2019-06-29T16:25:43.997526: step 699, loss 0.456521, acc 0.796875\n",
      "2019-06-29T16:25:44.284730: step 700, loss 0.58373, acc 0.6875\n",
      "\n",
      "Evaluation:\n",
      "2019-06-29T16:25:45.016253: step 700, loss 0.610921, acc 0.659475\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\mao\\Desktop\\Tensorflow\\CNN\\1\\cnn-text-classification-tf-master\\runs\\1561796526\\checkpoints\\model-700\n",
      "\n",
      "2019-06-29T16:25:47.890659: step 701, loss 0.47125, acc 0.703125\n",
      "2019-06-29T16:25:48.156849: step 702, loss 0.47835, acc 0.8125\n",
      "2019-06-29T16:25:48.424039: step 703, loss 0.475959, acc 0.8125\n",
      "2019-06-29T16:25:48.687227: step 704, loss 0.486841, acc 0.765625\n",
      "2019-06-29T16:25:48.956418: step 705, loss 0.452175, acc 0.8125\n",
      "2019-06-29T16:25:49.231615: step 706, loss 0.550683, acc 0.71875\n",
      "2019-06-29T16:25:49.499806: step 707, loss 0.451252, acc 0.765625\n",
      "2019-06-29T16:25:49.763994: step 708, loss 0.458239, acc 0.796875\n",
      "2019-06-29T16:25:50.045194: step 709, loss 0.549085, acc 0.71875\n",
      "2019-06-29T16:25:50.311383: step 710, loss 0.476733, acc 0.796875\n",
      "2019-06-29T16:25:50.579574: step 711, loss 0.465969, acc 0.765625\n",
      "2019-06-29T16:25:50.839759: step 712, loss 0.498512, acc 0.734375\n",
      "2019-06-29T16:25:51.119959: step 713, loss 0.612106, acc 0.703125\n",
      "2019-06-29T16:25:51.382145: step 714, loss 0.495927, acc 0.765625\n",
      "2019-06-29T16:25:51.655340: step 715, loss 0.528849, acc 0.828125\n",
      "2019-06-29T16:25:51.917526: step 716, loss 0.429616, acc 0.828125\n",
      "2019-06-29T16:25:52.198728: step 717, loss 0.407181, acc 0.796875\n",
      "2019-06-29T16:25:52.492936: step 718, loss 0.52459, acc 0.765625\n",
      "2019-06-29T16:25:52.763129: step 719, loss 0.453413, acc 0.71875\n",
      "2019-06-29T16:25:53.038324: step 720, loss 0.602855, acc 0.75\n",
      "2019-06-29T16:25:53.323527: step 721, loss 0.570006, acc 0.78125\n",
      "2019-06-29T16:25:53.596370: step 722, loss 0.469256, acc 0.75\n",
      "2019-06-29T16:25:53.860064: step 723, loss 0.474389, acc 0.78125\n",
      "2019-06-29T16:25:54.125253: step 724, loss 0.502257, acc 0.78125\n",
      "2019-06-29T16:25:54.408455: step 725, loss 0.545788, acc 0.703125\n",
      "2019-06-29T16:25:54.703664: step 726, loss 0.553001, acc 0.671875\n",
      "2019-06-29T16:25:54.996874: step 727, loss 0.552315, acc 0.71875\n",
      "2019-06-29T16:25:55.302090: step 728, loss 0.424908, acc 0.8125\n",
      "2019-06-29T16:25:55.600762: step 729, loss 0.705809, acc 0.671875\n",
      "2019-06-29T16:25:55.888967: step 730, loss 0.52139, acc 0.78125\n",
      "2019-06-29T16:25:56.162162: step 731, loss 0.489002, acc 0.703125\n",
      "2019-06-29T16:25:56.436358: step 732, loss 0.482746, acc 0.75\n",
      "2019-06-29T16:25:56.722561: step 733, loss 0.538681, acc 0.734375\n",
      "2019-06-29T16:25:56.998757: step 734, loss 0.357471, acc 0.859375\n",
      "2019-06-29T16:25:57.284962: step 735, loss 0.391133, acc 0.796875\n",
      "2019-06-29T16:25:57.597186: step 736, loss 0.535219, acc 0.796875\n",
      "2019-06-29T16:25:57.904402: step 737, loss 0.508369, acc 0.75\n",
      "2019-06-29T16:25:58.185602: step 738, loss 0.402935, acc 0.78125\n",
      "2019-06-29T16:25:58.459798: step 739, loss 0.460483, acc 0.796875\n",
      "2019-06-29T16:25:58.737996: step 740, loss 0.478792, acc 0.765625\n",
      "2019-06-29T16:25:59.009189: step 741, loss 0.589456, acc 0.71875\n",
      "2019-06-29T16:25:59.302397: step 742, loss 0.556104, acc 0.6875\n",
      "2019-06-29T16:25:59.595607: step 743, loss 0.456502, acc 0.765625\n",
      "2019-06-29T16:25:59.925841: step 744, loss 0.650203, acc 0.703125\n",
      "2019-06-29T16:26:00.270620: step 745, loss 0.461509, acc 0.8125\n",
      "2019-06-29T16:26:00.545814: step 746, loss 0.486831, acc 0.796875\n",
      "2019-06-29T16:26:00.843027: step 747, loss 0.398202, acc 0.84375\n",
      "2019-06-29T16:26:01.133233: step 748, loss 0.57429, acc 0.703125\n",
      "2019-06-29T16:26:01.413433: step 749, loss 0.467421, acc 0.78125\n",
      "2019-06-29T16:26:01.688628: step 750, loss 0.548211, acc 0.75\n",
      "2019-06-29T16:26:01.984839: step 751, loss 0.357045, acc 0.84375\n",
      "2019-06-29T16:26:02.269042: step 752, loss 0.494349, acc 0.765625\n",
      "2019-06-29T16:26:02.548240: step 753, loss 0.402545, acc 0.796875\n",
      "2019-06-29T16:26:02.810427: step 754, loss 0.40437, acc 0.875\n",
      "2019-06-29T16:26:03.097631: step 755, loss 0.464634, acc 0.8125\n",
      "2019-06-29T16:26:03.360819: step 756, loss 0.460275, acc 0.765625\n",
      "2019-06-29T16:26:03.620003: step 757, loss 0.476525, acc 0.765625\n",
      "2019-06-29T16:26:03.887193: step 758, loss 0.390948, acc 0.796875\n",
      "2019-06-29T16:26:04.152382: step 759, loss 0.512551, acc 0.78125\n",
      "2019-06-29T16:26:04.416570: step 760, loss 0.484363, acc 0.765625\n",
      "2019-06-29T16:26:04.682760: step 761, loss 0.479691, acc 0.75\n",
      "2019-06-29T16:26:04.948949: step 762, loss 0.356861, acc 0.8125\n",
      "2019-06-29T16:26:05.215139: step 763, loss 0.349634, acc 0.875\n",
      "2019-06-29T16:26:05.478326: step 764, loss 0.403566, acc 0.796875\n",
      "2019-06-29T16:26:05.740513: step 765, loss 0.524333, acc 0.78125\n",
      "2019-06-29T16:26:06.003700: step 766, loss 0.471688, acc 0.75\n",
      "2019-06-29T16:26:06.277896: step 767, loss 0.33294, acc 0.859375\n",
      "2019-06-29T16:26:06.545086: step 768, loss 0.465991, acc 0.828125\n",
      "2019-06-29T16:26:06.809274: step 769, loss 0.304246, acc 0.90625\n",
      "2019-06-29T16:26:07.084470: step 770, loss 0.408333, acc 0.796875\n",
      "2019-06-29T16:26:07.362668: step 771, loss 0.437531, acc 0.8125\n",
      "2019-06-29T16:26:07.621852: step 772, loss 0.487138, acc 0.828125\n",
      "2019-06-29T16:26:07.890043: step 773, loss 0.505713, acc 0.734375\n",
      "2019-06-29T16:26:08.156233: step 774, loss 0.31865, acc 0.859375\n",
      "2019-06-29T16:26:08.428427: step 775, loss 0.495434, acc 0.71875\n",
      "2019-06-29T16:26:08.695617: step 776, loss 0.421707, acc 0.796875\n",
      "2019-06-29T16:26:08.964808: step 777, loss 0.343886, acc 0.828125\n",
      "2019-06-29T16:26:09.228997: step 778, loss 0.412479, acc 0.8125\n",
      "2019-06-29T16:26:09.496187: step 779, loss 0.46155, acc 0.78125\n",
      "2019-06-29T16:26:09.788204: step 780, loss 0.37842, acc 0.84375\n",
      "2019-06-29T16:26:10.080412: step 781, loss 0.456198, acc 0.75\n",
      "2019-06-29T16:26:10.343599: step 782, loss 0.437052, acc 0.84375\n",
      "2019-06-29T16:26:10.673086: step 783, loss 0.324961, acc 0.875\n",
      "2019-06-29T16:26:10.982305: step 784, loss 0.521984, acc 0.765625\n",
      "2019-06-29T16:26:11.399601: step 785, loss 0.425943, acc 0.765625\n",
      "2019-06-29T16:26:11.672830: step 786, loss 0.537911, acc 0.78125\n",
      "2019-06-29T16:26:11.943983: step 787, loss 0.429767, acc 0.84375\n",
      "2019-06-29T16:26:12.209612: step 788, loss 0.454252, acc 0.828125\n",
      "2019-06-29T16:26:12.506492: step 789, loss 0.413547, acc 0.796875\n",
      "2019-06-29T16:26:12.897123: step 790, loss 0.564723, acc 0.734375\n",
      "2019-06-29T16:26:13.211744: step 791, loss 0.475592, acc 0.78125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-29T16:26:13.492997: step 792, loss 0.469027, acc 0.8125\n",
      "2019-06-29T16:26:13.774251: step 793, loss 0.380252, acc 0.828125\n",
      "2019-06-29T16:26:14.097859: step 794, loss 0.441244, acc 0.796875\n",
      "2019-06-29T16:26:14.363487: step 795, loss 0.543355, acc 0.75\n",
      "2019-06-29T16:26:14.644741: step 796, loss 0.416905, acc 0.8125\n",
      "2019-06-29T16:26:14.908486: step 797, loss 0.540211, acc 0.765625\n",
      "2019-06-29T16:26:15.236616: step 798, loss 0.427012, acc 0.8125\n",
      "2019-06-29T16:26:15.520163: step 799, loss 0.422502, acc 0.78125\n",
      "2019-06-29T16:26:15.848291: step 800, loss 0.345986, acc 0.828125\n",
      "\n",
      "Evaluation:\n",
      "2019-06-29T16:26:16.679390: step 800, loss 0.602063, acc 0.661351\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\mao\\Desktop\\Tensorflow\\CNN\\1\\cnn-text-classification-tf-master\\runs\\1561796526\\checkpoints\\model-800\n",
      "\n",
      "2019-06-29T16:26:18.305396: step 801, loss 0.379918, acc 0.8125\n",
      "2019-06-29T16:26:18.586649: step 802, loss 0.486819, acc 0.78125\n",
      "2019-06-29T16:26:18.867904: step 803, loss 0.482165, acc 0.765625\n",
      "2019-06-29T16:26:19.123157: step 804, loss 0.382076, acc 0.78125\n",
      "2019-06-29T16:26:19.388787: step 805, loss 0.39815, acc 0.859375\n",
      "2019-06-29T16:26:19.654415: step 806, loss 0.451931, acc 0.765625\n",
      "2019-06-29T16:26:19.910874: step 807, loss 0.453074, acc 0.78125\n",
      "2019-06-29T16:26:20.176503: step 808, loss 0.480357, acc 0.75\n",
      "2019-06-29T16:26:20.442132: step 809, loss 0.537962, acc 0.703125\n",
      "2019-06-29T16:26:20.723386: step 810, loss 0.51846, acc 0.765625\n",
      "2019-06-29T16:26:20.978784: step 811, loss 0.4196, acc 0.8125\n",
      "2019-06-29T16:26:21.260038: step 812, loss 0.440314, acc 0.796875\n",
      "2019-06-29T16:26:21.525669: step 813, loss 0.460328, acc 0.8125\n",
      "2019-06-29T16:26:21.783551: step 814, loss 0.404575, acc 0.875\n",
      "2019-06-29T16:26:22.054279: step 815, loss 0.570145, acc 0.71875\n",
      "2019-06-29T16:26:22.304282: step 816, loss 0.539851, acc 0.734375\n",
      "2019-06-29T16:26:22.585537: step 817, loss 0.49309, acc 0.78125\n",
      "2019-06-29T16:26:22.856276: step 818, loss 0.395238, acc 0.875\n",
      "2019-06-29T16:26:23.121904: step 819, loss 0.364337, acc 0.84375\n",
      "2019-06-29T16:26:23.387535: step 820, loss 0.548992, acc 0.765625\n",
      "2019-06-29T16:26:23.653164: step 821, loss 0.458465, acc 0.78125\n",
      "2019-06-29T16:26:23.908534: step 822, loss 0.384744, acc 0.890625\n",
      "2019-06-29T16:26:24.189788: step 823, loss 0.40948, acc 0.796875\n",
      "2019-06-29T16:26:24.455417: step 824, loss 0.402578, acc 0.84375\n",
      "2019-06-29T16:26:24.736672: step 825, loss 0.322551, acc 0.875\n",
      "2019-06-29T16:26:25.007521: step 826, loss 0.414362, acc 0.8125\n",
      "2019-06-29T16:26:25.257524: step 827, loss 0.48909, acc 0.75\n",
      "2019-06-29T16:26:25.538780: step 828, loss 0.384626, acc 0.8125\n",
      "2019-06-29T16:26:25.804409: step 829, loss 0.362348, acc 0.8125\n",
      "2019-06-29T16:26:26.075970: step 830, loss 0.441365, acc 0.828125\n",
      "2019-06-29T16:26:26.337001: step 831, loss 0.499832, acc 0.765625\n",
      "2019-06-29T16:26:26.649507: step 832, loss 0.412574, acc 0.75\n",
      "2019-06-29T16:26:26.952879: step 833, loss 0.373852, acc 0.8125\n",
      "2019-06-29T16:26:27.234133: step 834, loss 0.508976, acc 0.734375\n",
      "2019-06-29T16:26:27.508581: step 835, loss 0.544982, acc 0.765625\n",
      "2019-06-29T16:26:27.774211: step 836, loss 0.41121, acc 0.78125\n",
      "2019-06-29T16:26:28.060903: step 837, loss 0.43665, acc 0.796875\n",
      "2019-06-29T16:26:28.342158: step 838, loss 0.425863, acc 0.828125\n",
      "2019-06-29T16:26:28.607787: step 839, loss 0.430546, acc 0.796875\n",
      "2019-06-29T16:26:28.878806: step 840, loss 0.504169, acc 0.796875\n",
      "2019-06-29T16:26:29.175685: step 841, loss 0.489388, acc 0.734375\n",
      "2019-06-29T16:26:29.441314: step 842, loss 0.587263, acc 0.734375\n",
      "2019-06-29T16:26:29.706943: step 843, loss 0.461727, acc 0.78125\n",
      "2019-06-29T16:26:29.977819: step 844, loss 0.400997, acc 0.75\n",
      "2019-06-29T16:26:30.274698: step 845, loss 0.434564, acc 0.796875\n",
      "2019-06-29T16:26:30.555951: step 846, loss 0.50822, acc 0.765625\n",
      "2019-06-29T16:26:30.821582: step 847, loss 0.380903, acc 0.8125\n",
      "2019-06-29T16:26:31.108241: step 848, loss 0.318026, acc 0.859375\n",
      "2019-06-29T16:26:31.405117: step 849, loss 0.451999, acc 0.78125\n",
      "2019-06-29T16:26:31.686370: step 850, loss 0.443948, acc 0.8125\n",
      "2019-06-29T16:26:31.957423: step 851, loss 0.381995, acc 0.828125\n",
      "2019-06-29T16:26:32.238677: step 852, loss 0.421675, acc 0.8125\n",
      "2019-06-29T16:26:32.535557: step 853, loss 0.444549, acc 0.75\n",
      "2019-06-29T16:26:32.816811: step 854, loss 0.570771, acc 0.703125\n",
      "2019-06-29T16:26:33.087716: step 855, loss 0.494653, acc 0.8125\n",
      "2019-06-29T16:26:33.368970: step 856, loss 0.513016, acc 0.71875\n",
      "2019-06-29T16:26:33.650224: step 857, loss 0.349572, acc 0.84375\n",
      "2019-06-29T16:26:33.936933: step 858, loss 0.398642, acc 0.8125\n",
      "2019-06-29T16:26:34.213497: step 859, loss 0.413111, acc 0.828125\n",
      "2019-06-29T16:26:34.479126: step 860, loss 0.428416, acc 0.796875\n",
      "2019-06-29T16:26:34.744755: step 861, loss 0.462785, acc 0.75\n",
      "2019-06-29T16:26:35.000144: step 862, loss 0.513891, acc 0.75\n",
      "2019-06-29T16:26:35.265772: step 863, loss 0.481077, acc 0.796875\n",
      "2019-06-29T16:26:35.531402: step 864, loss 0.49106, acc 0.765625\n",
      "2019-06-29T16:26:35.797030: step 865, loss 0.434429, acc 0.765625\n",
      "2019-06-29T16:26:36.067944: step 866, loss 0.490405, acc 0.78125\n",
      "2019-06-29T16:26:36.333573: step 867, loss 0.490654, acc 0.71875\n",
      "2019-06-29T16:26:36.583577: step 868, loss 0.405628, acc 0.71875\n",
      "2019-06-29T16:26:36.870144: step 869, loss 0.357649, acc 0.84375\n",
      "2019-06-29T16:26:37.135774: step 870, loss 0.436716, acc 0.78125\n",
      "2019-06-29T16:26:37.385777: step 871, loss 0.363504, acc 0.875\n",
      "2019-06-29T16:26:37.635781: step 872, loss 0.316492, acc 0.875\n",
      "2019-06-29T16:26:37.922431: step 873, loss 0.410249, acc 0.78125\n",
      "2019-06-29T16:26:38.188059: step 874, loss 0.440576, acc 0.84375\n",
      "2019-06-29T16:26:38.438062: step 875, loss 0.49534, acc 0.796875\n",
      "2019-06-29T16:26:38.719317: step 876, loss 0.488639, acc 0.78125\n",
      "2019-06-29T16:26:38.990378: step 877, loss 0.444738, acc 0.828125\n",
      "2019-06-29T16:26:39.256007: step 878, loss 0.370225, acc 0.84375\n",
      "2019-06-29T16:26:39.521635: step 879, loss 0.502212, acc 0.765625\n",
      "2019-06-29T16:26:39.802890: step 880, loss 0.439367, acc 0.703125\n",
      "2019-06-29T16:26:40.089804: step 881, loss 0.421105, acc 0.828125\n",
      "2019-06-29T16:26:40.355433: step 882, loss 0.402418, acc 0.84375\n",
      "2019-06-29T16:26:40.621061: step 883, loss 0.556344, acc 0.71875\n",
      "2019-06-29T16:26:40.942824: step 884, loss 0.506844, acc 0.734375\n",
      "2019-06-29T16:26:41.208454: step 885, loss 0.440402, acc 0.765625\n",
      "2019-06-29T16:26:41.505332: step 886, loss 0.346637, acc 0.859375\n",
      "2019-06-29T16:26:41.788501: step 887, loss 0.430665, acc 0.84375\n",
      "2019-06-29T16:26:42.043918: step 888, loss 0.525322, acc 0.71875\n",
      "2019-06-29T16:26:42.325172: step 889, loss 0.308159, acc 0.890625\n",
      "2019-06-29T16:26:42.784193: step 890, loss 0.476728, acc 0.75\n",
      "2019-06-29T16:26:43.123435: step 891, loss 0.40718, acc 0.78125\n",
      "2019-06-29T16:26:43.440401: step 892, loss 0.432406, acc 0.75\n",
      "2019-06-29T16:26:43.738940: step 893, loss 0.376495, acc 0.8125\n",
      "2019-06-29T16:26:44.012134: step 894, loss 0.546432, acc 0.75\n",
      "2019-06-29T16:26:44.291783: step 895, loss 0.604927, acc 0.703125\n",
      "2019-06-29T16:26:44.596002: step 896, loss 0.311595, acc 0.90625\n",
      "2019-06-29T16:26:44.976270: step 897, loss 0.481282, acc 0.765625\n",
      "2019-06-29T16:26:45.353542: step 898, loss 0.318371, acc 0.921875\n",
      "2019-06-29T16:26:45.999020: step 899, loss 0.386016, acc 0.828125\n",
      "2019-06-29T16:26:46.277556: step 900, loss 0.523365, acc 0.783333\n",
      "\n",
      "Evaluation:\n",
      "2019-06-29T16:26:47.406361: step 900, loss 0.596691, acc 0.672608\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\mao\\Desktop\\Tensorflow\\CNN\\1\\cnn-text-classification-tf-master\\runs\\1561796526\\checkpoints\\model-900\n",
      "\n",
      "2019-06-29T16:26:49.165608: step 901, loss 0.500583, acc 0.78125\n",
      "2019-06-29T16:26:49.486115: step 902, loss 0.408435, acc 0.84375\n",
      "2019-06-29T16:26:49.776498: step 903, loss 0.318573, acc 0.828125\n",
      "2019-06-29T16:26:50.111500: step 904, loss 0.229509, acc 0.921875\n",
      "2019-06-29T16:26:50.465868: step 905, loss 0.366599, acc 0.828125\n",
      "2019-06-29T16:26:50.756929: step 906, loss 0.337607, acc 0.84375\n",
      "2019-06-29T16:26:51.048136: step 907, loss 0.377024, acc 0.828125\n",
      "2019-06-29T16:26:51.333082: step 908, loss 0.451921, acc 0.796875\n",
      "2019-06-29T16:26:51.676962: step 909, loss 0.291259, acc 0.890625\n",
      "2019-06-29T16:26:52.045225: step 910, loss 0.498562, acc 0.828125\n",
      "2019-06-29T16:26:52.409483: step 911, loss 0.288411, acc 0.890625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-29T16:26:52.793899: step 912, loss 0.410104, acc 0.828125\n",
      "2019-06-29T16:26:53.049820: step 913, loss 0.418762, acc 0.765625\n",
      "2019-06-29T16:26:53.331663: step 914, loss 0.373537, acc 0.859375\n",
      "2019-06-29T16:26:53.597852: step 915, loss 0.3178, acc 0.90625\n",
      "2019-06-29T16:26:53.873686: step 916, loss 0.372278, acc 0.859375\n",
      "2019-06-29T16:26:54.137615: step 917, loss 0.46385, acc 0.8125\n",
      "2019-06-29T16:26:54.402508: step 918, loss 0.376205, acc 0.828125\n",
      "2019-06-29T16:26:54.687336: step 919, loss 0.313738, acc 0.859375\n",
      "2019-06-29T16:26:54.966749: step 920, loss 0.421236, acc 0.796875\n",
      "2019-06-29T16:26:55.235798: step 921, loss 0.369814, acc 0.828125\n",
      "2019-06-29T16:26:55.500640: step 922, loss 0.40197, acc 0.828125\n",
      "2019-06-29T16:26:55.767797: step 923, loss 0.340912, acc 0.875\n",
      "2019-06-29T16:26:56.054000: step 924, loss 0.265923, acc 0.90625\n",
      "2019-06-29T16:26:56.317355: step 925, loss 0.287837, acc 0.828125\n",
      "2019-06-29T16:26:56.583842: step 926, loss 0.439329, acc 0.796875\n",
      "2019-06-29T16:26:56.852277: step 927, loss 0.437224, acc 0.734375\n",
      "2019-06-29T16:26:57.249321: step 928, loss 0.292985, acc 0.921875\n",
      "2019-06-29T16:26:57.515695: step 929, loss 0.31497, acc 0.859375\n",
      "2019-06-29T16:26:57.767500: step 930, loss 0.247069, acc 0.90625\n",
      "2019-06-29T16:26:58.034117: step 931, loss 0.38218, acc 0.859375\n",
      "2019-06-29T16:26:58.316237: step 932, loss 0.279512, acc 0.890625\n",
      "2019-06-29T16:26:58.613728: step 933, loss 0.352569, acc 0.859375\n",
      "2019-06-29T16:26:58.911743: step 934, loss 0.382748, acc 0.796875\n",
      "2019-06-29T16:26:59.199938: step 935, loss 0.255084, acc 0.9375\n",
      "2019-06-29T16:26:59.500450: step 936, loss 0.352007, acc 0.890625\n",
      "2019-06-29T16:26:59.895383: step 937, loss 0.384706, acc 0.8125\n",
      "2019-06-29T16:27:00.265012: step 938, loss 0.389239, acc 0.859375\n",
      "2019-06-29T16:27:00.559496: step 939, loss 0.251911, acc 0.9375\n",
      "2019-06-29T16:27:00.847514: step 940, loss 0.374433, acc 0.84375\n",
      "2019-06-29T16:27:01.117698: step 941, loss 0.264824, acc 0.875\n",
      "2019-06-29T16:27:01.413880: step 942, loss 0.450523, acc 0.734375\n",
      "2019-06-29T16:27:01.683706: step 943, loss 0.499838, acc 0.796875\n",
      "2019-06-29T16:27:01.957687: step 944, loss 0.372657, acc 0.78125\n",
      "2019-06-29T16:27:02.240776: step 945, loss 0.368759, acc 0.8125\n",
      "2019-06-29T16:27:02.516547: step 946, loss 0.404832, acc 0.859375\n",
      "2019-06-29T16:27:02.788941: step 947, loss 0.27937, acc 0.890625\n",
      "2019-06-29T16:27:03.063847: step 948, loss 0.433137, acc 0.828125\n",
      "2019-06-29T16:27:03.332596: step 949, loss 0.39629, acc 0.84375\n",
      "2019-06-29T16:27:03.632182: step 950, loss 0.404144, acc 0.796875\n",
      "2019-06-29T16:27:03.900326: step 951, loss 0.337928, acc 0.890625\n",
      "2019-06-29T16:27:04.173317: step 952, loss 0.404101, acc 0.765625\n",
      "2019-06-29T16:27:04.449468: step 953, loss 0.394849, acc 0.734375\n",
      "2019-06-29T16:27:04.732474: step 954, loss 0.401683, acc 0.796875\n",
      "2019-06-29T16:27:05.007807: step 955, loss 0.448776, acc 0.828125\n",
      "2019-06-29T16:27:05.283953: step 956, loss 0.428934, acc 0.796875\n",
      "2019-06-29T16:27:05.564645: step 957, loss 0.402826, acc 0.84375\n",
      "2019-06-29T16:27:05.848448: step 958, loss 0.418927, acc 0.84375\n",
      "2019-06-29T16:27:06.116976: step 959, loss 0.379936, acc 0.796875\n",
      "2019-06-29T16:27:06.400093: step 960, loss 0.300681, acc 0.875\n",
      "2019-06-29T16:27:06.666103: step 961, loss 0.369901, acc 0.828125\n",
      "2019-06-29T16:27:06.939030: step 962, loss 0.221743, acc 0.90625\n",
      "2019-06-29T16:27:07.215947: step 963, loss 0.55944, acc 0.703125\n",
      "2019-06-29T16:27:07.474413: step 964, loss 0.250118, acc 0.90625\n",
      "2019-06-29T16:27:07.749381: step 965, loss 0.29217, acc 0.890625\n",
      "2019-06-29T16:27:08.017405: step 966, loss 0.388709, acc 0.796875\n",
      "2019-06-29T16:27:08.283468: step 967, loss 0.435952, acc 0.8125\n",
      "2019-06-29T16:27:08.540727: step 968, loss 0.310248, acc 0.859375\n",
      "2019-06-29T16:27:08.808661: step 969, loss 0.396853, acc 0.75\n",
      "2019-06-29T16:27:09.072198: step 970, loss 0.385753, acc 0.765625\n",
      "2019-06-29T16:27:09.333309: step 971, loss 0.29833, acc 0.875\n",
      "2019-06-29T16:27:09.597547: step 972, loss 0.33033, acc 0.875\n",
      "2019-06-29T16:27:09.862882: step 973, loss 0.295407, acc 0.8125\n",
      "2019-06-29T16:27:10.135130: step 974, loss 0.532158, acc 0.71875\n",
      "2019-06-29T16:27:10.400503: step 975, loss 0.451705, acc 0.8125\n",
      "2019-06-29T16:27:10.662949: step 976, loss 0.232816, acc 0.921875\n",
      "2019-06-29T16:27:10.926165: step 977, loss 0.357868, acc 0.875\n",
      "2019-06-29T16:27:11.209368: step 978, loss 0.409102, acc 0.796875\n",
      "2019-06-29T16:27:11.469778: step 979, loss 0.355947, acc 0.8125\n",
      "2019-06-29T16:27:11.738586: step 980, loss 0.43889, acc 0.796875\n",
      "2019-06-29T16:27:12.012571: step 981, loss 0.314135, acc 0.859375\n",
      "2019-06-29T16:27:12.298775: step 982, loss 0.356566, acc 0.828125\n",
      "2019-06-29T16:27:12.565224: step 983, loss 0.30455, acc 0.875\n",
      "2019-06-29T16:27:12.839981: step 984, loss 0.394265, acc 0.78125\n",
      "2019-06-29T16:27:13.103728: step 985, loss 0.291484, acc 0.859375\n",
      "2019-06-29T16:27:13.367427: step 986, loss 0.303146, acc 0.890625\n",
      "2019-06-29T16:27:13.650161: step 987, loss 0.418586, acc 0.796875\n",
      "2019-06-29T16:27:13.921723: step 988, loss 0.318402, acc 0.890625\n",
      "2019-06-29T16:27:14.196663: step 989, loss 0.368741, acc 0.78125\n",
      "2019-06-29T16:27:14.476571: step 990, loss 0.37312, acc 0.8125\n",
      "2019-06-29T16:27:14.767434: step 991, loss 0.384962, acc 0.828125\n",
      "2019-06-29T16:27:15.086444: step 992, loss 0.335385, acc 0.828125\n",
      "2019-06-29T16:27:15.367708: step 993, loss 0.303217, acc 0.859375\n",
      "2019-06-29T16:27:15.673219: step 994, loss 0.441296, acc 0.75\n",
      "2019-06-29T16:27:15.945585: step 995, loss 0.327164, acc 0.890625\n",
      "2019-06-29T16:27:16.216277: step 996, loss 0.306546, acc 0.84375\n",
      "2019-06-29T16:27:16.499097: step 997, loss 0.260179, acc 0.875\n",
      "2019-06-29T16:27:16.797523: step 998, loss 0.332703, acc 0.859375\n",
      "2019-06-29T16:27:17.079391: step 999, loss 0.47372, acc 0.75\n",
      "2019-06-29T16:27:17.349695: step 1000, loss 0.343124, acc 0.84375\n",
      "\n",
      "Evaluation:\n",
      "2019-06-29T16:27:18.130778: step 1000, loss 0.599677, acc 0.703565\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\mao\\Desktop\\Tensorflow\\CNN\\1\\cnn-text-classification-tf-master\\runs\\1561796526\\checkpoints\\model-1000\n",
      "\n",
      "2019-06-29T16:27:19.768491: step 1001, loss 0.342719, acc 0.84375\n",
      "2019-06-29T16:27:20.092737: step 1002, loss 0.48798, acc 0.78125\n",
      "2019-06-29T16:27:20.394420: step 1003, loss 0.461639, acc 0.8125\n",
      "2019-06-29T16:27:20.744769: step 1004, loss 0.351106, acc 0.828125\n",
      "2019-06-29T16:27:21.048266: step 1005, loss 0.454005, acc 0.796875\n",
      "2019-06-29T16:27:21.332775: step 1006, loss 0.364125, acc 0.859375\n",
      "2019-06-29T16:27:21.605745: step 1007, loss 0.322412, acc 0.796875\n",
      "2019-06-29T16:27:21.948515: step 1008, loss 0.372417, acc 0.828125\n",
      "2019-06-29T16:27:22.261187: step 1009, loss 0.374074, acc 0.859375\n",
      "2019-06-29T16:27:22.543763: step 1010, loss 0.497682, acc 0.78125\n",
      "2019-06-29T16:27:22.816776: step 1011, loss 0.406137, acc 0.828125\n",
      "2019-06-29T16:27:23.147182: step 1012, loss 0.322387, acc 0.875\n",
      "2019-06-29T16:27:23.434645: step 1013, loss 0.358693, acc 0.8125\n",
      "2019-06-29T16:27:23.725853: step 1014, loss 0.36518, acc 0.875\n",
      "2019-06-29T16:27:23.993904: step 1015, loss 0.501717, acc 0.765625\n",
      "2019-06-29T16:27:24.299355: step 1016, loss 0.370578, acc 0.8125\n",
      "2019-06-29T16:27:24.574785: step 1017, loss 0.329726, acc 0.859375\n",
      "2019-06-29T16:27:24.848321: step 1018, loss 0.359667, acc 0.84375\n",
      "2019-06-29T16:27:25.144246: step 1019, loss 0.409899, acc 0.8125\n",
      "2019-06-29T16:27:25.414275: step 1020, loss 0.423815, acc 0.796875\n",
      "2019-06-29T16:27:25.680547: step 1021, loss 0.460928, acc 0.75\n",
      "2019-06-29T16:27:25.958973: step 1022, loss 0.355552, acc 0.8125\n",
      "2019-06-29T16:27:26.222247: step 1023, loss 0.563553, acc 0.75\n",
      "2019-06-29T16:27:26.497458: step 1024, loss 0.602013, acc 0.671875\n",
      "2019-06-29T16:27:26.757904: step 1025, loss 0.342648, acc 0.859375\n",
      "2019-06-29T16:27:27.027855: step 1026, loss 0.344981, acc 0.84375\n",
      "2019-06-29T16:27:27.295481: step 1027, loss 0.289831, acc 0.859375\n",
      "2019-06-29T16:27:27.565782: step 1028, loss 0.383666, acc 0.859375\n",
      "2019-06-29T16:27:27.829764: step 1029, loss 0.537604, acc 0.734375\n",
      "2019-06-29T16:27:28.099065: step 1030, loss 0.33802, acc 0.84375\n",
      "2019-06-29T16:27:28.366136: step 1031, loss 0.401545, acc 0.8125\n",
      "2019-06-29T16:27:28.640845: step 1032, loss 0.472245, acc 0.734375\n",
      "2019-06-29T16:27:28.957070: step 1033, loss 0.349086, acc 0.84375\n",
      "2019-06-29T16:27:29.227505: step 1034, loss 0.467244, acc 0.8125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-29T16:27:29.497926: step 1035, loss 0.350697, acc 0.8125\n",
      "2019-06-29T16:27:29.774952: step 1036, loss 0.315296, acc 0.859375\n",
      "2019-06-29T16:27:30.091177: step 1037, loss 0.377578, acc 0.890625\n",
      "2019-06-29T16:27:30.353787: step 1038, loss 0.424607, acc 0.8125\n",
      "2019-06-29T16:27:30.751071: step 1039, loss 0.34729, acc 0.828125\n",
      "2019-06-29T16:27:31.048713: step 1040, loss 0.491633, acc 0.796875\n",
      "2019-06-29T16:27:31.345810: step 1041, loss 0.281871, acc 0.875\n",
      "2019-06-29T16:27:31.619971: step 1042, loss 0.308227, acc 0.890625\n",
      "2019-06-29T16:27:31.931669: step 1043, loss 0.31755, acc 0.890625\n",
      "2019-06-29T16:27:32.209106: step 1044, loss 0.440777, acc 0.8125\n",
      "2019-06-29T16:27:32.487901: step 1045, loss 0.429009, acc 0.828125\n",
      "2019-06-29T16:27:32.763082: step 1046, loss 0.342317, acc 0.84375\n",
      "2019-06-29T16:27:33.046502: step 1047, loss 0.358583, acc 0.828125\n",
      "2019-06-29T16:27:33.330382: step 1048, loss 0.358388, acc 0.875\n",
      "2019-06-29T16:27:33.642604: step 1049, loss 0.386091, acc 0.796875\n",
      "2019-06-29T16:27:33.915741: step 1050, loss 0.40054, acc 0.8\n",
      "2019-06-29T16:27:34.210714: step 1051, loss 0.311677, acc 0.9375\n",
      "2019-06-29T16:27:34.482729: step 1052, loss 0.301564, acc 0.890625\n",
      "2019-06-29T16:27:34.754316: step 1053, loss 0.339706, acc 0.875\n",
      "2019-06-29T16:27:35.033516: step 1054, loss 0.260838, acc 0.890625\n",
      "2019-06-29T16:27:35.345213: step 1055, loss 0.364158, acc 0.875\n",
      "2019-06-29T16:27:35.640429: step 1056, loss 0.322037, acc 0.859375\n",
      "2019-06-29T16:27:35.911367: step 1057, loss 0.179388, acc 0.953125\n",
      "2019-06-29T16:27:36.197011: step 1058, loss 0.377277, acc 0.8125\n",
      "2019-06-29T16:27:36.482108: step 1059, loss 0.340594, acc 0.84375\n",
      "2019-06-29T16:27:36.758565: step 1060, loss 0.303319, acc 0.875\n",
      "2019-06-29T16:27:37.032216: step 1061, loss 0.355476, acc 0.78125\n",
      "2019-06-29T16:27:37.316058: step 1062, loss 0.32855, acc 0.890625\n",
      "2019-06-29T16:27:37.597087: step 1063, loss 0.334319, acc 0.875\n",
      "2019-06-29T16:27:37.880563: step 1064, loss 0.316729, acc 0.875\n",
      "2019-06-29T16:27:38.160626: step 1065, loss 0.240816, acc 0.90625\n",
      "2019-06-29T16:27:38.453836: step 1066, loss 0.328701, acc 0.875\n",
      "2019-06-29T16:27:38.727917: step 1067, loss 0.308724, acc 0.8125\n",
      "2019-06-29T16:27:38.981837: step 1068, loss 0.300062, acc 0.890625\n",
      "2019-06-29T16:27:39.255601: step 1069, loss 0.373095, acc 0.796875\n",
      "2019-06-29T16:27:39.513573: step 1070, loss 0.220339, acc 0.890625\n",
      "2019-06-29T16:27:39.786487: step 1071, loss 0.221209, acc 0.921875\n",
      "2019-06-29T16:27:40.065120: step 1072, loss 0.423478, acc 0.859375\n",
      "2019-06-29T16:27:40.326013: step 1073, loss 0.325508, acc 0.890625\n",
      "2019-06-29T16:27:40.582624: step 1074, loss 0.339033, acc 0.859375\n",
      "2019-06-29T16:27:40.848455: step 1075, loss 0.233107, acc 0.90625\n",
      "2019-06-29T16:27:41.114302: step 1076, loss 0.278764, acc 0.84375\n",
      "2019-06-29T16:27:41.381268: step 1077, loss 0.276358, acc 0.90625\n",
      "2019-06-29T16:27:41.643483: step 1078, loss 0.313185, acc 0.84375\n",
      "2019-06-29T16:27:41.899094: step 1079, loss 0.32331, acc 0.828125\n",
      "2019-06-29T16:27:42.163450: step 1080, loss 0.316809, acc 0.875\n",
      "2019-06-29T16:27:42.434643: step 1081, loss 0.527462, acc 0.75\n",
      "2019-06-29T16:27:42.710839: step 1082, loss 0.299249, acc 0.859375\n",
      "2019-06-29T16:27:42.980031: step 1083, loss 0.297113, acc 0.875\n",
      "2019-06-29T16:27:43.248719: step 1084, loss 0.305516, acc 0.828125\n",
      "2019-06-29T16:27:43.511629: step 1085, loss 0.423493, acc 0.84375\n",
      "2019-06-29T16:27:43.770829: step 1086, loss 0.326976, acc 0.84375\n",
      "2019-06-29T16:27:44.032299: step 1087, loss 0.315227, acc 0.875\n",
      "2019-06-29T16:27:44.293105: step 1088, loss 0.233682, acc 0.875\n",
      "2019-06-29T16:27:44.566584: step 1089, loss 0.21724, acc 0.953125\n",
      "2019-06-29T16:27:44.880822: step 1090, loss 0.234336, acc 0.90625\n",
      "2019-06-29T16:27:45.145761: step 1091, loss 0.294185, acc 0.890625\n",
      "2019-06-29T16:27:45.412951: step 1092, loss 0.362912, acc 0.84375\n",
      "2019-06-29T16:27:45.684368: step 1093, loss 0.176849, acc 0.9375\n",
      "2019-06-29T16:27:45.948872: step 1094, loss 0.268015, acc 0.890625\n",
      "2019-06-29T16:27:46.223034: step 1095, loss 0.253184, acc 0.90625\n",
      "2019-06-29T16:27:46.482839: step 1096, loss 0.340508, acc 0.859375\n",
      "2019-06-29T16:27:46.787042: step 1097, loss 0.252011, acc 0.890625\n",
      "2019-06-29T16:27:47.128199: step 1098, loss 0.263585, acc 0.875\n",
      "2019-06-29T16:27:47.428701: step 1099, loss 0.302906, acc 0.859375\n",
      "2019-06-29T16:27:47.714599: step 1100, loss 0.377205, acc 0.828125\n",
      "\n",
      "Evaluation:\n",
      "2019-06-29T16:27:48.496395: step 1100, loss 0.600427, acc 0.681051\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\mao\\Desktop\\Tensorflow\\CNN\\1\\cnn-text-classification-tf-master\\runs\\1561796526\\checkpoints\\model-1100\n",
      "\n",
      "2019-06-29T16:27:49.902739: step 1101, loss 0.311859, acc 0.828125\n",
      "2019-06-29T16:27:50.278961: step 1102, loss 0.320956, acc 0.8125\n",
      "2019-06-29T16:27:50.564572: step 1103, loss 0.290791, acc 0.875\n",
      "2019-06-29T16:27:50.844864: step 1104, loss 0.258271, acc 0.890625\n",
      "2019-06-29T16:27:51.155902: step 1105, loss 0.288425, acc 0.953125\n",
      "2019-06-29T16:27:51.431329: step 1106, loss 0.276855, acc 0.890625\n",
      "2019-06-29T16:27:51.714334: step 1107, loss 0.351665, acc 0.828125\n",
      "2019-06-29T16:27:51.982380: step 1108, loss 0.456348, acc 0.796875\n",
      "2019-06-29T16:27:52.263596: step 1109, loss 0.295841, acc 0.90625\n",
      "2019-06-29T16:27:52.536579: step 1110, loss 0.296256, acc 0.921875\n",
      "2019-06-29T16:27:52.828873: step 1111, loss 0.325016, acc 0.828125\n",
      "2019-06-29T16:27:53.114542: step 1112, loss 0.407753, acc 0.84375\n",
      "2019-06-29T16:27:53.385010: step 1113, loss 0.278006, acc 0.90625\n",
      "2019-06-29T16:27:53.664658: step 1114, loss 0.283138, acc 0.890625\n",
      "2019-06-29T16:27:53.947656: step 1115, loss 0.24461, acc 0.9375\n",
      "2019-06-29T16:27:54.213815: step 1116, loss 0.299404, acc 0.859375\n",
      "2019-06-29T16:27:54.486205: step 1117, loss 0.340903, acc 0.84375\n",
      "2019-06-29T16:27:54.751644: step 1118, loss 0.29763, acc 0.859375\n",
      "2019-06-29T16:27:55.015326: step 1119, loss 0.234531, acc 0.890625\n",
      "2019-06-29T16:27:55.317632: step 1120, loss 0.314249, acc 0.84375\n",
      "2019-06-29T16:27:55.591767: step 1121, loss 0.272296, acc 0.875\n",
      "2019-06-29T16:27:55.864536: step 1122, loss 0.242321, acc 0.890625\n",
      "2019-06-29T16:27:56.147738: step 1123, loss 0.339849, acc 0.84375\n",
      "2019-06-29T16:27:56.418930: step 1124, loss 0.426417, acc 0.8125\n",
      "2019-06-29T16:27:56.684120: step 1125, loss 0.21552, acc 0.921875\n",
      "2019-06-29T16:27:56.956823: step 1126, loss 0.394294, acc 0.796875\n",
      "2019-06-29T16:27:57.230153: step 1127, loss 0.153888, acc 0.953125\n",
      "2019-06-29T16:27:57.502972: step 1128, loss 0.330179, acc 0.84375\n",
      "2019-06-29T16:27:57.765495: step 1129, loss 0.422692, acc 0.8125\n",
      "2019-06-29T16:27:58.048746: step 1130, loss 0.30971, acc 0.90625\n",
      "2019-06-29T16:27:58.329224: step 1131, loss 0.270961, acc 0.921875\n",
      "2019-06-29T16:27:58.597866: step 1132, loss 0.451992, acc 0.796875\n",
      "2019-06-29T16:27:58.863240: step 1133, loss 0.246916, acc 0.875\n",
      "2019-06-29T16:27:59.131104: step 1134, loss 0.254549, acc 0.90625\n",
      "2019-06-29T16:27:59.415011: step 1135, loss 0.278759, acc 0.890625\n",
      "2019-06-29T16:27:59.710103: step 1136, loss 0.298563, acc 0.875\n",
      "2019-06-29T16:28:00.018323: step 1137, loss 0.199184, acc 0.921875\n",
      "2019-06-29T16:28:00.292040: step 1138, loss 0.314855, acc 0.875\n",
      "2019-06-29T16:28:00.559230: step 1139, loss 0.283949, acc 0.90625\n",
      "2019-06-29T16:28:00.881459: step 1140, loss 0.294796, acc 0.890625\n",
      "2019-06-29T16:28:01.142857: step 1141, loss 0.446716, acc 0.765625\n",
      "2019-06-29T16:28:01.487622: step 1142, loss 0.269894, acc 0.921875\n",
      "2019-06-29T16:28:01.746892: step 1143, loss 0.305964, acc 0.890625\n",
      "2019-06-29T16:28:02.016085: step 1144, loss 0.299929, acc 0.859375\n",
      "2019-06-29T16:28:02.281307: step 1145, loss 0.251222, acc 0.921875\n",
      "2019-06-29T16:28:02.577119: step 1146, loss 0.286268, acc 0.859375\n",
      "2019-06-29T16:28:02.867182: step 1147, loss 0.367936, acc 0.78125\n",
      "2019-06-29T16:28:03.160476: step 1148, loss 0.387134, acc 0.84375\n",
      "2019-06-29T16:28:03.456688: step 1149, loss 0.133335, acc 0.984375\n",
      "2019-06-29T16:28:03.729046: step 1150, loss 0.349495, acc 0.875\n",
      "2019-06-29T16:28:04.014439: step 1151, loss 0.301472, acc 0.859375\n",
      "2019-06-29T16:28:04.289171: step 1152, loss 0.342807, acc 0.828125\n",
      "2019-06-29T16:28:04.562403: step 1153, loss 0.284361, acc 0.890625\n",
      "2019-06-29T16:28:04.849957: step 1154, loss 0.286721, acc 0.859375\n",
      "2019-06-29T16:28:05.136732: step 1155, loss 0.343012, acc 0.8125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-29T16:28:05.414388: step 1156, loss 0.344411, acc 0.890625\n",
      "2019-06-29T16:28:05.687557: step 1157, loss 0.41221, acc 0.765625\n",
      "2019-06-29T16:28:05.979240: step 1158, loss 0.207092, acc 0.921875\n",
      "2019-06-29T16:28:06.247790: step 1159, loss 0.202224, acc 0.96875\n",
      "2019-06-29T16:28:06.531665: step 1160, loss 0.44608, acc 0.75\n",
      "2019-06-29T16:28:06.797065: step 1161, loss 0.282927, acc 0.890625\n",
      "2019-06-29T16:28:07.100583: step 1162, loss 0.26736, acc 0.859375\n",
      "2019-06-29T16:28:07.385711: step 1163, loss 0.286729, acc 0.859375\n",
      "2019-06-29T16:28:07.677877: step 1164, loss 0.500047, acc 0.78125\n",
      "2019-06-29T16:28:07.970650: step 1165, loss 0.343069, acc 0.859375\n",
      "2019-06-29T16:28:08.260855: step 1166, loss 0.266918, acc 0.875\n",
      "2019-06-29T16:28:08.543700: step 1167, loss 0.423805, acc 0.796875\n",
      "2019-06-29T16:28:08.827847: step 1168, loss 0.250102, acc 0.890625\n",
      "2019-06-29T16:28:09.111686: step 1169, loss 0.339184, acc 0.875\n",
      "2019-06-29T16:28:09.393792: step 1170, loss 0.401147, acc 0.765625\n",
      "2019-06-29T16:28:09.676840: step 1171, loss 0.290735, acc 0.859375\n",
      "2019-06-29T16:28:09.962043: step 1172, loss 0.357276, acc 0.84375\n",
      "2019-06-29T16:28:10.251946: step 1173, loss 0.38155, acc 0.859375\n",
      "2019-06-29T16:28:10.534489: step 1174, loss 0.295629, acc 0.859375\n",
      "2019-06-29T16:28:10.810009: step 1175, loss 0.487098, acc 0.8125\n",
      "2019-06-29T16:28:11.063923: step 1176, loss 0.255929, acc 0.859375\n",
      "2019-06-29T16:28:11.354239: step 1177, loss 0.311589, acc 0.890625\n",
      "2019-06-29T16:28:11.639581: step 1178, loss 0.410187, acc 0.828125\n",
      "2019-06-29T16:28:11.896350: step 1179, loss 0.244371, acc 0.921875\n",
      "2019-06-29T16:28:12.167170: step 1180, loss 0.394188, acc 0.828125\n",
      "2019-06-29T16:28:12.515418: step 1181, loss 0.330258, acc 0.875\n",
      "2019-06-29T16:28:12.777382: step 1182, loss 0.381609, acc 0.859375\n",
      "2019-06-29T16:28:13.037524: step 1183, loss 0.246764, acc 0.921875\n",
      "2019-06-29T16:28:13.307717: step 1184, loss 0.311075, acc 0.875\n",
      "2019-06-29T16:28:13.577204: step 1185, loss 0.337779, acc 0.859375\n",
      "2019-06-29T16:28:13.850721: step 1186, loss 0.30797, acc 0.875\n",
      "2019-06-29T16:28:14.116911: step 1187, loss 0.450218, acc 0.8125\n",
      "2019-06-29T16:28:14.375977: step 1188, loss 0.247428, acc 0.921875\n",
      "2019-06-29T16:28:14.681747: step 1189, loss 0.36333, acc 0.828125\n",
      "2019-06-29T16:28:14.947584: step 1190, loss 0.283508, acc 0.90625\n",
      "2019-06-29T16:28:15.207795: step 1191, loss 0.322188, acc 0.859375\n",
      "2019-06-29T16:28:15.468055: step 1192, loss 0.269824, acc 0.921875\n",
      "2019-06-29T16:28:15.748022: step 1193, loss 0.242913, acc 0.90625\n",
      "2019-06-29T16:28:15.998180: step 1194, loss 0.463501, acc 0.765625\n",
      "2019-06-29T16:28:16.275136: step 1195, loss 0.322112, acc 0.875\n",
      "2019-06-29T16:28:16.530686: step 1196, loss 0.29911, acc 0.90625\n",
      "2019-06-29T16:28:16.814120: step 1197, loss 0.23387, acc 0.859375\n",
      "2019-06-29T16:28:17.072834: step 1198, loss 0.474111, acc 0.8125\n",
      "2019-06-29T16:28:17.337649: step 1199, loss 0.185229, acc 0.953125\n",
      "2019-06-29T16:28:17.592616: step 1200, loss 0.302908, acc 0.883333\n",
      "\n",
      "Evaluation:\n",
      "2019-06-29T16:28:18.257599: step 1200, loss 0.592771, acc 0.699812\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\mao\\Desktop\\Tensorflow\\CNN\\1\\cnn-text-classification-tf-master\\runs\\1561796526\\checkpoints\\model-1200\n",
      "\n",
      "2019-06-29T16:28:21.017802: step 1201, loss 0.23447, acc 0.9375\n",
      "2019-06-29T16:28:21.328024: step 1202, loss 0.169932, acc 0.953125\n",
      "2019-06-29T16:28:21.614179: step 1203, loss 0.186522, acc 0.9375\n",
      "2019-06-29T16:28:21.881263: step 1204, loss 0.251511, acc 0.921875\n",
      "2019-06-29T16:28:22.173904: step 1205, loss 0.186394, acc 0.9375\n",
      "2019-06-29T16:28:22.473462: step 1206, loss 0.16492, acc 0.9375\n",
      "2019-06-29T16:28:22.747760: step 1207, loss 0.222382, acc 0.90625\n",
      "2019-06-29T16:28:23.029782: step 1208, loss 0.269408, acc 0.90625\n",
      "2019-06-29T16:28:23.327565: step 1209, loss 0.150393, acc 0.9375\n",
      "2019-06-29T16:28:23.612573: step 1210, loss 0.315221, acc 0.890625\n",
      "2019-06-29T16:28:23.892296: step 1211, loss 0.273335, acc 0.90625\n",
      "2019-06-29T16:28:24.203946: step 1212, loss 0.242282, acc 0.921875\n",
      "2019-06-29T16:28:24.495097: step 1213, loss 0.214468, acc 0.90625\n",
      "2019-06-29T16:28:24.775909: step 1214, loss 0.240526, acc 0.890625\n",
      "2019-06-29T16:28:25.030326: step 1215, loss 0.320191, acc 0.84375\n",
      "2019-06-29T16:28:25.297501: step 1216, loss 0.2685, acc 0.890625\n",
      "2019-06-29T16:28:25.564214: step 1217, loss 0.251769, acc 0.875\n",
      "2019-06-29T16:28:25.830611: step 1218, loss 0.194424, acc 0.9375\n",
      "2019-06-29T16:28:26.096569: step 1219, loss 0.341623, acc 0.84375\n",
      "2019-06-29T16:28:26.363400: step 1220, loss 0.201959, acc 0.890625\n",
      "2019-06-29T16:28:26.677911: step 1221, loss 0.271597, acc 0.875\n",
      "2019-06-29T16:28:26.946015: step 1222, loss 0.163397, acc 0.96875\n",
      "2019-06-29T16:28:27.213988: step 1223, loss 0.158683, acc 0.953125\n",
      "2019-06-29T16:28:27.463494: step 1224, loss 0.322657, acc 0.859375\n",
      "2019-06-29T16:28:27.745984: step 1225, loss 0.170759, acc 0.9375\n",
      "2019-06-29T16:28:28.012867: step 1226, loss 0.313991, acc 0.84375\n",
      "2019-06-29T16:28:28.277161: step 1227, loss 0.21732, acc 0.90625\n",
      "2019-06-29T16:28:28.526929: step 1228, loss 0.249002, acc 0.90625\n",
      "2019-06-29T16:28:28.797095: step 1229, loss 0.309293, acc 0.90625\n",
      "2019-06-29T16:28:29.047414: step 1230, loss 0.293248, acc 0.890625\n",
      "2019-06-29T16:28:29.319349: step 1231, loss 0.188895, acc 0.96875\n",
      "2019-06-29T16:28:29.580761: step 1232, loss 0.22497, acc 0.875\n",
      "2019-06-29T16:28:29.845610: step 1233, loss 0.20384, acc 0.921875\n",
      "2019-06-29T16:28:30.112426: step 1234, loss 0.238509, acc 0.84375\n",
      "2019-06-29T16:28:30.379257: step 1235, loss 0.358788, acc 0.875\n",
      "2019-06-29T16:28:30.642455: step 1236, loss 0.217076, acc 0.921875\n",
      "2019-06-29T16:28:30.897330: step 1237, loss 0.283193, acc 0.875\n",
      "2019-06-29T16:28:31.178985: step 1238, loss 0.274317, acc 0.859375\n",
      "2019-06-29T16:28:31.461305: step 1239, loss 0.280496, acc 0.875\n",
      "2019-06-29T16:28:31.729738: step 1240, loss 0.185259, acc 0.921875\n",
      "2019-06-29T16:28:32.001876: step 1241, loss 0.345505, acc 0.90625\n",
      "2019-06-29T16:28:32.273792: step 1242, loss 0.109482, acc 1\n",
      "2019-06-29T16:28:32.546439: step 1243, loss 0.219646, acc 0.890625\n",
      "2019-06-29T16:28:32.828246: step 1244, loss 0.249521, acc 0.875\n",
      "2019-06-29T16:28:33.086640: step 1245, loss 0.210949, acc 0.953125\n",
      "2019-06-29T16:28:33.363502: step 1246, loss 0.355269, acc 0.890625\n",
      "2019-06-29T16:28:33.618786: step 1247, loss 0.264684, acc 0.875\n",
      "2019-06-29T16:28:33.880201: step 1248, loss 0.2417, acc 0.90625\n",
      "2019-06-29T16:28:34.199472: step 1249, loss 0.231305, acc 0.90625\n",
      "2019-06-29T16:28:34.471990: step 1250, loss 0.207401, acc 0.953125\n",
      "2019-06-29T16:28:34.778003: step 1251, loss 0.231683, acc 0.875\n",
      "2019-06-29T16:28:35.095418: step 1252, loss 0.22812, acc 0.90625\n",
      "2019-06-29T16:28:35.410657: step 1253, loss 0.231531, acc 0.890625\n",
      "2019-06-29T16:28:35.691490: step 1254, loss 0.313756, acc 0.90625\n",
      "2019-06-29T16:28:35.967462: step 1255, loss 0.248663, acc 0.921875\n",
      "2019-06-29T16:28:36.241849: step 1256, loss 0.179729, acc 0.921875\n",
      "2019-06-29T16:28:36.530330: step 1257, loss 0.252774, acc 0.90625\n",
      "2019-06-29T16:28:36.818209: step 1258, loss 0.252309, acc 0.90625\n",
      "2019-06-29T16:28:37.096256: step 1259, loss 0.282741, acc 0.859375\n",
      "2019-06-29T16:28:37.378965: step 1260, loss 0.266439, acc 0.875\n",
      "2019-06-29T16:28:37.675920: step 1261, loss 0.191952, acc 0.921875\n",
      "2019-06-29T16:28:37.961520: step 1262, loss 0.21893, acc 0.921875\n",
      "2019-06-29T16:28:38.229962: step 1263, loss 0.222704, acc 0.90625\n",
      "2019-06-29T16:28:38.496470: step 1264, loss 0.226398, acc 0.921875\n",
      "2019-06-29T16:28:38.780037: step 1265, loss 0.303973, acc 0.875\n",
      "2019-06-29T16:28:39.127127: step 1266, loss 0.247861, acc 0.90625\n",
      "2019-06-29T16:28:39.406887: step 1267, loss 0.289903, acc 0.84375\n",
      "2019-06-29T16:28:39.675029: step 1268, loss 0.301036, acc 0.84375\n",
      "2019-06-29T16:28:39.962670: step 1269, loss 0.240562, acc 0.875\n",
      "2019-06-29T16:28:40.294848: step 1270, loss 0.214538, acc 0.921875\n",
      "2019-06-29T16:28:40.578125: step 1271, loss 0.156808, acc 0.953125\n",
      "2019-06-29T16:28:40.894306: step 1272, loss 0.230595, acc 0.9375\n",
      "2019-06-29T16:28:41.179196: step 1273, loss 0.458565, acc 0.84375\n",
      "2019-06-29T16:28:41.449839: step 1274, loss 0.347445, acc 0.875\n",
      "2019-06-29T16:28:41.744445: step 1275, loss 0.301965, acc 0.875\n",
      "2019-06-29T16:28:42.059882: step 1276, loss 0.312187, acc 0.859375\n",
      "2019-06-29T16:28:42.345819: step 1277, loss 0.260872, acc 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-29T16:28:42.629704: step 1278, loss 0.340661, acc 0.859375\n",
      "2019-06-29T16:28:42.878754: step 1279, loss 0.288268, acc 0.875\n",
      "2019-06-29T16:28:43.151390: step 1280, loss 0.264738, acc 0.859375\n",
      "2019-06-29T16:28:43.412760: step 1281, loss 0.439167, acc 0.78125\n",
      "2019-06-29T16:28:43.693025: step 1282, loss 0.276797, acc 0.90625\n",
      "2019-06-29T16:28:43.945015: step 1283, loss 0.24688, acc 0.90625\n",
      "2019-06-29T16:28:44.227918: step 1284, loss 0.279383, acc 0.890625\n",
      "2019-06-29T16:28:44.492430: step 1285, loss 0.397697, acc 0.875\n",
      "2019-06-29T16:28:44.753794: step 1286, loss 0.18899, acc 0.90625\n",
      "2019-06-29T16:28:45.019629: step 1287, loss 0.30103, acc 0.9375\n",
      "2019-06-29T16:28:45.296604: step 1288, loss 0.222863, acc 0.90625\n",
      "2019-06-29T16:28:45.563515: step 1289, loss 0.155298, acc 0.953125\n",
      "2019-06-29T16:28:45.828170: step 1290, loss 0.248949, acc 0.859375\n",
      "2019-06-29T16:28:46.094809: step 1291, loss 0.215016, acc 0.875\n",
      "2019-06-29T16:28:46.362342: step 1292, loss 0.265657, acc 0.90625\n",
      "2019-06-29T16:28:46.628937: step 1293, loss 0.275936, acc 0.875\n",
      "2019-06-29T16:28:46.879881: step 1294, loss 0.224831, acc 0.921875\n",
      "2019-06-29T16:28:47.144389: step 1295, loss 0.224481, acc 0.90625\n",
      "2019-06-29T16:28:47.412050: step 1296, loss 0.180393, acc 0.921875\n",
      "2019-06-29T16:28:47.679275: step 1297, loss 0.205628, acc 0.890625\n",
      "2019-06-29T16:28:47.951802: step 1298, loss 0.245415, acc 0.859375\n",
      "2019-06-29T16:28:48.211709: step 1299, loss 0.258121, acc 0.84375\n",
      "2019-06-29T16:28:48.471677: step 1300, loss 0.338111, acc 0.890625\n",
      "\n",
      "Evaluation:\n",
      "2019-06-29T16:28:49.080955: step 1300, loss 0.60994, acc 0.721388\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\mao\\Desktop\\Tensorflow\\CNN\\1\\cnn-text-classification-tf-master\\runs\\1561796526\\checkpoints\\model-1300\n",
      "\n",
      "2019-06-29T16:28:50.179563: step 1301, loss 0.211692, acc 0.90625\n",
      "2019-06-29T16:28:50.677702: step 1302, loss 0.206686, acc 0.890625\n",
      "2019-06-29T16:28:50.982843: step 1303, loss 0.295042, acc 0.875\n",
      "2019-06-29T16:28:51.281941: step 1304, loss 0.279804, acc 0.828125\n",
      "2019-06-29T16:28:51.576712: step 1305, loss 0.261756, acc 0.875\n",
      "2019-06-29T16:28:51.844319: step 1306, loss 0.289877, acc 0.90625\n",
      "2019-06-29T16:28:52.128612: step 1307, loss 0.274527, acc 0.859375\n",
      "2019-06-29T16:28:52.410449: step 1308, loss 0.310911, acc 0.875\n",
      "2019-06-29T16:28:52.693775: step 1309, loss 0.26047, acc 0.90625\n",
      "2019-06-29T16:28:52.976154: step 1310, loss 0.185545, acc 0.953125\n",
      "2019-06-29T16:28:53.246233: step 1311, loss 0.37353, acc 0.84375\n",
      "2019-06-29T16:28:53.554206: step 1312, loss 0.253433, acc 0.921875\n",
      "2019-06-29T16:28:53.873433: step 1313, loss 0.242064, acc 0.90625\n",
      "2019-06-29T16:28:54.161638: step 1314, loss 0.232257, acc 0.9375\n",
      "2019-06-29T16:28:54.448012: step 1315, loss 0.146994, acc 0.9375\n",
      "2019-06-29T16:28:54.805276: step 1316, loss 0.267678, acc 0.921875\n",
      "2019-06-29T16:28:55.230541: step 1317, loss 0.343146, acc 0.859375\n",
      "2019-06-29T16:28:55.523750: step 1318, loss 0.2658, acc 0.890625\n",
      "2019-06-29T16:28:55.819821: step 1319, loss 0.293253, acc 0.859375\n",
      "2019-06-29T16:28:56.138187: step 1320, loss 0.301594, acc 0.859375\n",
      "2019-06-29T16:28:56.528500: step 1321, loss 0.409997, acc 0.78125\n",
      "2019-06-29T16:28:56.804334: step 1322, loss 0.182551, acc 0.9375\n",
      "2019-06-29T16:28:57.081161: step 1323, loss 0.213549, acc 0.90625\n",
      "2019-06-29T16:28:57.490632: step 1324, loss 0.335687, acc 0.8125\n",
      "2019-06-29T16:28:57.792920: step 1325, loss 0.288631, acc 0.890625\n",
      "2019-06-29T16:28:58.140495: step 1326, loss 0.293474, acc 0.875\n",
      "2019-06-29T16:28:58.448104: step 1327, loss 0.205536, acc 0.90625\n",
      "2019-06-29T16:28:58.774964: step 1328, loss 0.240888, acc 0.875\n",
      "2019-06-29T16:28:59.124451: step 1329, loss 0.249407, acc 0.890625\n",
      "2019-06-29T16:28:59.393934: step 1330, loss 0.208045, acc 0.921875\n",
      "2019-06-29T16:28:59.735177: step 1331, loss 0.197193, acc 0.921875\n",
      "2019-06-29T16:29:00.015176: step 1332, loss 0.29091, acc 0.84375\n",
      "2019-06-29T16:29:00.329894: step 1333, loss 0.344091, acc 0.875\n",
      "2019-06-29T16:29:00.685146: step 1334, loss 0.266099, acc 0.921875\n",
      "2019-06-29T16:29:01.019384: step 1335, loss 0.283026, acc 0.84375\n",
      "2019-06-29T16:29:01.351620: step 1336, loss 0.204255, acc 0.9375\n",
      "2019-06-29T16:29:01.621183: step 1337, loss 0.232247, acc 0.9375\n",
      "2019-06-29T16:29:01.962425: step 1338, loss 0.239578, acc 0.90625\n",
      "2019-06-29T16:29:02.233778: step 1339, loss 0.199103, acc 0.953125\n",
      "2019-06-29T16:29:02.504971: step 1340, loss 0.324516, acc 0.828125\n",
      "2019-06-29T16:29:02.771849: step 1341, loss 0.193368, acc 0.9375\n",
      "2019-06-29T16:29:03.090147: step 1342, loss 0.286217, acc 0.890625\n",
      "2019-06-29T16:29:03.357638: step 1343, loss 0.224964, acc 0.921875\n",
      "2019-06-29T16:29:03.629833: step 1344, loss 0.270133, acc 0.9375\n",
      "2019-06-29T16:29:03.894947: step 1345, loss 0.331882, acc 0.890625\n",
      "2019-06-29T16:29:04.165855: step 1346, loss 0.182327, acc 0.921875\n",
      "2019-06-29T16:29:04.429005: step 1347, loss 0.195944, acc 0.890625\n",
      "2019-06-29T16:29:04.751235: step 1348, loss 0.396951, acc 0.828125\n",
      "2019-06-29T16:29:05.018424: step 1349, loss 0.214427, acc 0.890625\n",
      "2019-06-29T16:29:05.282878: step 1350, loss 0.277359, acc 0.9\n",
      "2019-06-29T16:29:05.545194: step 1351, loss 0.184434, acc 0.953125\n",
      "2019-06-29T16:29:05.818037: step 1352, loss 0.0922678, acc 0.984375\n",
      "2019-06-29T16:29:06.092997: step 1353, loss 0.148804, acc 0.96875\n",
      "2019-06-29T16:29:06.360914: step 1354, loss 0.211278, acc 0.90625\n",
      "2019-06-29T16:29:06.657667: step 1355, loss 0.187803, acc 0.90625\n",
      "2019-06-29T16:29:06.954877: step 1356, loss 0.121325, acc 0.96875\n",
      "2019-06-29T16:29:07.262285: step 1357, loss 0.322813, acc 0.84375\n",
      "2019-06-29T16:29:07.542969: step 1358, loss 0.330748, acc 0.859375\n",
      "2019-06-29T16:29:07.836915: step 1359, loss 0.171336, acc 0.921875\n",
      "2019-06-29T16:29:08.109111: step 1360, loss 0.162598, acc 0.9375\n",
      "2019-06-29T16:29:08.394635: step 1361, loss 0.21321, acc 0.875\n",
      "2019-06-29T16:29:08.712623: step 1362, loss 0.159623, acc 0.921875\n",
      "2019-06-29T16:29:08.999738: step 1363, loss 0.169628, acc 0.921875\n",
      "2019-06-29T16:29:09.277841: step 1364, loss 0.147369, acc 0.96875\n",
      "2019-06-29T16:29:09.563086: step 1365, loss 0.180274, acc 0.9375\n",
      "2019-06-29T16:29:09.844165: step 1366, loss 0.124455, acc 0.953125\n",
      "2019-06-29T16:29:10.114595: step 1367, loss 0.252354, acc 0.859375\n",
      "2019-06-29T16:29:10.392055: step 1368, loss 0.271125, acc 0.875\n",
      "2019-06-29T16:29:10.667735: step 1369, loss 0.284742, acc 0.921875\n",
      "2019-06-29T16:29:10.940073: step 1370, loss 0.202848, acc 0.890625\n",
      "2019-06-29T16:29:11.233750: step 1371, loss 0.11898, acc 0.953125\n",
      "2019-06-29T16:29:11.501412: step 1372, loss 0.218457, acc 0.890625\n",
      "2019-06-29T16:29:11.807071: step 1373, loss 0.156048, acc 0.9375\n",
      "2019-06-29T16:29:12.094276: step 1374, loss 0.179131, acc 0.921875\n",
      "2019-06-29T16:29:12.374926: step 1375, loss 0.139701, acc 0.96875\n",
      "2019-06-29T16:29:12.662958: step 1376, loss 0.226721, acc 0.890625\n",
      "2019-06-29T16:29:12.947941: step 1377, loss 0.185839, acc 0.953125\n",
      "2019-06-29T16:29:13.228256: step 1378, loss 0.241091, acc 0.875\n",
      "2019-06-29T16:29:13.510928: step 1379, loss 0.218627, acc 0.9375\n",
      "2019-06-29T16:29:13.802000: step 1380, loss 0.135188, acc 0.96875\n",
      "2019-06-29T16:29:14.077865: step 1381, loss 0.167827, acc 0.921875\n",
      "2019-06-29T16:29:14.356760: step 1382, loss 0.147663, acc 0.9375\n",
      "2019-06-29T16:29:14.626664: step 1383, loss 0.177832, acc 0.9375\n",
      "2019-06-29T16:29:14.910494: step 1384, loss 0.149262, acc 0.953125\n",
      "2019-06-29T16:29:15.174099: step 1385, loss 0.221434, acc 0.90625\n",
      "2019-06-29T16:29:15.442289: step 1386, loss 0.0974616, acc 0.984375\n",
      "2019-06-29T16:29:15.701967: step 1387, loss 0.185216, acc 0.90625\n",
      "2019-06-29T16:29:15.968795: step 1388, loss 0.248124, acc 0.90625\n",
      "2019-06-29T16:29:16.239048: step 1389, loss 0.21065, acc 0.9375\n",
      "2019-06-29T16:29:16.515245: step 1390, loss 0.154712, acc 0.953125\n",
      "2019-06-29T16:29:16.779401: step 1391, loss 0.142631, acc 0.953125\n",
      "2019-06-29T16:29:17.045624: step 1392, loss 0.252113, acc 0.921875\n",
      "2019-06-29T16:29:17.311856: step 1393, loss 0.231191, acc 0.90625\n",
      "2019-06-29T16:29:17.578222: step 1394, loss 0.182428, acc 0.921875\n",
      "2019-06-29T16:29:17.838837: step 1395, loss 0.239102, acc 0.890625\n",
      "2019-06-29T16:29:18.108724: step 1396, loss 0.192539, acc 0.890625\n",
      "2019-06-29T16:29:18.377087: step 1397, loss 0.152411, acc 0.953125\n",
      "2019-06-29T16:29:18.636796: step 1398, loss 0.136904, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-29T16:29:18.894231: step 1399, loss 0.159362, acc 0.96875\n",
      "2019-06-29T16:29:19.159726: step 1400, loss 0.146103, acc 0.890625\n",
      "\n",
      "Evaluation:\n",
      "2019-06-29T16:29:19.777021: step 1400, loss 0.637681, acc 0.692308\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\mao\\Desktop\\Tensorflow\\CNN\\1\\cnn-text-classification-tf-master\\runs\\1561796526\\checkpoints\\model-1400\n",
      "\n",
      "2019-06-29T16:29:20.880824: step 1401, loss 0.218003, acc 0.921875\n",
      "2019-06-29T16:29:21.302719: step 1402, loss 0.167822, acc 0.96875\n",
      "2019-06-29T16:29:21.561715: step 1403, loss 0.256695, acc 0.9375\n",
      "2019-06-29T16:29:21.842736: step 1404, loss 0.148818, acc 0.9375\n",
      "2019-06-29T16:29:22.094809: step 1405, loss 0.16038, acc 0.921875\n",
      "2019-06-29T16:29:22.373658: step 1406, loss 0.263013, acc 0.859375\n",
      "2019-06-29T16:29:22.658194: step 1407, loss 0.267715, acc 0.875\n",
      "2019-06-29T16:29:22.956056: step 1408, loss 0.158365, acc 0.9375\n",
      "2019-06-29T16:29:23.244493: step 1409, loss 0.291937, acc 0.84375\n",
      "2019-06-29T16:29:23.539775: step 1410, loss 0.202619, acc 0.921875\n",
      "2019-06-29T16:29:23.811909: step 1411, loss 0.122731, acc 0.96875\n",
      "2019-06-29T16:29:24.084334: step 1412, loss 0.218182, acc 0.890625\n",
      "2019-06-29T16:29:24.360116: step 1413, loss 0.38451, acc 0.828125\n",
      "2019-06-29T16:29:24.658052: step 1414, loss 0.175045, acc 0.9375\n",
      "2019-06-29T16:29:24.930596: step 1415, loss 0.264571, acc 0.875\n",
      "2019-06-29T16:29:25.209280: step 1416, loss 0.238997, acc 0.90625\n",
      "2019-06-29T16:29:25.477116: step 1417, loss 0.12924, acc 0.96875\n",
      "2019-06-29T16:29:25.774101: step 1418, loss 0.110969, acc 0.984375\n",
      "2019-06-29T16:29:26.059698: step 1419, loss 0.138379, acc 0.953125\n",
      "2019-06-29T16:29:26.332346: step 1420, loss 0.206564, acc 0.9375\n",
      "2019-06-29T16:29:26.609320: step 1421, loss 0.156008, acc 0.953125\n",
      "2019-06-29T16:29:26.897524: step 1422, loss 0.296058, acc 0.84375\n",
      "2019-06-29T16:29:27.175654: step 1423, loss 0.159318, acc 0.9375\n",
      "2019-06-29T16:29:27.473718: step 1424, loss 0.185079, acc 0.921875\n",
      "2019-06-29T16:29:27.743785: step 1425, loss 0.245096, acc 0.90625\n",
      "2019-06-29T16:29:28.048006: step 1426, loss 0.149253, acc 0.953125\n",
      "2019-06-29T16:29:28.310903: step 1427, loss 0.193034, acc 0.90625\n",
      "2019-06-29T16:29:28.588878: step 1428, loss 0.173275, acc 0.953125\n",
      "2019-06-29T16:29:28.875170: step 1429, loss 0.118425, acc 0.96875\n",
      "2019-06-29T16:29:29.153721: step 1430, loss 0.136561, acc 0.953125\n",
      "2019-06-29T16:29:29.426603: step 1431, loss 0.204649, acc 0.921875\n",
      "2019-06-29T16:29:29.715840: step 1432, loss 0.282323, acc 0.859375\n",
      "2019-06-29T16:29:30.003161: step 1433, loss 0.112517, acc 0.96875\n",
      "2019-06-29T16:29:30.295370: step 1434, loss 0.180276, acc 0.90625\n",
      "2019-06-29T16:29:30.575568: step 1435, loss 0.124319, acc 0.96875\n",
      "2019-06-29T16:29:30.841764: step 1436, loss 0.183302, acc 0.90625\n",
      "2019-06-29T16:29:31.180005: step 1437, loss 0.273179, acc 0.859375\n",
      "2019-06-29T16:29:31.454827: step 1438, loss 0.208425, acc 0.921875\n",
      "2019-06-29T16:29:31.778271: step 1439, loss 0.239013, acc 0.921875\n",
      "2019-06-29T16:29:32.049691: step 1440, loss 0.215803, acc 0.9375\n",
      "2019-06-29T16:29:32.350226: step 1441, loss 0.156912, acc 0.9375\n",
      "2019-06-29T16:29:32.621192: step 1442, loss 0.118964, acc 0.953125\n",
      "2019-06-29T16:29:32.878065: step 1443, loss 0.283, acc 0.890625\n",
      "2019-06-29T16:29:33.143908: step 1444, loss 0.355463, acc 0.875\n",
      "2019-06-29T16:29:33.427139: step 1445, loss 0.124564, acc 0.984375\n",
      "2019-06-29T16:29:33.692358: step 1446, loss 0.2539, acc 0.90625\n",
      "2019-06-29T16:29:33.977670: step 1447, loss 0.239794, acc 0.890625\n",
      "2019-06-29T16:29:34.264466: step 1448, loss 0.216134, acc 0.921875\n",
      "2019-06-29T16:29:34.601707: step 1449, loss 0.206213, acc 0.90625\n",
      "2019-06-29T16:29:34.896916: step 1450, loss 0.152825, acc 0.9375\n",
      "2019-06-29T16:29:35.168548: step 1451, loss 0.21245, acc 0.890625\n",
      "2019-06-29T16:29:35.438354: step 1452, loss 0.0891053, acc 0.984375\n",
      "2019-06-29T16:29:35.704851: step 1453, loss 0.185208, acc 0.953125\n",
      "2019-06-29T16:29:35.971451: step 1454, loss 0.265911, acc 0.90625\n",
      "2019-06-29T16:29:36.223945: step 1455, loss 0.255711, acc 0.921875\n",
      "2019-06-29T16:29:36.512021: step 1456, loss 0.292917, acc 0.859375\n",
      "2019-06-29T16:29:36.798629: step 1457, loss 0.166504, acc 0.96875\n",
      "2019-06-29T16:29:37.080493: step 1458, loss 0.227732, acc 0.921875\n",
      "2019-06-29T16:29:37.369699: step 1459, loss 0.200345, acc 0.90625\n",
      "2019-06-29T16:29:37.661464: step 1460, loss 0.30959, acc 0.875\n",
      "2019-06-29T16:29:37.949607: step 1461, loss 0.164224, acc 0.953125\n",
      "2019-06-29T16:29:38.232348: step 1462, loss 0.169121, acc 0.921875\n",
      "2019-06-29T16:29:38.523764: step 1463, loss 0.265419, acc 0.890625\n",
      "2019-06-29T16:29:38.897425: step 1464, loss 0.243997, acc 0.859375\n",
      "2019-06-29T16:29:39.224991: step 1465, loss 0.280357, acc 0.859375\n",
      "2019-06-29T16:29:39.544681: step 1466, loss 0.126395, acc 0.9375\n",
      "2019-06-29T16:29:39.820112: step 1467, loss 0.309674, acc 0.890625\n",
      "2019-06-29T16:29:40.108987: step 1468, loss 0.190642, acc 0.921875\n",
      "2019-06-29T16:29:40.393868: step 1469, loss 0.269453, acc 0.875\n",
      "2019-06-29T16:29:40.709039: step 1470, loss 0.134805, acc 0.9375\n",
      "2019-06-29T16:29:41.008415: step 1471, loss 0.236387, acc 0.9375\n",
      "2019-06-29T16:29:41.277182: step 1472, loss 0.349827, acc 0.90625\n",
      "2019-06-29T16:29:41.568043: step 1473, loss 0.252632, acc 0.90625\n",
      "2019-06-29T16:29:41.849962: step 1474, loss 0.177082, acc 0.921875\n",
      "2019-06-29T16:29:42.127805: step 1475, loss 0.195945, acc 0.921875\n",
      "2019-06-29T16:29:42.409954: step 1476, loss 0.0870905, acc 0.96875\n",
      "2019-06-29T16:29:42.682019: step 1477, loss 0.112506, acc 0.96875\n",
      "2019-06-29T16:29:42.958844: step 1478, loss 0.253581, acc 0.921875\n",
      "2019-06-29T16:29:43.244049: step 1479, loss 0.241797, acc 0.875\n",
      "2019-06-29T16:29:43.515592: step 1480, loss 0.376031, acc 0.84375\n",
      "2019-06-29T16:29:43.792642: step 1481, loss 0.160123, acc 0.953125\n",
      "2019-06-29T16:29:44.077680: step 1482, loss 0.229469, acc 0.921875\n",
      "2019-06-29T16:29:44.383154: step 1483, loss 0.135362, acc 0.96875\n",
      "2019-06-29T16:29:44.660509: step 1484, loss 0.163535, acc 0.9375\n",
      "2019-06-29T16:29:44.943384: step 1485, loss 0.142444, acc 0.9375\n",
      "2019-06-29T16:29:45.227053: step 1486, loss 0.202665, acc 0.921875\n",
      "2019-06-29T16:29:45.538814: step 1487, loss 0.186674, acc 0.921875\n",
      "2019-06-29T16:29:45.810179: step 1488, loss 0.201071, acc 0.890625\n",
      "2019-06-29T16:29:46.092719: step 1489, loss 0.168115, acc 0.953125\n",
      "2019-06-29T16:29:46.375328: step 1490, loss 0.132707, acc 0.953125\n",
      "2019-06-29T16:29:46.653677: step 1491, loss 0.164352, acc 0.9375\n",
      "2019-06-29T16:29:46.927751: step 1492, loss 0.209829, acc 0.90625\n",
      "2019-06-29T16:29:47.193502: step 1493, loss 0.285117, acc 0.890625\n",
      "2019-06-29T16:29:47.443778: step 1494, loss 0.181306, acc 0.90625\n",
      "2019-06-29T16:29:47.742095: step 1495, loss 0.151388, acc 0.9375\n",
      "2019-06-29T16:29:48.010975: step 1496, loss 0.173826, acc 0.9375\n",
      "2019-06-29T16:29:48.280416: step 1497, loss 0.292499, acc 0.859375\n",
      "2019-06-29T16:29:48.544267: step 1498, loss 0.257333, acc 0.890625\n",
      "2019-06-29T16:29:48.826815: step 1499, loss 0.29141, acc 0.859375\n",
      "2019-06-29T16:29:49.087005: step 1500, loss 0.186378, acc 0.933333\n",
      "\n",
      "Evaluation:\n",
      "2019-06-29T16:29:49.709605: step 1500, loss 0.625076, acc 0.713884\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\mao\\Desktop\\Tensorflow\\CNN\\1\\cnn-text-classification-tf-master\\runs\\1561796526\\checkpoints\\model-1500\n",
      "\n",
      "2019-06-29T16:29:50.802846: step 1501, loss 0.11564, acc 0.96875\n",
      "2019-06-29T16:29:51.210489: step 1502, loss 0.144635, acc 0.921875\n",
      "2019-06-29T16:29:51.482782: step 1503, loss 0.113523, acc 0.9375\n",
      "2019-06-29T16:29:51.745331: step 1504, loss 0.0921989, acc 0.953125\n",
      "2019-06-29T16:29:52.010477: step 1505, loss 0.225402, acc 0.921875\n",
      "2019-06-29T16:29:52.271916: step 1506, loss 0.11554, acc 0.953125\n",
      "2019-06-29T16:29:52.538223: step 1507, loss 0.173706, acc 0.9375\n",
      "2019-06-29T16:29:52.794205: step 1508, loss 0.148228, acc 0.90625\n",
      "2019-06-29T16:29:53.069816: step 1509, loss 0.121358, acc 0.953125\n",
      "2019-06-29T16:29:53.326606: step 1510, loss 0.120198, acc 0.953125\n",
      "2019-06-29T16:29:53.598420: step 1511, loss 0.0878464, acc 0.984375\n",
      "2019-06-29T16:29:53.860627: step 1512, loss 0.112428, acc 0.9375\n",
      "2019-06-29T16:29:54.134204: step 1513, loss 0.227063, acc 0.90625\n",
      "2019-06-29T16:29:54.393596: step 1514, loss 0.177266, acc 0.921875\n",
      "2019-06-29T16:29:54.677232: step 1515, loss 0.168933, acc 0.96875\n",
      "2019-06-29T16:29:54.974118: step 1516, loss 0.319931, acc 0.859375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-29T16:29:55.265550: step 1517, loss 0.144781, acc 0.921875\n",
      "2019-06-29T16:29:55.542877: step 1518, loss 0.216226, acc 0.9375\n",
      "2019-06-29T16:29:55.824265: step 1519, loss 0.179558, acc 0.921875\n",
      "2019-06-29T16:29:56.108177: step 1520, loss 0.212506, acc 0.875\n",
      "2019-06-29T16:29:56.374355: step 1521, loss 0.140707, acc 0.9375\n",
      "2019-06-29T16:29:56.659521: step 1522, loss 0.166789, acc 0.921875\n",
      "2019-06-29T16:29:56.924833: step 1523, loss 0.230473, acc 0.921875\n",
      "2019-06-29T16:29:57.202580: step 1524, loss 0.204006, acc 0.921875\n",
      "2019-06-29T16:29:57.474455: step 1525, loss 0.21735, acc 0.9375\n",
      "2019-06-29T16:29:57.742805: step 1526, loss 0.0984929, acc 0.96875\n",
      "2019-06-29T16:29:58.009562: step 1527, loss 0.106605, acc 0.96875\n",
      "2019-06-29T16:29:58.290327: step 1528, loss 0.227377, acc 0.890625\n",
      "2019-06-29T16:29:58.558608: step 1529, loss 0.120543, acc 0.96875\n",
      "2019-06-29T16:29:58.827105: step 1530, loss 0.161455, acc 0.9375\n",
      "2019-06-29T16:29:59.108500: step 1531, loss 0.102463, acc 0.984375\n",
      "2019-06-29T16:29:59.391390: step 1532, loss 0.147406, acc 0.921875\n",
      "2019-06-29T16:29:59.674913: step 1533, loss 0.185745, acc 0.921875\n",
      "2019-06-29T16:29:59.947966: step 1534, loss 0.192833, acc 0.9375\n",
      "2019-06-29T16:30:00.225454: step 1535, loss 0.136608, acc 0.96875\n",
      "2019-06-29T16:30:00.506846: step 1536, loss 0.133642, acc 0.9375\n",
      "2019-06-29T16:30:00.806838: step 1537, loss 0.113554, acc 0.9375\n",
      "2019-06-29T16:30:01.091975: step 1538, loss 0.170755, acc 0.953125\n",
      "2019-06-29T16:30:01.360522: step 1539, loss 0.147977, acc 0.90625\n",
      "2019-06-29T16:30:01.640873: step 1540, loss 0.120178, acc 0.953125\n",
      "2019-06-29T16:30:01.940131: step 1541, loss 0.0924905, acc 0.96875\n",
      "2019-06-29T16:30:02.223883: step 1542, loss 0.121955, acc 0.9375\n",
      "2019-06-29T16:30:02.490039: step 1543, loss 0.192428, acc 0.953125\n",
      "2019-06-29T16:30:02.743121: step 1544, loss 0.0976222, acc 0.984375\n",
      "2019-06-29T16:30:03.026992: step 1545, loss 0.124934, acc 0.953125\n",
      "2019-06-29T16:30:03.306619: step 1546, loss 0.210958, acc 0.90625\n",
      "2019-06-29T16:30:03.560317: step 1547, loss 0.273947, acc 0.921875\n",
      "2019-06-29T16:30:03.825277: step 1548, loss 0.0719982, acc 0.984375\n",
      "2019-06-29T16:30:04.093862: step 1549, loss 0.158422, acc 0.9375\n",
      "2019-06-29T16:30:04.358949: step 1550, loss 0.122165, acc 0.9375\n",
      "2019-06-29T16:30:04.626880: step 1551, loss 0.157936, acc 0.921875\n",
      "2019-06-29T16:30:04.876337: step 1552, loss 0.197685, acc 0.921875\n",
      "2019-06-29T16:30:05.160014: step 1553, loss 0.236787, acc 0.890625\n",
      "2019-06-29T16:30:05.425980: step 1554, loss 0.191828, acc 0.921875\n",
      "2019-06-29T16:30:05.691613: step 1555, loss 0.105897, acc 0.96875\n",
      "2019-06-29T16:30:05.943318: step 1556, loss 0.0969856, acc 0.96875\n",
      "2019-06-29T16:30:06.216254: step 1557, loss 0.0498116, acc 1\n",
      "2019-06-29T16:30:06.476790: step 1558, loss 0.148514, acc 0.953125\n",
      "2019-06-29T16:30:06.741687: step 1559, loss 0.0797326, acc 0.984375\n",
      "2019-06-29T16:30:07.009299: step 1560, loss 0.137033, acc 0.953125\n",
      "2019-06-29T16:30:07.271756: step 1561, loss 0.103158, acc 0.96875\n",
      "2019-06-29T16:30:07.541920: step 1562, loss 0.227671, acc 0.921875\n",
      "2019-06-29T16:30:07.814308: step 1563, loss 0.108239, acc 0.96875\n",
      "2019-06-29T16:30:08.089025: step 1564, loss 0.0802949, acc 0.96875\n",
      "2019-06-29T16:30:08.367223: step 1565, loss 0.17398, acc 0.921875\n",
      "2019-06-29T16:30:08.632411: step 1566, loss 0.157399, acc 0.953125\n",
      "2019-06-29T16:30:08.896716: step 1567, loss 0.0892388, acc 0.96875\n",
      "2019-06-29T16:30:09.167909: step 1568, loss 0.106051, acc 0.96875\n",
      "2019-06-29T16:30:09.434099: step 1569, loss 0.281413, acc 0.921875\n",
      "2019-06-29T16:30:09.705344: step 1570, loss 0.17913, acc 0.921875\n",
      "2019-06-29T16:30:09.974043: step 1571, loss 0.100041, acc 0.984375\n",
      "2019-06-29T16:30:10.247983: step 1572, loss 0.123571, acc 0.984375\n",
      "2019-06-29T16:30:10.524525: step 1573, loss 0.215002, acc 0.890625\n",
      "2019-06-29T16:30:10.828439: step 1574, loss 0.144455, acc 0.96875\n",
      "2019-06-29T16:30:11.123819: step 1575, loss 0.145, acc 0.9375\n",
      "2019-06-29T16:30:11.428325: step 1576, loss 0.203779, acc 0.921875\n",
      "2019-06-29T16:30:11.714036: step 1577, loss 0.100409, acc 0.984375\n",
      "2019-06-29T16:30:11.997237: step 1578, loss 0.204632, acc 0.953125\n",
      "2019-06-29T16:30:12.275027: step 1579, loss 0.158983, acc 0.9375\n",
      "2019-06-29T16:30:12.553065: step 1580, loss 0.145973, acc 0.9375\n",
      "2019-06-29T16:30:12.860912: step 1581, loss 0.088736, acc 0.96875\n",
      "2019-06-29T16:30:13.126358: step 1582, loss 0.0958628, acc 0.953125\n",
      "2019-06-29T16:30:13.409028: step 1583, loss 0.182817, acc 0.921875\n",
      "2019-06-29T16:30:13.725410: step 1584, loss 0.169243, acc 0.9375\n",
      "2019-06-29T16:30:14.020209: step 1585, loss 0.208778, acc 0.921875\n",
      "2019-06-29T16:30:14.309415: step 1586, loss 0.0886668, acc 0.984375\n",
      "2019-06-29T16:30:14.594618: step 1587, loss 0.176562, acc 0.9375\n",
      "2019-06-29T16:30:14.879027: step 1588, loss 0.172509, acc 0.921875\n",
      "2019-06-29T16:30:15.162228: step 1589, loss 0.0883631, acc 0.96875\n",
      "2019-06-29T16:30:15.498898: step 1590, loss 0.0728778, acc 0.984375\n",
      "2019-06-29T16:30:15.869161: step 1591, loss 0.135553, acc 0.9375\n",
      "2019-06-29T16:30:16.177160: step 1592, loss 0.242212, acc 0.9375\n",
      "2019-06-29T16:30:16.458215: step 1593, loss 0.151072, acc 0.9375\n",
      "2019-06-29T16:30:16.741557: step 1594, loss 0.2062, acc 0.9375\n",
      "2019-06-29T16:30:17.025165: step 1595, loss 0.201315, acc 0.9375\n",
      "2019-06-29T16:30:17.331693: step 1596, loss 0.0912818, acc 0.96875\n",
      "2019-06-29T16:30:17.644918: step 1597, loss 0.102139, acc 0.9375\n",
      "2019-06-29T16:30:17.951554: step 1598, loss 0.150646, acc 0.9375\n",
      "2019-06-29T16:30:18.278415: step 1599, loss 0.163511, acc 0.9375\n",
      "2019-06-29T16:30:18.597641: step 1600, loss 0.196456, acc 0.953125\n",
      "\n",
      "Evaluation:\n",
      "2019-06-29T16:30:19.364945: step 1600, loss 0.648834, acc 0.716698\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\mao\\Desktop\\Tensorflow\\CNN\\1\\cnn-text-classification-tf-master\\runs\\1561796526\\checkpoints\\model-1600\n",
      "\n",
      "2019-06-29T16:30:20.740925: step 1601, loss 0.123544, acc 0.953125\n",
      "2019-06-29T16:30:21.098178: step 1602, loss 0.166619, acc 0.953125\n",
      "2019-06-29T16:30:21.396392: step 1603, loss 0.147909, acc 0.953125\n",
      "2019-06-29T16:30:21.708532: step 1604, loss 0.0876143, acc 0.984375\n",
      "2019-06-29T16:30:21.982744: step 1605, loss 0.140547, acc 0.921875\n",
      "2019-06-29T16:30:22.244910: step 1606, loss 0.218617, acc 0.875\n",
      "2019-06-29T16:30:22.525599: step 1607, loss 0.166894, acc 0.9375\n",
      "2019-06-29T16:30:22.807897: step 1608, loss 0.194482, acc 0.921875\n",
      "2019-06-29T16:30:23.092080: step 1609, loss 0.265675, acc 0.890625\n",
      "2019-06-29T16:30:23.367276: step 1610, loss 0.14892, acc 0.953125\n",
      "2019-06-29T16:30:23.635467: step 1611, loss 0.197987, acc 0.953125\n",
      "2019-06-29T16:30:23.923672: step 1612, loss 0.283069, acc 0.875\n",
      "2019-06-29T16:30:24.199240: step 1613, loss 0.314456, acc 0.875\n",
      "2019-06-29T16:30:24.511460: step 1614, loss 0.147197, acc 0.953125\n",
      "2019-06-29T16:30:24.835691: step 1615, loss 0.160387, acc 0.9375\n",
      "2019-06-29T16:30:25.164926: step 1616, loss 0.159078, acc 0.9375\n",
      "2019-06-29T16:30:25.599236: step 1617, loss 0.235749, acc 0.921875\n",
      "2019-06-29T16:30:25.928846: step 1618, loss 0.0577406, acc 0.984375\n",
      "2019-06-29T16:30:26.194081: step 1619, loss 0.114087, acc 0.96875\n",
      "2019-06-29T16:30:26.474283: step 1620, loss 0.23592, acc 0.90625\n",
      "2019-06-29T16:30:26.763118: step 1621, loss 0.0802391, acc 0.984375\n",
      "2019-06-29T16:30:27.065581: step 1622, loss 0.124229, acc 0.921875\n",
      "2019-06-29T16:30:27.660664: step 1623, loss 0.250085, acc 0.890625\n",
      "2019-06-29T16:30:27.919276: step 1624, loss 0.153299, acc 0.9375\n",
      "2019-06-29T16:30:28.321251: step 1625, loss 0.104198, acc 0.96875\n",
      "2019-06-29T16:30:28.655982: step 1626, loss 0.0723302, acc 1\n",
      "2019-06-29T16:30:28.949357: step 1627, loss 0.172072, acc 0.9375\n",
      "2019-06-29T16:30:29.236221: step 1628, loss 0.179929, acc 0.90625\n",
      "2019-06-29T16:30:29.528128: step 1629, loss 0.158889, acc 0.953125\n",
      "2019-06-29T16:30:29.792470: step 1630, loss 0.207292, acc 0.921875\n",
      "2019-06-29T16:30:30.142973: step 1631, loss 0.256153, acc 0.90625\n",
      "2019-06-29T16:30:30.441792: step 1632, loss 0.128407, acc 0.9375\n",
      "2019-06-29T16:30:30.714301: step 1633, loss 0.161912, acc 0.9375\n",
      "2019-06-29T16:30:30.995016: step 1634, loss 0.129282, acc 0.953125\n",
      "2019-06-29T16:30:31.275902: step 1635, loss 0.173876, acc 0.953125\n",
      "2019-06-29T16:30:31.589977: step 1636, loss 0.137265, acc 0.953125\n",
      "2019-06-29T16:30:31.985258: step 1637, loss 0.265435, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-29T16:30:32.284471: step 1638, loss 0.119676, acc 0.921875\n",
      "2019-06-29T16:30:32.722166: step 1639, loss 0.177332, acc 0.921875\n",
      "2019-06-29T16:30:33.013506: step 1640, loss 0.093824, acc 0.96875\n",
      "2019-06-29T16:30:33.374762: step 1641, loss 0.188137, acc 0.921875\n",
      "2019-06-29T16:30:33.685281: step 1642, loss 0.148791, acc 0.953125\n",
      "2019-06-29T16:30:34.060903: step 1643, loss 0.199171, acc 0.9375\n",
      "2019-06-29T16:30:34.371678: step 1644, loss 0.122618, acc 0.9375\n",
      "2019-06-29T16:30:34.735040: step 1645, loss 0.221435, acc 0.890625\n",
      "2019-06-29T16:30:35.029860: step 1646, loss 0.0741295, acc 0.984375\n",
      "2019-06-29T16:30:35.302054: step 1647, loss 0.148575, acc 0.921875\n",
      "2019-06-29T16:30:35.567736: step 1648, loss 0.108726, acc 0.953125\n",
      "2019-06-29T16:30:35.826632: step 1649, loss 0.23468, acc 0.828125\n",
      "2019-06-29T16:30:36.131783: step 1650, loss 0.127548, acc 0.95\n",
      "2019-06-29T16:30:36.412982: step 1651, loss 0.173137, acc 0.953125\n",
      "2019-06-29T16:30:36.673948: step 1652, loss 0.114567, acc 0.9375\n",
      "2019-06-29T16:30:36.945950: step 1653, loss 0.0967864, acc 0.96875\n",
      "2019-06-29T16:30:37.209078: step 1654, loss 0.072981, acc 0.96875\n",
      "2019-06-29T16:30:37.474041: step 1655, loss 0.166567, acc 0.921875\n",
      "2019-06-29T16:30:37.738230: step 1656, loss 0.162573, acc 0.953125\n",
      "2019-06-29T16:30:38.011318: step 1657, loss 0.168376, acc 0.9375\n",
      "2019-06-29T16:30:38.273103: step 1658, loss 0.0434912, acc 0.984375\n",
      "2019-06-29T16:30:38.540033: step 1659, loss 0.0478489, acc 1\n",
      "2019-06-29T16:30:38.810881: step 1660, loss 0.124495, acc 0.953125\n",
      "2019-06-29T16:30:39.082074: step 1661, loss 0.109193, acc 0.96875\n",
      "2019-06-29T16:30:39.356149: step 1662, loss 0.103432, acc 0.96875\n",
      "2019-06-29T16:30:39.677378: step 1663, loss 0.166926, acc 0.9375\n",
      "2019-06-29T16:30:39.941342: step 1664, loss 0.0824707, acc 0.984375\n",
      "2019-06-29T16:30:40.218330: step 1665, loss 0.11027, acc 0.953125\n",
      "2019-06-29T16:30:40.549566: step 1666, loss 0.0548785, acc 1\n",
      "2019-06-29T16:30:40.815133: step 1667, loss 0.122905, acc 0.953125\n",
      "2019-06-29T16:30:41.120989: step 1668, loss 0.0893327, acc 0.953125\n",
      "2019-06-29T16:30:41.415248: step 1669, loss 0.109632, acc 0.9375\n",
      "2019-06-29T16:30:41.681242: step 1670, loss 0.173668, acc 0.953125\n",
      "2019-06-29T16:30:41.946050: step 1671, loss 0.0687201, acc 0.984375\n",
      "2019-06-29T16:30:42.211010: step 1672, loss 0.109463, acc 0.953125\n",
      "2019-06-29T16:30:42.485687: step 1673, loss 0.117454, acc 0.953125\n",
      "2019-06-29T16:30:42.772940: step 1674, loss 0.059975, acc 1\n",
      "2019-06-29T16:30:43.080249: step 1675, loss 0.148408, acc 0.953125\n",
      "2019-06-29T16:30:43.385466: step 1676, loss 0.145211, acc 0.953125\n",
      "2019-06-29T16:30:43.657342: step 1677, loss 0.139523, acc 0.953125\n",
      "2019-06-29T16:30:43.935613: step 1678, loss 0.13011, acc 0.9375\n",
      "2019-06-29T16:30:44.206667: step 1679, loss 0.108301, acc 0.96875\n",
      "2019-06-29T16:30:44.475689: step 1680, loss 0.177487, acc 0.9375\n",
      "2019-06-29T16:30:44.771786: step 1681, loss 0.165298, acc 0.96875\n",
      "2019-06-29T16:30:45.052436: step 1682, loss 0.147341, acc 0.953125\n",
      "2019-06-29T16:30:45.330634: step 1683, loss 0.177828, acc 0.921875\n",
      "2019-06-29T16:30:45.606270: step 1684, loss 0.0439188, acc 1\n",
      "2019-06-29T16:30:45.888568: step 1685, loss 0.123495, acc 0.96875\n",
      "2019-06-29T16:30:46.194786: step 1686, loss 0.241036, acc 0.890625\n",
      "2019-06-29T16:30:46.572002: step 1687, loss 0.166999, acc 0.9375\n",
      "2019-06-29T16:30:46.900434: step 1688, loss 0.141023, acc 0.9375\n",
      "2019-06-29T16:30:47.171936: step 1689, loss 0.188404, acc 0.921875\n",
      "2019-06-29T16:30:47.454740: step 1690, loss 0.0854697, acc 0.984375\n",
      "2019-06-29T16:30:47.742892: step 1691, loss 0.18308, acc 0.9375\n",
      "2019-06-29T16:30:48.022123: step 1692, loss 0.0811103, acc 0.984375\n",
      "2019-06-29T16:30:48.308229: step 1693, loss 0.128955, acc 0.9375\n",
      "2019-06-29T16:30:48.579702: step 1694, loss 0.217499, acc 0.921875\n",
      "2019-06-29T16:30:48.884615: step 1695, loss 0.101297, acc 0.953125\n",
      "2019-06-29T16:30:49.233267: step 1696, loss 0.223565, acc 0.890625\n",
      "2019-06-29T16:30:49.507120: step 1697, loss 0.0664079, acc 0.96875\n",
      "2019-06-29T16:30:49.790840: step 1698, loss 0.144996, acc 0.953125\n",
      "2019-06-29T16:30:50.071328: step 1699, loss 0.093894, acc 0.953125\n",
      "2019-06-29T16:30:50.343522: step 1700, loss 0.139233, acc 0.921875\n",
      "\n",
      "Evaluation:\n",
      "2019-06-29T16:30:51.019007: step 1700, loss 0.678903, acc 0.703565\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\mao\\Desktop\\Tensorflow\\CNN\\1\\cnn-text-classification-tf-master\\runs\\1561796526\\checkpoints\\model-1700\n",
      "\n",
      "2019-06-29T16:30:52.088579: step 1701, loss 0.106971, acc 0.953125\n",
      "2019-06-29T16:30:52.535805: step 1702, loss 0.0698811, acc 0.96875\n",
      "2019-06-29T16:30:52.787946: step 1703, loss 0.0954562, acc 0.984375\n",
      "2019-06-29T16:30:53.056526: step 1704, loss 0.115005, acc 0.96875\n",
      "2019-06-29T16:30:53.373209: step 1705, loss 0.159302, acc 0.9375\n",
      "2019-06-29T16:30:53.640663: step 1706, loss 0.181487, acc 0.9375\n",
      "2019-06-29T16:30:53.909982: step 1707, loss 0.182464, acc 0.890625\n",
      "2019-06-29T16:30:54.216354: step 1708, loss 0.116182, acc 0.953125\n",
      "2019-06-29T16:30:54.491071: step 1709, loss 0.118726, acc 0.953125\n",
      "2019-06-29T16:30:54.751314: step 1710, loss 0.117313, acc 0.96875\n",
      "2019-06-29T16:30:55.025286: step 1711, loss 0.0802862, acc 0.96875\n",
      "2019-06-29T16:30:55.322603: step 1712, loss 0.116655, acc 0.96875\n",
      "2019-06-29T16:30:55.598420: step 1713, loss 0.172782, acc 0.9375\n",
      "2019-06-29T16:30:55.864298: step 1714, loss 0.149786, acc 0.921875\n",
      "2019-06-29T16:30:56.121011: step 1715, loss 0.0870645, acc 0.984375\n",
      "2019-06-29T16:30:56.386359: step 1716, loss 0.104077, acc 0.96875\n",
      "2019-06-29T16:30:56.726602: step 1717, loss 0.0775935, acc 0.984375\n",
      "2019-06-29T16:30:56.992585: step 1718, loss 0.085142, acc 0.984375\n",
      "2019-06-29T16:30:57.311811: step 1719, loss 0.12449, acc 0.953125\n",
      "2019-06-29T16:30:57.574028: step 1720, loss 0.0508739, acc 0.984375\n",
      "2019-06-29T16:30:57.873416: step 1721, loss 0.15114, acc 0.9375\n",
      "2019-06-29T16:30:58.149395: step 1722, loss 0.241608, acc 0.953125\n",
      "2019-06-29T16:30:58.422264: step 1723, loss 0.125231, acc 0.953125\n",
      "2019-06-29T16:30:58.737367: step 1724, loss 0.139578, acc 0.9375\n",
      "2019-06-29T16:30:59.038368: step 1725, loss 0.135353, acc 0.9375\n",
      "2019-06-29T16:30:59.322424: step 1726, loss 0.126872, acc 0.96875\n",
      "2019-06-29T16:30:59.622764: step 1727, loss 0.0608965, acc 0.96875\n",
      "2019-06-29T16:30:59.907839: step 1728, loss 0.123928, acc 0.96875\n",
      "2019-06-29T16:31:00.205608: step 1729, loss 0.143573, acc 0.953125\n",
      "2019-06-29T16:31:00.473886: step 1730, loss 0.180678, acc 0.953125\n",
      "2019-06-29T16:31:00.741061: step 1731, loss 0.135702, acc 0.9375\n",
      "2019-06-29T16:31:01.023445: step 1732, loss 0.121782, acc 0.9375\n",
      "2019-06-29T16:31:01.305371: step 1733, loss 0.163356, acc 0.96875\n",
      "2019-06-29T16:31:01.579434: step 1734, loss 0.0602862, acc 0.984375\n",
      "2019-06-29T16:31:01.857565: step 1735, loss 0.167406, acc 0.921875\n",
      "2019-06-29T16:31:02.146094: step 1736, loss 0.103997, acc 0.953125\n",
      "2019-06-29T16:31:02.423289: step 1737, loss 0.080568, acc 0.953125\n",
      "2019-06-29T16:31:02.707571: step 1738, loss 0.115354, acc 0.953125\n",
      "2019-06-29T16:31:02.974963: step 1739, loss 0.0785549, acc 0.984375\n",
      "2019-06-29T16:31:03.273798: step 1740, loss 0.0658717, acc 0.984375\n",
      "2019-06-29T16:31:03.557904: step 1741, loss 0.168476, acc 0.9375\n",
      "2019-06-29T16:31:03.838815: step 1742, loss 0.0577514, acc 1\n",
      "2019-06-29T16:31:04.107564: step 1743, loss 0.209687, acc 0.921875\n",
      "2019-06-29T16:31:04.423063: step 1744, loss 0.0882553, acc 0.96875\n",
      "2019-06-29T16:31:04.702463: step 1745, loss 0.107083, acc 0.953125\n",
      "2019-06-29T16:31:04.989258: step 1746, loss 0.106839, acc 0.96875\n",
      "2019-06-29T16:31:05.257954: step 1747, loss 0.20009, acc 0.9375\n",
      "2019-06-29T16:31:05.555725: step 1748, loss 0.1277, acc 0.953125\n",
      "2019-06-29T16:31:05.839914: step 1749, loss 0.16095, acc 0.953125\n",
      "2019-06-29T16:31:06.118897: step 1750, loss 0.084594, acc 0.96875\n",
      "2019-06-29T16:31:06.406858: step 1751, loss 0.131204, acc 0.9375\n",
      "2019-06-29T16:31:06.689306: step 1752, loss 0.126718, acc 0.953125\n",
      "2019-06-29T16:31:06.960185: step 1753, loss 0.0934326, acc 0.953125\n",
      "2019-06-29T16:31:07.223549: step 1754, loss 0.06449, acc 0.96875\n",
      "2019-06-29T16:31:07.496076: step 1755, loss 0.141105, acc 0.9375\n",
      "2019-06-29T16:31:07.771029: step 1756, loss 0.10231, acc 0.96875\n",
      "2019-06-29T16:31:08.033456: step 1757, loss 0.139224, acc 0.984375\n",
      "2019-06-29T16:31:08.301923: step 1758, loss 0.101301, acc 0.921875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-29T16:31:08.562429: step 1759, loss 0.219226, acc 0.921875\n",
      "2019-06-29T16:31:08.822802: step 1760, loss 0.080961, acc 0.96875\n",
      "2019-06-29T16:31:09.089613: step 1761, loss 0.12357, acc 0.984375\n",
      "2019-06-29T16:31:09.340871: step 1762, loss 0.0795542, acc 0.96875\n",
      "2019-06-29T16:31:09.622217: step 1763, loss 0.150801, acc 0.9375\n",
      "2019-06-29T16:31:09.873375: step 1764, loss 0.0902898, acc 0.9375\n",
      "2019-06-29T16:31:10.139055: step 1765, loss 0.0742801, acc 0.96875\n",
      "2019-06-29T16:31:10.406153: step 1766, loss 0.150414, acc 0.953125\n",
      "2019-06-29T16:31:10.672795: step 1767, loss 0.108422, acc 0.9375\n",
      "2019-06-29T16:31:10.939483: step 1768, loss 0.100934, acc 0.96875\n",
      "2019-06-29T16:31:11.205091: step 1769, loss 0.104427, acc 0.9375\n",
      "2019-06-29T16:31:11.477740: step 1770, loss 0.11879, acc 0.953125\n",
      "2019-06-29T16:31:11.724294: step 1771, loss 0.107791, acc 0.96875\n",
      "2019-06-29T16:31:12.004576: step 1772, loss 0.0767328, acc 0.96875\n",
      "2019-06-29T16:31:12.256548: step 1773, loss 0.141828, acc 0.953125\n",
      "2019-06-29T16:31:12.522222: step 1774, loss 0.122455, acc 0.953125\n",
      "2019-06-29T16:31:12.805156: step 1775, loss 0.204088, acc 0.9375\n",
      "2019-06-29T16:31:13.071919: step 1776, loss 0.20077, acc 0.90625\n",
      "2019-06-29T16:31:13.339578: step 1777, loss 0.0638584, acc 0.984375\n",
      "2019-06-29T16:31:13.604870: step 1778, loss 0.12065, acc 0.953125\n",
      "2019-06-29T16:31:13.872954: step 1779, loss 0.0799152, acc 0.984375\n",
      "2019-06-29T16:31:14.138704: step 1780, loss 0.186935, acc 0.953125\n",
      "2019-06-29T16:31:14.406485: step 1781, loss 0.0861003, acc 0.984375\n",
      "2019-06-29T16:31:14.688092: step 1782, loss 0.0734573, acc 0.984375\n",
      "2019-06-29T16:31:14.973089: step 1783, loss 0.102704, acc 0.9375\n",
      "2019-06-29T16:31:15.293146: step 1784, loss 0.138504, acc 0.9375\n",
      "2019-06-29T16:31:15.570724: step 1785, loss 0.156139, acc 0.9375\n",
      "2019-06-29T16:31:15.859986: step 1786, loss 0.0938241, acc 0.984375\n",
      "2019-06-29T16:31:16.149133: step 1787, loss 0.166607, acc 0.921875\n",
      "2019-06-29T16:31:16.425340: step 1788, loss 0.124769, acc 0.953125\n",
      "2019-06-29T16:31:16.688916: step 1789, loss 0.125354, acc 0.953125\n",
      "2019-06-29T16:31:16.971265: step 1790, loss 0.101404, acc 0.984375\n",
      "2019-06-29T16:31:17.254674: step 1791, loss 0.0425003, acc 1\n",
      "2019-06-29T16:31:17.538048: step 1792, loss 0.0960147, acc 0.984375\n",
      "2019-06-29T16:31:17.806488: step 1793, loss 0.148675, acc 0.9375\n",
      "2019-06-29T16:31:18.072486: step 1794, loss 0.111129, acc 0.953125\n",
      "2019-06-29T16:31:18.356431: step 1795, loss 0.111289, acc 0.921875\n",
      "2019-06-29T16:31:18.623840: step 1796, loss 0.124489, acc 0.921875\n",
      "2019-06-29T16:31:18.905554: step 1797, loss 0.105225, acc 0.96875\n",
      "2019-06-29T16:31:19.188057: step 1798, loss 0.0807437, acc 0.96875\n",
      "2019-06-29T16:31:19.487409: step 1799, loss 0.281188, acc 0.921875\n",
      "2019-06-29T16:31:19.755514: step 1800, loss 0.143071, acc 0.933333\n",
      "\n",
      "Evaluation:\n",
      "2019-06-29T16:31:20.478162: step 1800, loss 0.698359, acc 0.712946\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\mao\\Desktop\\Tensorflow\\CNN\\1\\cnn-text-classification-tf-master\\runs\\1561796526\\checkpoints\\model-1800\n",
      "\n",
      "2019-06-29T16:31:23.356438: step 1801, loss 0.129556, acc 0.953125\n",
      "2019-06-29T16:31:23.622503: step 1802, loss 0.0883298, acc 0.96875\n",
      "2019-06-29T16:31:23.889938: step 1803, loss 0.049166, acc 1\n",
      "2019-06-29T16:31:24.145247: step 1804, loss 0.0354936, acc 1\n",
      "2019-06-29T16:31:24.407282: step 1805, loss 0.0943244, acc 0.953125\n",
      "2019-06-29T16:31:24.680291: step 1806, loss 0.126865, acc 0.9375\n",
      "2019-06-29T16:31:24.956292: step 1807, loss 0.0904966, acc 0.984375\n",
      "2019-06-29T16:31:25.219989: step 1808, loss 0.0531044, acc 1\n",
      "2019-06-29T16:31:25.473584: step 1809, loss 0.0669192, acc 1\n",
      "2019-06-29T16:31:25.738561: step 1810, loss 0.123927, acc 0.921875\n",
      "2019-06-29T16:31:25.990010: step 1811, loss 0.0715627, acc 0.984375\n",
      "2019-06-29T16:31:26.257055: step 1812, loss 0.0573829, acc 0.984375\n",
      "2019-06-29T16:31:26.523073: step 1813, loss 0.107797, acc 0.953125\n",
      "2019-06-29T16:31:26.790057: step 1814, loss 0.110288, acc 0.96875\n",
      "2019-06-29T16:31:27.055535: step 1815, loss 0.04367, acc 1\n",
      "2019-06-29T16:31:27.314424: step 1816, loss 0.0346193, acc 1\n",
      "2019-06-29T16:31:27.571979: step 1817, loss 0.130468, acc 0.953125\n",
      "2019-06-29T16:31:27.823466: step 1818, loss 0.153893, acc 0.953125\n",
      "2019-06-29T16:31:28.096454: step 1819, loss 0.141416, acc 0.953125\n",
      "2019-06-29T16:31:28.356515: step 1820, loss 0.0678872, acc 0.984375\n",
      "2019-06-29T16:31:28.622599: step 1821, loss 0.130349, acc 0.953125\n",
      "2019-06-29T16:31:28.888898: step 1822, loss 0.055156, acc 0.984375\n",
      "2019-06-29T16:31:29.156013: step 1823, loss 0.0695443, acc 0.984375\n",
      "2019-06-29T16:31:29.421968: step 1824, loss 0.119399, acc 0.953125\n",
      "2019-06-29T16:31:29.700090: step 1825, loss 0.147328, acc 0.9375\n",
      "2019-06-29T16:31:29.956725: step 1826, loss 0.0411567, acc 0.984375\n",
      "2019-06-29T16:31:30.206232: step 1827, loss 0.0801135, acc 0.96875\n",
      "2019-06-29T16:31:30.489156: step 1828, loss 0.0736454, acc 0.984375\n",
      "2019-06-29T16:31:30.786671: step 1829, loss 0.120435, acc 0.953125\n",
      "2019-06-29T16:31:31.071287: step 1830, loss 0.0537824, acc 0.984375\n",
      "2019-06-29T16:31:31.369926: step 1831, loss 0.133575, acc 0.953125\n",
      "2019-06-29T16:31:31.639337: step 1832, loss 0.121782, acc 0.953125\n",
      "2019-06-29T16:31:31.921067: step 1833, loss 0.115384, acc 0.953125\n",
      "2019-06-29T16:31:32.188513: step 1834, loss 0.118883, acc 0.9375\n",
      "2019-06-29T16:31:32.472081: step 1835, loss 0.0661187, acc 0.96875\n",
      "2019-06-29T16:31:32.739202: step 1836, loss 0.0368216, acc 1\n",
      "2019-06-29T16:31:33.020965: step 1837, loss 0.0775513, acc 0.984375\n",
      "2019-06-29T16:31:33.289181: step 1838, loss 0.132889, acc 0.953125\n",
      "2019-06-29T16:31:33.587450: step 1839, loss 0.0874786, acc 0.96875\n",
      "2019-06-29T16:31:33.856593: step 1840, loss 0.116587, acc 0.953125\n",
      "2019-06-29T16:31:34.138323: step 1841, loss 0.0914303, acc 0.96875\n",
      "2019-06-29T16:31:34.455403: step 1842, loss 0.100203, acc 0.984375\n",
      "2019-06-29T16:31:34.754616: step 1843, loss 0.072587, acc 0.96875\n",
      "2019-06-29T16:31:35.037585: step 1844, loss 0.116139, acc 0.96875\n",
      "2019-06-29T16:31:35.321668: step 1845, loss 0.160641, acc 0.90625\n",
      "2019-06-29T16:31:35.604446: step 1846, loss 0.0354449, acc 0.984375\n",
      "2019-06-29T16:31:35.902821: step 1847, loss 0.113091, acc 0.9375\n",
      "2019-06-29T16:31:36.172924: step 1848, loss 0.173636, acc 0.953125\n",
      "2019-06-29T16:31:36.501008: step 1849, loss 0.132843, acc 0.953125\n",
      "2019-06-29T16:31:36.783210: step 1850, loss 0.07115, acc 0.953125\n",
      "2019-06-29T16:31:37.055831: step 1851, loss 0.0356947, acc 1\n",
      "2019-06-29T16:31:37.337617: step 1852, loss 0.102843, acc 0.96875\n",
      "2019-06-29T16:31:37.605688: step 1853, loss 0.0643283, acc 0.96875\n",
      "2019-06-29T16:31:37.904331: step 1854, loss 0.121281, acc 0.953125\n",
      "2019-06-29T16:31:38.173308: step 1855, loss 0.158887, acc 0.9375\n",
      "2019-06-29T16:31:38.453915: step 1856, loss 0.189233, acc 0.90625\n",
      "2019-06-29T16:31:38.706335: step 1857, loss 0.11305, acc 0.96875\n",
      "2019-06-29T16:31:38.981824: step 1858, loss 0.0824044, acc 0.96875\n",
      "2019-06-29T16:31:39.239332: step 1859, loss 0.087977, acc 0.96875\n",
      "2019-06-29T16:31:39.506345: step 1860, loss 0.0928429, acc 0.953125\n",
      "2019-06-29T16:31:39.770192: step 1861, loss 0.0592812, acc 0.96875\n",
      "2019-06-29T16:31:40.035277: step 1862, loss 0.0815251, acc 0.984375\n",
      "2019-06-29T16:31:40.296942: step 1863, loss 0.116488, acc 0.9375\n",
      "2019-06-29T16:31:40.557466: step 1864, loss 0.119822, acc 0.953125\n",
      "2019-06-29T16:31:40.822337: step 1865, loss 0.0920329, acc 0.96875\n",
      "2019-06-29T16:31:41.084649: step 1866, loss 0.0559995, acc 1\n",
      "2019-06-29T16:31:41.350257: step 1867, loss 0.0757407, acc 0.96875\n",
      "2019-06-29T16:31:41.615548: step 1868, loss 0.0667455, acc 0.96875\n",
      "2019-06-29T16:31:41.880098: step 1869, loss 0.035034, acc 1\n",
      "2019-06-29T16:31:42.145751: step 1870, loss 0.0504348, acc 1\n",
      "2019-06-29T16:31:42.405905: step 1871, loss 0.112847, acc 0.9375\n",
      "2019-06-29T16:31:42.675912: step 1872, loss 0.0653144, acc 1\n",
      "2019-06-29T16:31:42.946306: step 1873, loss 0.123062, acc 0.953125\n",
      "2019-06-29T16:31:43.210931: step 1874, loss 0.0738959, acc 0.984375\n",
      "2019-06-29T16:31:43.477604: step 1875, loss 0.0919363, acc 0.984375\n",
      "2019-06-29T16:31:43.740041: step 1876, loss 0.0518731, acc 1\n",
      "2019-06-29T16:31:43.998872: step 1877, loss 0.0664407, acc 0.984375\n",
      "2019-06-29T16:31:44.265952: step 1878, loss 0.103268, acc 0.96875\n",
      "2019-06-29T16:31:44.532542: step 1879, loss 0.0455886, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-29T16:31:44.796060: step 1880, loss 0.106901, acc 0.921875\n",
      "2019-06-29T16:31:45.055343: step 1881, loss 0.0889856, acc 0.96875\n",
      "2019-06-29T16:31:45.321191: step 1882, loss 0.184906, acc 0.953125\n",
      "2019-06-29T16:31:45.587649: step 1883, loss 0.068603, acc 0.96875\n",
      "2019-06-29T16:31:45.855366: step 1884, loss 0.108138, acc 0.953125\n",
      "2019-06-29T16:31:46.120098: step 1885, loss 0.113016, acc 0.953125\n",
      "2019-06-29T16:31:46.377089: step 1886, loss 0.0534565, acc 0.984375\n",
      "2019-06-29T16:31:46.684711: step 1887, loss 0.107289, acc 0.953125\n",
      "2019-06-29T16:31:46.965416: step 1888, loss 0.139024, acc 0.9375\n",
      "2019-06-29T16:31:47.268738: step 1889, loss 0.074684, acc 0.984375\n",
      "2019-06-29T16:31:47.537767: step 1890, loss 0.0473653, acc 0.984375\n",
      "2019-06-29T16:31:47.826162: step 1891, loss 0.141212, acc 0.921875\n",
      "2019-06-29T16:31:48.088289: step 1892, loss 0.0557666, acc 0.984375\n",
      "2019-06-29T16:31:48.370805: step 1893, loss 0.0751163, acc 0.984375\n",
      "2019-06-29T16:31:48.652910: step 1894, loss 0.0897498, acc 0.96875\n",
      "2019-06-29T16:31:48.936218: step 1895, loss 0.0734465, acc 0.984375\n",
      "2019-06-29T16:31:49.208991: step 1896, loss 0.0430764, acc 1\n",
      "2019-06-29T16:31:49.487798: step 1897, loss 0.0958646, acc 0.96875\n",
      "2019-06-29T16:31:49.754225: step 1898, loss 0.0637534, acc 0.984375\n",
      "2019-06-29T16:31:50.037464: step 1899, loss 0.0564949, acc 1\n",
      "2019-06-29T16:31:50.305134: step 1900, loss 0.0984894, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2019-06-29T16:31:51.047667: step 1900, loss 0.70978, acc 0.721388\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\mao\\Desktop\\Tensorflow\\CNN\\1\\cnn-text-classification-tf-master\\runs\\1561796526\\checkpoints\\model-1900\n",
      "\n",
      "2019-06-29T16:31:52.384604: step 1901, loss 0.0542338, acc 0.96875\n",
      "2019-06-29T16:31:52.716776: step 1902, loss 0.0427812, acc 0.984375\n",
      "2019-06-29T16:31:53.002307: step 1903, loss 0.100643, acc 0.984375\n",
      "2019-06-29T16:31:53.286830: step 1904, loss 0.0745706, acc 0.96875\n",
      "2019-06-29T16:31:53.575154: step 1905, loss 0.157022, acc 0.9375\n",
      "2019-06-29T16:31:53.839371: step 1906, loss 0.133352, acc 0.921875\n",
      "2019-06-29T16:31:54.138662: step 1907, loss 0.229141, acc 0.921875\n",
      "2019-06-29T16:31:54.396003: step 1908, loss 0.0817856, acc 0.984375\n",
      "2019-06-29T16:31:54.654947: step 1909, loss 0.101056, acc 0.96875\n",
      "2019-06-29T16:31:54.922979: step 1910, loss 0.0988799, acc 0.96875\n",
      "2019-06-29T16:31:55.188157: step 1911, loss 0.040367, acc 1\n",
      "2019-06-29T16:31:55.454049: step 1912, loss 0.0408282, acc 0.984375\n",
      "2019-06-29T16:31:55.705615: step 1913, loss 0.136492, acc 0.953125\n",
      "2019-06-29T16:31:55.974759: step 1914, loss 0.0402792, acc 0.984375\n",
      "2019-06-29T16:31:56.238982: step 1915, loss 0.0904595, acc 0.984375\n",
      "2019-06-29T16:31:56.504077: step 1916, loss 0.0253968, acc 1\n",
      "2019-06-29T16:31:56.771132: step 1917, loss 0.140814, acc 0.953125\n",
      "2019-06-29T16:31:57.039275: step 1918, loss 0.0499803, acc 0.984375\n",
      "2019-06-29T16:31:57.304035: step 1919, loss 0.0431296, acc 1\n",
      "2019-06-29T16:31:57.571196: step 1920, loss 0.158855, acc 0.953125\n",
      "2019-06-29T16:31:57.839189: step 1921, loss 0.104621, acc 0.953125\n",
      "2019-06-29T16:31:58.088599: step 1922, loss 0.0756301, acc 0.96875\n",
      "2019-06-29T16:31:58.354641: step 1923, loss 0.0726147, acc 0.96875\n",
      "2019-06-29T16:31:58.621927: step 1924, loss 0.209621, acc 0.90625\n",
      "2019-06-29T16:31:58.888002: step 1925, loss 0.0835942, acc 0.96875\n",
      "2019-06-29T16:31:59.154114: step 1926, loss 0.157122, acc 0.9375\n",
      "2019-06-29T16:31:59.420637: step 1927, loss 0.19702, acc 0.953125\n",
      "2019-06-29T16:31:59.681967: step 1928, loss 0.0851305, acc 0.96875\n",
      "2019-06-29T16:31:59.938353: step 1929, loss 0.0530997, acc 0.984375\n",
      "2019-06-29T16:32:00.203909: step 1930, loss 0.0577808, acc 0.984375\n",
      "2019-06-29T16:32:00.471146: step 1931, loss 0.0762575, acc 1\n",
      "2019-06-29T16:32:00.735097: step 1932, loss 0.0616707, acc 1\n",
      "2019-06-29T16:32:00.988822: step 1933, loss 0.222951, acc 0.953125\n",
      "2019-06-29T16:32:01.255479: step 1934, loss 0.0663281, acc 0.96875\n",
      "2019-06-29T16:32:01.520983: step 1935, loss 0.0827176, acc 0.96875\n",
      "2019-06-29T16:32:01.782377: step 1936, loss 0.152468, acc 0.953125\n",
      "2019-06-29T16:32:02.055400: step 1937, loss 0.0786656, acc 0.96875\n",
      "2019-06-29T16:32:02.306872: step 1938, loss 0.0448138, acc 1\n",
      "2019-06-29T16:32:02.603261: step 1939, loss 0.106637, acc 0.984375\n",
      "2019-06-29T16:32:02.902260: step 1940, loss 0.087256, acc 0.953125\n",
      "2019-06-29T16:32:03.187479: step 1941, loss 0.0481344, acc 0.984375\n",
      "2019-06-29T16:32:03.479120: step 1942, loss 0.135531, acc 0.953125\n",
      "2019-06-29T16:32:03.738178: step 1943, loss 0.115126, acc 0.9375\n",
      "2019-06-29T16:32:04.015133: step 1944, loss 0.124714, acc 0.96875\n",
      "2019-06-29T16:32:04.287409: step 1945, loss 0.0790464, acc 0.953125\n",
      "2019-06-29T16:32:04.570745: step 1946, loss 0.142913, acc 0.953125\n",
      "2019-06-29T16:32:04.853461: step 1947, loss 0.0500911, acc 0.984375\n",
      "2019-06-29T16:32:05.136310: step 1948, loss 0.257183, acc 0.9375\n",
      "2019-06-29T16:32:05.404710: step 1949, loss 0.0676524, acc 0.984375\n",
      "2019-06-29T16:32:05.671881: step 1950, loss 0.218455, acc 0.95\n",
      "2019-06-29T16:32:05.952981: step 1951, loss 0.0545199, acc 0.984375\n",
      "2019-06-29T16:32:06.236739: step 1952, loss 0.0528902, acc 1\n",
      "2019-06-29T16:32:06.505242: step 1953, loss 0.0506093, acc 1\n",
      "2019-06-29T16:32:06.787139: step 1954, loss 0.0485889, acc 1\n",
      "2019-06-29T16:32:07.075541: step 1955, loss 0.0385689, acc 0.984375\n",
      "2019-06-29T16:32:07.366987: step 1956, loss 0.0529718, acc 0.96875\n",
      "2019-06-29T16:32:07.653725: step 1957, loss 0.0986657, acc 0.953125\n",
      "2019-06-29T16:32:07.926839: step 1958, loss 0.081003, acc 0.96875\n",
      "2019-06-29T16:32:08.252490: step 1959, loss 0.183402, acc 0.9375\n",
      "2019-06-29T16:32:08.536172: step 1960, loss 0.0506799, acc 0.984375\n",
      "2019-06-29T16:32:08.820866: step 1961, loss 0.0381254, acc 0.984375\n",
      "2019-06-29T16:32:09.095960: step 1962, loss 0.0533599, acc 0.984375\n",
      "2019-06-29T16:32:09.403313: step 1963, loss 0.0810204, acc 0.953125\n",
      "2019-06-29T16:32:09.687047: step 1964, loss 0.0449279, acc 0.984375\n",
      "2019-06-29T16:32:09.960025: step 1965, loss 0.0854544, acc 0.953125\n",
      "2019-06-29T16:32:10.237217: step 1966, loss 0.0548722, acc 0.96875\n",
      "2019-06-29T16:32:10.504174: step 1967, loss 0.0411728, acc 0.984375\n",
      "2019-06-29T16:32:10.785817: step 1968, loss 0.0558045, acc 0.984375\n",
      "2019-06-29T16:32:11.038324: step 1969, loss 0.0645509, acc 0.984375\n",
      "2019-06-29T16:32:11.303436: step 1970, loss 0.0762535, acc 0.953125\n",
      "2019-06-29T16:32:11.571554: step 1971, loss 0.0575512, acc 0.96875\n",
      "2019-06-29T16:32:11.838581: step 1972, loss 0.0762596, acc 0.984375\n",
      "2019-06-29T16:32:12.103524: step 1973, loss 0.153411, acc 0.9375\n",
      "2019-06-29T16:32:12.371668: step 1974, loss 0.0732439, acc 0.984375\n",
      "2019-06-29T16:32:12.653893: step 1975, loss 0.0921935, acc 0.96875\n",
      "2019-06-29T16:32:12.920880: step 1976, loss 0.0905288, acc 0.953125\n",
      "2019-06-29T16:32:13.186863: step 1977, loss 0.099365, acc 0.953125\n",
      "2019-06-29T16:32:13.459835: step 1978, loss 0.0540737, acc 0.984375\n",
      "2019-06-29T16:32:13.768038: step 1979, loss 0.0529574, acc 1\n",
      "2019-06-29T16:32:14.021448: step 1980, loss 0.0616869, acc 0.984375\n",
      "2019-06-29T16:32:14.300376: step 1981, loss 0.122351, acc 0.9375\n",
      "2019-06-29T16:32:14.571893: step 1982, loss 0.0389857, acc 1\n",
      "2019-06-29T16:32:14.853955: step 1983, loss 0.0472278, acc 0.984375\n",
      "2019-06-29T16:32:15.119208: step 1984, loss 0.0949442, acc 0.96875\n",
      "2019-06-29T16:32:15.386815: step 1985, loss 0.0762555, acc 0.96875\n",
      "2019-06-29T16:32:15.657970: step 1986, loss 0.0428443, acc 1\n",
      "2019-06-29T16:32:15.937512: step 1987, loss 0.139299, acc 0.953125\n",
      "2019-06-29T16:32:16.208505: step 1988, loss 0.0667566, acc 0.984375\n",
      "2019-06-29T16:32:16.473084: step 1989, loss 0.0724886, acc 0.96875\n",
      "2019-06-29T16:32:16.738121: step 1990, loss 0.0269555, acc 1\n",
      "2019-06-29T16:32:17.004600: step 1991, loss 0.0214257, acc 1\n",
      "2019-06-29T16:32:17.283697: step 1992, loss 0.0259333, acc 1\n",
      "2019-06-29T16:32:17.552357: step 1993, loss 0.0791261, acc 0.984375\n",
      "2019-06-29T16:32:17.820907: step 1994, loss 0.0778608, acc 0.96875\n",
      "2019-06-29T16:32:18.087809: step 1995, loss 0.0400111, acc 1\n",
      "2019-06-29T16:32:18.361931: step 1996, loss 0.0662462, acc 0.96875\n",
      "2019-06-29T16:32:18.643873: step 1997, loss 0.092549, acc 0.984375\n",
      "2019-06-29T16:32:18.935988: step 1998, loss 0.0451085, acc 1\n",
      "2019-06-29T16:32:19.252021: step 1999, loss 0.0570363, acc 0.984375\n",
      "2019-06-29T16:32:19.535882: step 2000, loss 0.152875, acc 0.921875\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-29T16:32:20.295506: step 2000, loss 0.745581, acc 0.727955\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\mao\\Desktop\\Tensorflow\\CNN\\1\\cnn-text-classification-tf-master\\runs\\1561796526\\checkpoints\\model-2000\n",
      "\n",
      "2019-06-29T16:32:21.644996: step 2001, loss 0.10516, acc 0.953125\n",
      "2019-06-29T16:32:21.944196: step 2002, loss 0.119393, acc 0.96875\n",
      "2019-06-29T16:32:22.234667: step 2003, loss 0.0383131, acc 0.984375\n",
      "2019-06-29T16:32:22.518443: step 2004, loss 0.0464077, acc 1\n",
      "2019-06-29T16:32:22.802748: step 2005, loss 0.0470776, acc 0.984375\n",
      "2019-06-29T16:32:23.085217: step 2006, loss 0.0514938, acc 0.984375\n",
      "2019-06-29T16:32:23.383502: step 2007, loss 0.104489, acc 0.96875\n",
      "2019-06-29T16:32:23.669661: step 2008, loss 0.0497663, acc 0.984375\n",
      "2019-06-29T16:32:23.938867: step 2009, loss 0.0413197, acc 0.984375\n",
      "2019-06-29T16:32:24.218373: step 2010, loss 0.0758209, acc 0.96875\n",
      "2019-06-29T16:32:24.502805: step 2011, loss 0.0658546, acc 0.984375\n",
      "2019-06-29T16:32:24.791087: step 2012, loss 0.123926, acc 0.9375\n",
      "2019-06-29T16:32:25.084990: step 2013, loss 0.0591859, acc 0.984375\n",
      "2019-06-29T16:32:25.368497: step 2014, loss 0.0889352, acc 0.96875\n",
      "2019-06-29T16:32:25.641777: step 2015, loss 0.0827098, acc 0.9375\n",
      "2019-06-29T16:32:25.904878: step 2016, loss 0.0558836, acc 1\n",
      "2019-06-29T16:32:26.169931: step 2017, loss 0.0749659, acc 0.984375\n",
      "2019-06-29T16:32:26.444682: step 2018, loss 0.0713926, acc 0.953125\n",
      "2019-06-29T16:32:26.719552: step 2019, loss 0.06596, acc 1\n",
      "2019-06-29T16:32:27.000955: step 2020, loss 0.0299414, acc 1\n",
      "2019-06-29T16:32:27.285463: step 2021, loss 0.0851379, acc 0.953125\n",
      "2019-06-29T16:32:27.545488: step 2022, loss 0.0600014, acc 1\n",
      "2019-06-29T16:32:27.819050: step 2023, loss 0.0328016, acc 1\n",
      "2019-06-29T16:32:28.084216: step 2024, loss 0.0794466, acc 0.953125\n",
      "2019-06-29T16:32:28.353396: step 2025, loss 0.0307583, acc 1\n",
      "2019-06-29T16:32:28.611778: step 2026, loss 0.100383, acc 0.984375\n",
      "2019-06-29T16:32:28.886655: step 2027, loss 0.13119, acc 0.9375\n",
      "2019-06-29T16:32:29.145050: step 2028, loss 0.126351, acc 0.953125\n",
      "2019-06-29T16:32:29.419893: step 2029, loss 0.0909511, acc 0.96875\n",
      "2019-06-29T16:32:29.677230: step 2030, loss 0.0937423, acc 0.953125\n",
      "2019-06-29T16:32:29.951861: step 2031, loss 0.0991231, acc 0.96875\n",
      "2019-06-29T16:32:30.216943: step 2032, loss 0.163009, acc 0.9375\n",
      "2019-06-29T16:32:30.470508: step 2033, loss 0.0563892, acc 0.96875\n",
      "2019-06-29T16:32:30.736517: step 2034, loss 0.0953634, acc 0.9375\n",
      "2019-06-29T16:32:31.004025: step 2035, loss 0.0609726, acc 0.96875\n",
      "2019-06-29T16:32:31.297244: step 2036, loss 0.0494118, acc 0.984375\n",
      "2019-06-29T16:32:31.584748: step 2037, loss 0.12043, acc 0.921875\n",
      "2019-06-29T16:32:31.884023: step 2038, loss 0.0531149, acc 0.984375\n",
      "2019-06-29T16:32:32.170085: step 2039, loss 0.0954841, acc 0.953125\n",
      "2019-06-29T16:32:32.526576: step 2040, loss 0.0711284, acc 0.984375\n",
      "2019-06-29T16:32:32.803511: step 2041, loss 0.0398287, acc 1\n",
      "2019-06-29T16:32:33.071107: step 2042, loss 0.122654, acc 0.953125\n",
      "2019-06-29T16:32:33.352785: step 2043, loss 0.0527736, acc 0.984375\n",
      "2019-06-29T16:32:33.637083: step 2044, loss 0.0973362, acc 0.96875\n",
      "2019-06-29T16:32:33.903272: step 2045, loss 0.037926, acc 1\n",
      "2019-06-29T16:32:34.168414: step 2046, loss 0.115435, acc 0.9375\n",
      "2019-06-29T16:32:34.452402: step 2047, loss 0.129427, acc 0.953125\n",
      "2019-06-29T16:32:34.735162: step 2048, loss 0.049257, acc 0.984375\n",
      "2019-06-29T16:32:35.035638: step 2049, loss 0.0398669, acc 0.984375\n",
      "2019-06-29T16:32:35.319325: step 2050, loss 0.0565356, acc 0.96875\n",
      "2019-06-29T16:32:35.618783: step 2051, loss 0.0463061, acc 1\n",
      "2019-06-29T16:32:35.981274: step 2052, loss 0.0197814, acc 1\n",
      "2019-06-29T16:32:36.301244: step 2053, loss 0.0760836, acc 0.96875\n",
      "2019-06-29T16:32:36.672670: step 2054, loss 0.0499208, acc 0.984375\n",
      "2019-06-29T16:32:36.999392: step 2055, loss 0.0804147, acc 0.9375\n",
      "2019-06-29T16:32:37.269750: step 2056, loss 0.0790261, acc 0.953125\n",
      "2019-06-29T16:32:37.550714: step 2057, loss 0.0514647, acc 0.984375\n",
      "2019-06-29T16:32:37.835794: step 2058, loss 0.0753659, acc 0.984375\n",
      "2019-06-29T16:32:38.119439: step 2059, loss 0.0478124, acc 1\n",
      "2019-06-29T16:32:38.418067: step 2060, loss 0.0363705, acc 1\n",
      "2019-06-29T16:32:38.701836: step 2061, loss 0.14435, acc 0.953125\n",
      "2019-06-29T16:32:38.985730: step 2062, loss 0.0534036, acc 0.984375\n",
      "2019-06-29T16:32:39.268844: step 2063, loss 0.180129, acc 0.953125\n",
      "2019-06-29T16:32:39.551664: step 2064, loss 0.104886, acc 0.96875\n",
      "2019-06-29T16:32:39.933009: step 2065, loss 0.0596252, acc 0.984375\n",
      "2019-06-29T16:32:40.218096: step 2066, loss 0.0525122, acc 0.984375\n",
      "2019-06-29T16:32:40.517504: step 2067, loss 0.0549541, acc 0.984375\n",
      "2019-06-29T16:32:40.817933: step 2068, loss 0.161889, acc 0.9375\n",
      "2019-06-29T16:32:41.117388: step 2069, loss 0.0535836, acc 0.96875\n",
      "2019-06-29T16:32:41.416936: step 2070, loss 0.0671495, acc 0.96875\n",
      "2019-06-29T16:32:41.702853: step 2071, loss 0.0465879, acc 0.984375\n",
      "2019-06-29T16:32:41.986387: step 2072, loss 0.0312523, acc 1\n",
      "2019-06-29T16:32:42.284959: step 2073, loss 0.139022, acc 0.9375\n",
      "2019-06-29T16:32:42.584301: step 2074, loss 0.068672, acc 0.96875\n",
      "2019-06-29T16:32:42.852358: step 2075, loss 0.118352, acc 0.984375\n",
      "2019-06-29T16:32:43.135010: step 2076, loss 0.0830582, acc 0.96875\n",
      "2019-06-29T16:32:43.402116: step 2077, loss 0.0792044, acc 0.984375\n",
      "2019-06-29T16:32:43.653624: step 2078, loss 0.0942265, acc 0.984375\n",
      "2019-06-29T16:32:43.937416: step 2079, loss 0.0721865, acc 0.96875\n",
      "2019-06-29T16:32:44.201227: step 2080, loss 0.136911, acc 0.9375\n",
      "2019-06-29T16:32:44.469440: step 2081, loss 0.0817648, acc 0.984375\n",
      "2019-06-29T16:32:44.735775: step 2082, loss 0.0817977, acc 0.953125\n",
      "2019-06-29T16:32:45.002990: step 2083, loss 0.0912974, acc 0.96875\n",
      "2019-06-29T16:32:45.269198: step 2084, loss 0.0914166, acc 0.984375\n",
      "2019-06-29T16:32:45.537357: step 2085, loss 0.10309, acc 0.96875\n",
      "2019-06-29T16:32:45.818264: step 2086, loss 0.11731, acc 0.953125\n",
      "2019-06-29T16:32:46.085397: step 2087, loss 0.0612614, acc 0.96875\n",
      "2019-06-29T16:32:46.347915: step 2088, loss 0.0803222, acc 0.984375\n",
      "2019-06-29T16:32:46.634544: step 2089, loss 0.07353, acc 0.96875\n",
      "2019-06-29T16:32:46.887096: step 2090, loss 0.0878633, acc 0.953125\n",
      "2019-06-29T16:32:47.167754: step 2091, loss 0.0636154, acc 0.984375\n",
      "2019-06-29T16:32:47.437300: step 2092, loss 0.0657724, acc 0.96875\n",
      "2019-06-29T16:32:47.702275: step 2093, loss 0.0387106, acc 1\n",
      "2019-06-29T16:32:47.970413: step 2094, loss 0.0484259, acc 0.984375\n",
      "2019-06-29T16:32:48.236638: step 2095, loss 0.0493646, acc 0.96875\n",
      "2019-06-29T16:32:48.503319: step 2096, loss 0.0913095, acc 0.96875\n",
      "2019-06-29T16:32:48.770261: step 2097, loss 0.0623954, acc 0.984375\n",
      "2019-06-29T16:32:49.036249: step 2098, loss 0.098759, acc 0.96875\n",
      "2019-06-29T16:32:49.303841: step 2099, loss 0.0626436, acc 0.984375\n",
      "2019-06-29T16:32:49.577020: step 2100, loss 0.128995, acc 0.966667\n",
      "\n",
      "Evaluation:\n",
      "2019-06-29T16:32:50.208036: step 2100, loss 0.771663, acc 0.723265\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\mao\\Desktop\\Tensorflow\\CNN\\1\\cnn-text-classification-tf-master\\runs\\1561796526\\checkpoints\\model-2100\n",
      "\n",
      "2019-06-29T16:32:51.706729: step 2101, loss 0.0598754, acc 0.984375\n",
      "2019-06-29T16:32:52.001556: step 2102, loss 0.0961291, acc 0.953125\n",
      "2019-06-29T16:32:52.277006: step 2103, loss 0.0813776, acc 0.984375\n",
      "2019-06-29T16:32:52.553030: step 2104, loss 0.0550492, acc 0.984375\n",
      "2019-06-29T16:32:52.836348: step 2105, loss 0.0751964, acc 0.984375\n",
      "2019-06-29T16:32:53.119376: step 2106, loss 0.0379616, acc 0.984375\n",
      "2019-06-29T16:32:53.400476: step 2107, loss 0.0509231, acc 1\n",
      "2019-06-29T16:32:53.670245: step 2108, loss 0.133219, acc 0.96875\n",
      "2019-06-29T16:32:53.953671: step 2109, loss 0.0321658, acc 0.984375\n",
      "2019-06-29T16:32:54.235860: step 2110, loss 0.0707001, acc 0.96875\n",
      "2019-06-29T16:32:54.518182: step 2111, loss 0.0691964, acc 0.96875\n",
      "2019-06-29T16:32:54.814296: step 2112, loss 0.0265297, acc 0.984375\n",
      "2019-06-29T16:32:55.118360: step 2113, loss 0.0264373, acc 0.984375\n",
      "2019-06-29T16:32:55.386919: step 2114, loss 0.0693388, acc 0.953125\n",
      "2019-06-29T16:32:55.670247: step 2115, loss 0.0371481, acc 0.96875\n",
      "2019-06-29T16:32:55.969044: step 2116, loss 0.0383773, acc 0.984375\n",
      "2019-06-29T16:32:56.269767: step 2117, loss 0.0475079, acc 0.96875\n",
      "2019-06-29T16:32:56.544167: step 2118, loss 0.0528032, acc 0.984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-29T16:32:56.818310: step 2119, loss 0.0352336, acc 0.984375\n",
      "2019-06-29T16:32:57.115169: step 2120, loss 0.0620983, acc 0.96875\n",
      "2019-06-29T16:32:57.400556: step 2121, loss 0.0499584, acc 1\n",
      "2019-06-29T16:32:57.667219: step 2122, loss 0.0517128, acc 0.984375\n",
      "2019-06-29T16:32:57.950568: step 2123, loss 0.0741334, acc 0.953125\n",
      "2019-06-29T16:32:58.404350: step 2124, loss 0.0410155, acc 1\n",
      "2019-06-29T16:32:58.677545: step 2125, loss 0.0632602, acc 0.984375\n",
      "2019-06-29T16:32:58.951740: step 2126, loss 0.0238464, acc 1\n",
      "2019-06-29T16:32:59.218083: step 2127, loss 0.0604427, acc 0.984375\n",
      "2019-06-29T16:32:59.516738: step 2128, loss 0.021221, acc 0.984375\n",
      "2019-06-29T16:32:59.848601: step 2129, loss 0.070475, acc 0.96875\n",
      "2019-06-29T16:33:00.134282: step 2130, loss 0.015084, acc 1\n",
      "2019-06-29T16:33:00.400714: step 2131, loss 0.0395625, acc 0.984375\n",
      "2019-06-29T16:33:00.684000: step 2132, loss 0.0562839, acc 0.984375\n",
      "2019-06-29T16:33:00.942209: step 2133, loss 0.01256, acc 1\n",
      "2019-06-29T16:33:01.203244: step 2134, loss 0.221025, acc 0.90625\n",
      "2019-06-29T16:33:01.468132: step 2135, loss 0.061658, acc 0.96875\n",
      "2019-06-29T16:33:01.793437: step 2136, loss 0.0291253, acc 1\n",
      "2019-06-29T16:33:02.088646: step 2137, loss 0.0420013, acc 1\n",
      "2019-06-29T16:33:02.351577: step 2138, loss 0.0495416, acc 1\n",
      "2019-06-29T16:33:02.618251: step 2139, loss 0.0281557, acc 1\n",
      "2019-06-29T16:33:02.898977: step 2140, loss 0.0349906, acc 1\n",
      "2019-06-29T16:33:03.153105: step 2141, loss 0.0657215, acc 0.96875\n",
      "2019-06-29T16:33:03.482968: step 2142, loss 0.123827, acc 0.953125\n",
      "2019-06-29T16:33:03.767024: step 2143, loss 0.0746237, acc 0.984375\n",
      "2019-06-29T16:33:04.049495: step 2144, loss 0.0362681, acc 0.984375\n",
      "2019-06-29T16:33:04.306536: step 2145, loss 0.0398411, acc 0.984375\n",
      "2019-06-29T16:33:04.569522: step 2146, loss 0.0192341, acc 1\n",
      "2019-06-29T16:33:04.839605: step 2147, loss 0.127487, acc 0.953125\n",
      "2019-06-29T16:33:05.102621: step 2148, loss 0.067896, acc 0.96875\n",
      "2019-06-29T16:33:05.371236: step 2149, loss 0.058157, acc 0.984375\n",
      "2019-06-29T16:33:05.634938: step 2150, loss 0.0634835, acc 0.96875\n",
      "2019-06-29T16:33:05.901744: step 2151, loss 0.0774907, acc 0.984375\n",
      "2019-06-29T16:33:06.169791: step 2152, loss 0.0643537, acc 0.953125\n",
      "2019-06-29T16:33:06.433271: step 2153, loss 0.01811, acc 1\n",
      "2019-06-29T16:33:06.734365: step 2154, loss 0.0650839, acc 0.96875\n",
      "2019-06-29T16:33:07.032349: step 2155, loss 0.0211886, acc 1\n",
      "2019-06-29T16:33:07.317131: step 2156, loss 0.0661422, acc 0.984375\n",
      "2019-06-29T16:33:07.585901: step 2157, loss 0.0384622, acc 0.96875\n",
      "2019-06-29T16:33:07.873771: step 2158, loss 0.141033, acc 0.953125\n",
      "2019-06-29T16:33:08.167262: step 2159, loss 0.052151, acc 0.96875\n",
      "2019-06-29T16:33:08.450610: step 2160, loss 0.0413135, acc 1\n",
      "2019-06-29T16:33:08.723572: step 2161, loss 0.0343119, acc 0.984375\n",
      "2019-06-29T16:33:09.000379: step 2162, loss 0.0628235, acc 0.96875\n",
      "2019-06-29T16:33:09.300926: step 2163, loss 0.10601, acc 0.953125\n",
      "2019-06-29T16:33:09.584627: step 2164, loss 0.0866, acc 0.9375\n",
      "2019-06-29T16:33:09.852781: step 2165, loss 0.102781, acc 0.984375\n",
      "2019-06-29T16:33:10.133434: step 2166, loss 0.0668641, acc 0.96875\n",
      "2019-06-29T16:33:10.432899: step 2167, loss 0.0275778, acc 1\n",
      "2019-06-29T16:33:10.717401: step 2168, loss 0.0437709, acc 1\n",
      "2019-06-29T16:33:11.000758: step 2169, loss 0.0513461, acc 0.984375\n",
      "2019-06-29T16:33:11.284053: step 2170, loss 0.0753676, acc 0.96875\n",
      "2019-06-29T16:33:11.582513: step 2171, loss 0.0294993, acc 1\n",
      "2019-06-29T16:33:11.851510: step 2172, loss 0.0419349, acc 0.984375\n",
      "2019-06-29T16:33:12.135257: step 2173, loss 0.121918, acc 0.921875\n",
      "2019-06-29T16:33:12.417045: step 2174, loss 0.0520434, acc 0.96875\n",
      "2019-06-29T16:33:12.702119: step 2175, loss 0.059655, acc 0.96875\n",
      "2019-06-29T16:33:12.983894: step 2176, loss 0.0782349, acc 0.96875\n",
      "2019-06-29T16:33:13.267389: step 2177, loss 0.0345002, acc 0.984375\n",
      "2019-06-29T16:33:13.550111: step 2178, loss 0.0974528, acc 0.984375\n",
      "2019-06-29T16:33:13.818111: step 2179, loss 0.0358828, acc 0.984375\n",
      "2019-06-29T16:33:14.101786: step 2180, loss 0.0494938, acc 0.984375\n",
      "2019-06-29T16:33:14.368929: step 2181, loss 0.110392, acc 0.96875\n",
      "2019-06-29T16:33:14.651761: step 2182, loss 0.052581, acc 0.984375\n",
      "2019-06-29T16:33:14.917722: step 2183, loss 0.0365141, acc 1\n",
      "2019-06-29T16:33:15.185975: step 2184, loss 0.0391143, acc 0.984375\n",
      "2019-06-29T16:33:15.459019: step 2185, loss 0.0496672, acc 0.984375\n",
      "2019-06-29T16:33:15.734675: step 2186, loss 0.059862, acc 0.984375\n",
      "2019-06-29T16:33:16.010879: step 2187, loss 0.0443413, acc 0.96875\n",
      "2019-06-29T16:33:16.269217: step 2188, loss 0.059355, acc 0.96875\n",
      "2019-06-29T16:33:16.534267: step 2189, loss 0.0379152, acc 0.984375\n",
      "2019-06-29T16:33:16.802542: step 2190, loss 0.0735703, acc 0.96875\n",
      "2019-06-29T16:33:17.067626: step 2191, loss 0.0460633, acc 0.984375\n",
      "2019-06-29T16:33:17.335769: step 2192, loss 0.0420914, acc 1\n",
      "2019-06-29T16:33:17.596729: step 2193, loss 0.0636515, acc 0.984375\n",
      "2019-06-29T16:33:17.863369: step 2194, loss 0.103819, acc 0.984375\n",
      "2019-06-29T16:33:18.119052: step 2195, loss 0.0762235, acc 0.96875\n",
      "2019-06-29T16:33:18.384093: step 2196, loss 0.058906, acc 0.984375\n",
      "2019-06-29T16:33:18.650999: step 2197, loss 0.0770927, acc 0.96875\n",
      "2019-06-29T16:33:18.917507: step 2198, loss 0.0495093, acc 0.984375\n",
      "2019-06-29T16:33:19.182370: step 2199, loss 0.0706879, acc 0.96875\n",
      "2019-06-29T16:33:19.435297: step 2200, loss 0.0257588, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-06-29T16:33:20.077778: step 2200, loss 0.798038, acc 0.717636\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\mao\\Desktop\\Tensorflow\\CNN\\1\\cnn-text-classification-tf-master\\runs\\1561796526\\checkpoints\\model-2200\n",
      "\n",
      "2019-06-29T16:33:21.151953: step 2201, loss 0.056892, acc 0.984375\n",
      "2019-06-29T16:33:21.607558: step 2202, loss 0.0722118, acc 0.96875\n",
      "2019-06-29T16:33:21.868374: step 2203, loss 0.0257487, acc 1\n",
      "2019-06-29T16:33:22.149986: step 2204, loss 0.0737541, acc 0.984375\n",
      "2019-06-29T16:33:22.418031: step 2205, loss 0.0339311, acc 0.984375\n",
      "2019-06-29T16:33:22.700410: step 2206, loss 0.0774122, acc 0.96875\n",
      "2019-06-29T16:33:22.984091: step 2207, loss 0.0364713, acc 0.984375\n",
      "2019-06-29T16:33:23.284036: step 2208, loss 0.027213, acc 1\n",
      "2019-06-29T16:33:23.551003: step 2209, loss 0.0558453, acc 0.984375\n",
      "2019-06-29T16:33:23.833795: step 2210, loss 0.0760123, acc 0.984375\n",
      "2019-06-29T16:33:24.117918: step 2211, loss 0.0260735, acc 1\n",
      "2019-06-29T16:33:24.385305: step 2212, loss 0.0758491, acc 0.953125\n",
      "2019-06-29T16:33:24.668643: step 2213, loss 0.0147221, acc 1\n",
      "2019-06-29T16:33:24.950577: step 2214, loss 0.0350745, acc 0.96875\n",
      "2019-06-29T16:33:25.218430: step 2215, loss 0.0421388, acc 0.984375\n",
      "2019-06-29T16:33:25.506459: step 2216, loss 0.0447724, acc 0.984375\n",
      "2019-06-29T16:33:25.785509: step 2217, loss 0.0448114, acc 0.984375\n",
      "2019-06-29T16:33:26.067634: step 2218, loss 0.0260813, acc 1\n",
      "2019-06-29T16:33:26.347649: step 2219, loss 0.0329874, acc 1\n",
      "2019-06-29T16:33:26.648648: step 2220, loss 0.0521829, acc 1\n",
      "2019-06-29T16:33:26.917780: step 2221, loss 0.0472004, acc 0.984375\n",
      "2019-06-29T16:33:27.202333: step 2222, loss 0.0356296, acc 1\n",
      "2019-06-29T16:33:27.485515: step 2223, loss 0.0942529, acc 0.96875\n",
      "2019-06-29T16:33:27.784185: step 2224, loss 0.0567782, acc 0.96875\n",
      "2019-06-29T16:33:28.065155: step 2225, loss 0.0884882, acc 0.953125\n",
      "2019-06-29T16:33:28.350023: step 2226, loss 0.035069, acc 0.984375\n",
      "2019-06-29T16:33:28.633386: step 2227, loss 0.0578535, acc 0.984375\n",
      "2019-06-29T16:33:28.934109: step 2228, loss 0.0836775, acc 0.984375\n",
      "2019-06-29T16:33:29.222085: step 2229, loss 0.0261548, acc 1\n",
      "2019-06-29T16:33:29.502273: step 2230, loss 0.112851, acc 0.96875\n",
      "2019-06-29T16:33:29.768638: step 2231, loss 0.0953732, acc 0.984375\n",
      "2019-06-29T16:33:30.067441: step 2232, loss 0.05077, acc 1\n",
      "2019-06-29T16:33:30.350597: step 2233, loss 0.041586, acc 0.96875\n",
      "2019-06-29T16:33:30.618060: step 2234, loss 0.0607593, acc 0.984375\n",
      "2019-06-29T16:33:30.884693: step 2235, loss 0.0738111, acc 0.96875\n",
      "2019-06-29T16:33:31.166540: step 2236, loss 0.0910895, acc 0.96875\n",
      "2019-06-29T16:33:31.435139: step 2237, loss 0.0863155, acc 0.96875\n",
      "2019-06-29T16:33:31.699784: step 2238, loss 0.085419, acc 0.96875\n",
      "2019-06-29T16:33:31.962525: step 2239, loss 0.0677543, acc 0.984375\n",
      "2019-06-29T16:33:32.224232: step 2240, loss 0.0236798, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-29T16:33:32.490097: step 2241, loss 0.0238753, acc 0.984375\n",
      "2019-06-29T16:33:32.750413: step 2242, loss 0.048817, acc 1\n",
      "2019-06-29T16:33:33.016187: step 2243, loss 0.0596364, acc 0.984375\n",
      "2019-06-29T16:33:33.283770: step 2244, loss 0.0374513, acc 0.984375\n",
      "2019-06-29T16:33:33.535125: step 2245, loss 0.098257, acc 0.953125\n",
      "2019-06-29T16:33:33.799753: step 2246, loss 0.125929, acc 0.953125\n",
      "2019-06-29T16:33:34.072375: step 2247, loss 0.106668, acc 0.96875\n",
      "2019-06-29T16:33:34.349094: step 2248, loss 0.0379956, acc 0.984375\n",
      "2019-06-29T16:33:34.615703: step 2249, loss 0.0274331, acc 1\n",
      "2019-06-29T16:33:34.867731: step 2250, loss 0.0312125, acc 0.983333\n",
      "2019-06-29T16:33:35.140237: step 2251, loss 0.0368175, acc 0.984375\n",
      "2019-06-29T16:33:35.415874: step 2252, loss 0.0558538, acc 0.96875\n",
      "2019-06-29T16:33:35.682792: step 2253, loss 0.0365561, acc 0.984375\n",
      "2019-06-29T16:33:35.950502: step 2254, loss 0.0265307, acc 1\n",
      "2019-06-29T16:33:36.217323: step 2255, loss 0.0374058, acc 0.984375\n",
      "2019-06-29T16:33:36.484042: step 2256, loss 0.0258619, acc 1\n",
      "2019-06-29T16:33:36.749644: step 2257, loss 0.0936327, acc 0.953125\n",
      "2019-06-29T16:33:37.016292: step 2258, loss 0.0473366, acc 0.984375\n",
      "2019-06-29T16:33:37.281323: step 2259, loss 0.105644, acc 0.96875\n",
      "2019-06-29T16:33:37.565244: step 2260, loss 0.0312363, acc 0.984375\n",
      "2019-06-29T16:33:37.834087: step 2261, loss 0.0926464, acc 0.96875\n",
      "2019-06-29T16:33:38.085546: step 2262, loss 0.0334179, acc 1\n",
      "2019-06-29T16:33:38.350645: step 2263, loss 0.0440934, acc 0.96875\n",
      "2019-06-29T16:33:38.664069: step 2264, loss 0.0365503, acc 0.984375\n",
      "2019-06-29T16:33:38.964293: step 2265, loss 0.0412462, acc 1\n",
      "2019-06-29T16:33:39.248285: step 2266, loss 0.0707892, acc 0.96875\n",
      "2019-06-29T16:33:39.522806: step 2267, loss 0.0503765, acc 0.984375\n",
      "2019-06-29T16:33:39.799456: step 2268, loss 0.042341, acc 0.984375\n",
      "2019-06-29T16:33:40.067504: step 2269, loss 0.091409, acc 0.96875\n",
      "2019-06-29T16:33:40.349206: step 2270, loss 0.0469893, acc 0.984375\n",
      "2019-06-29T16:33:40.616929: step 2271, loss 0.0529935, acc 0.984375\n",
      "2019-06-29T16:33:40.903288: step 2272, loss 0.029623, acc 0.984375\n",
      "2019-06-29T16:33:41.168404: step 2273, loss 0.023235, acc 1\n",
      "2019-06-29T16:33:41.449519: step 2274, loss 0.0809345, acc 0.96875\n",
      "2019-06-29T16:33:41.732523: step 2275, loss 0.0300787, acc 1\n",
      "2019-06-29T16:33:42.000582: step 2276, loss 0.0521576, acc 0.984375\n",
      "2019-06-29T16:33:42.282180: step 2277, loss 0.0523923, acc 0.984375\n",
      "2019-06-29T16:33:42.550491: step 2278, loss 0.0375343, acc 1\n",
      "2019-06-29T16:33:42.832892: step 2279, loss 0.0766532, acc 0.96875\n",
      "2019-06-29T16:33:43.115852: step 2280, loss 0.141483, acc 0.953125\n",
      "2019-06-29T16:33:43.399565: step 2281, loss 0.0250283, acc 1\n",
      "2019-06-29T16:33:43.672610: step 2282, loss 0.0444326, acc 0.984375\n",
      "2019-06-29T16:33:43.950281: step 2283, loss 0.0906804, acc 0.984375\n",
      "2019-06-29T16:33:44.237999: step 2284, loss 0.0220915, acc 1\n",
      "2019-06-29T16:33:44.515695: step 2285, loss 0.0730487, acc 0.984375\n",
      "2019-06-29T16:33:44.799391: step 2286, loss 0.0405152, acc 0.984375\n",
      "2019-06-29T16:33:45.083122: step 2287, loss 0.0474701, acc 0.96875\n",
      "2019-06-29T16:33:45.365876: step 2288, loss 0.0356326, acc 0.984375\n",
      "2019-06-29T16:33:45.649472: step 2289, loss 0.0305685, acc 0.984375\n",
      "2019-06-29T16:33:45.938191: step 2290, loss 0.0520372, acc 0.984375\n",
      "2019-06-29T16:33:46.247323: step 2291, loss 0.0903329, acc 0.96875\n",
      "2019-06-29T16:33:46.516230: step 2292, loss 0.108208, acc 0.953125\n",
      "2019-06-29T16:33:46.776762: step 2293, loss 0.0183861, acc 1\n",
      "2019-06-29T16:33:47.033768: step 2294, loss 0.0841306, acc 0.984375\n",
      "2019-06-29T16:33:47.306762: step 2295, loss 0.0459795, acc 1\n",
      "2019-06-29T16:33:47.567771: step 2296, loss 0.040464, acc 0.984375\n",
      "2019-06-29T16:33:47.832720: step 2297, loss 0.0307644, acc 1\n",
      "2019-06-29T16:33:48.100077: step 2298, loss 0.0528848, acc 0.96875\n",
      "2019-06-29T16:33:48.373128: step 2299, loss 0.0335738, acc 1\n",
      "2019-06-29T16:33:48.634113: step 2300, loss 0.0160457, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-06-29T16:33:49.282337: step 2300, loss 0.829698, acc 0.711069\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\mao\\Desktop\\Tensorflow\\CNN\\1\\cnn-text-classification-tf-master\\runs\\1561796526\\checkpoints\\model-2300\n",
      "\n",
      "2019-06-29T16:33:50.384609: step 2301, loss 0.0127529, acc 1\n",
      "2019-06-29T16:33:50.823962: step 2302, loss 0.0988164, acc 0.96875\n",
      "2019-06-29T16:33:51.084513: step 2303, loss 0.030522, acc 0.984375\n",
      "2019-06-29T16:33:51.349627: step 2304, loss 0.0893542, acc 0.96875\n",
      "2019-06-29T16:33:51.616607: step 2305, loss 0.0170155, acc 1\n",
      "2019-06-29T16:33:51.882527: step 2306, loss 0.0627775, acc 0.984375\n",
      "2019-06-29T16:33:52.134091: step 2307, loss 0.0335201, acc 1\n",
      "2019-06-29T16:33:52.401068: step 2308, loss 0.0694938, acc 0.96875\n",
      "2019-06-29T16:33:52.667261: step 2309, loss 0.0332489, acc 1\n",
      "2019-06-29T16:33:52.933248: step 2310, loss 0.0468634, acc 0.984375\n",
      "2019-06-29T16:33:53.201378: step 2311, loss 0.0309755, acc 0.984375\n",
      "2019-06-29T16:33:53.468241: step 2312, loss 0.0245953, acc 1\n",
      "2019-06-29T16:33:53.746787: step 2313, loss 0.0420295, acc 1\n",
      "2019-06-29T16:33:54.011145: step 2314, loss 0.0803662, acc 0.96875\n",
      "2019-06-29T16:33:54.267984: step 2315, loss 0.0130351, acc 1\n",
      "2019-06-29T16:33:54.549875: step 2316, loss 0.0395311, acc 0.984375\n",
      "2019-06-29T16:33:54.841737: step 2317, loss 0.0680017, acc 0.96875\n",
      "2019-06-29T16:33:55.132655: step 2318, loss 0.0504588, acc 0.96875\n",
      "2019-06-29T16:33:55.426901: step 2319, loss 0.09142, acc 0.984375\n",
      "2019-06-29T16:33:55.684053: step 2320, loss 0.0133989, acc 1\n",
      "2019-06-29T16:33:55.981702: step 2321, loss 0.0714002, acc 0.96875\n",
      "2019-06-29T16:33:56.283046: step 2322, loss 0.0327235, acc 0.984375\n",
      "2019-06-29T16:33:56.580825: step 2323, loss 0.0595169, acc 0.984375\n",
      "2019-06-29T16:33:56.866755: step 2324, loss 0.0540714, acc 0.96875\n",
      "2019-06-29T16:33:57.164036: step 2325, loss 0.0137878, acc 1\n",
      "2019-06-29T16:33:57.433190: step 2326, loss 0.0710674, acc 0.96875\n",
      "2019-06-29T16:33:57.715965: step 2327, loss 0.0343537, acc 0.984375\n",
      "2019-06-29T16:33:57.982987: step 2328, loss 0.0603356, acc 0.96875\n",
      "2019-06-29T16:33:58.282029: step 2329, loss 0.023243, acc 1\n",
      "2019-06-29T16:33:58.550315: step 2330, loss 0.0278056, acc 1\n",
      "2019-06-29T16:33:58.832057: step 2331, loss 0.0700605, acc 0.96875\n",
      "2019-06-29T16:33:59.114835: step 2332, loss 0.0611244, acc 0.984375\n",
      "2019-06-29T16:33:59.398895: step 2333, loss 0.0301066, acc 0.984375\n",
      "2019-06-29T16:33:59.682720: step 2334, loss 0.0296078, acc 1\n",
      "2019-06-29T16:33:59.966522: step 2335, loss 0.0208698, acc 1\n",
      "2019-06-29T16:34:00.246931: step 2336, loss 0.0429492, acc 0.984375\n",
      "2019-06-29T16:34:00.532625: step 2337, loss 0.0359466, acc 1\n",
      "2019-06-29T16:34:00.805633: step 2338, loss 0.0205367, acc 1\n",
      "2019-06-29T16:34:01.099405: step 2339, loss 0.0524122, acc 0.984375\n",
      "2019-06-29T16:34:01.382716: step 2340, loss 0.0720879, acc 0.984375\n",
      "2019-06-29T16:34:01.666389: step 2341, loss 0.069865, acc 0.984375\n",
      "2019-06-29T16:34:01.948106: step 2342, loss 0.0365215, acc 0.984375\n",
      "2019-06-29T16:34:02.216269: step 2343, loss 0.0253383, acc 0.984375\n",
      "2019-06-29T16:34:02.504659: step 2344, loss 0.00961436, acc 1\n",
      "2019-06-29T16:34:02.780560: step 2345, loss 0.0420161, acc 0.984375\n",
      "2019-06-29T16:34:03.042978: step 2346, loss 0.0603414, acc 0.96875\n",
      "2019-06-29T16:34:03.307487: step 2347, loss 0.0141995, acc 1\n",
      "2019-06-29T16:34:03.593500: step 2348, loss 0.0062854, acc 1\n",
      "2019-06-29T16:34:03.850055: step 2349, loss 0.0329059, acc 0.984375\n",
      "2019-06-29T16:34:04.126910: step 2350, loss 0.0313854, acc 0.984375\n",
      "2019-06-29T16:34:04.397338: step 2351, loss 0.0760088, acc 0.984375\n",
      "2019-06-29T16:34:04.672439: step 2352, loss 0.0705144, acc 0.96875\n",
      "2019-06-29T16:34:04.947635: step 2353, loss 0.0375697, acc 0.984375\n",
      "2019-06-29T16:34:05.223832: step 2354, loss 0.0664054, acc 0.984375\n",
      "2019-06-29T16:34:05.497370: step 2355, loss 0.0549397, acc 0.96875\n",
      "2019-06-29T16:34:05.843617: step 2356, loss 0.01587, acc 1\n",
      "2019-06-29T16:34:06.120116: step 2357, loss 0.105994, acc 0.96875\n",
      "2019-06-29T16:34:06.395312: step 2358, loss 0.0667836, acc 0.984375\n",
      "2019-06-29T16:34:06.671509: step 2359, loss 0.023557, acc 1\n",
      "2019-06-29T16:34:06.960715: step 2360, loss 0.0409761, acc 0.96875\n",
      "2019-06-29T16:34:07.259929: step 2361, loss 0.0328031, acc 1\n",
      "2019-06-29T16:34:07.568148: step 2362, loss 0.0542517, acc 0.984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-29T16:34:07.875366: step 2363, loss 0.00884176, acc 1\n",
      "2019-06-29T16:34:08.148194: step 2364, loss 0.0866849, acc 0.984375\n",
      "2019-06-29T16:34:08.413308: step 2365, loss 0.0476504, acc 0.96875\n",
      "2019-06-29T16:34:08.688376: step 2366, loss 0.0417387, acc 0.984375\n",
      "2019-06-29T16:34:09.003600: step 2367, loss 0.0696938, acc 0.96875\n",
      "2019-06-29T16:34:09.304816: step 2368, loss 0.0309975, acc 0.984375\n",
      "2019-06-29T16:34:09.613712: step 2369, loss 0.0351331, acc 1\n",
      "2019-06-29T16:34:09.864400: step 2370, loss 0.0136604, acc 1\n",
      "2019-06-29T16:34:10.143619: step 2371, loss 0.0312579, acc 1\n",
      "2019-06-29T16:34:10.399847: step 2372, loss 0.0834945, acc 0.9375\n",
      "2019-06-29T16:34:10.690504: step 2373, loss 0.163708, acc 0.921875\n",
      "2019-06-29T16:34:10.984714: step 2374, loss 0.0239738, acc 1\n",
      "2019-06-29T16:34:11.318232: step 2375, loss 0.0743736, acc 0.953125\n",
      "2019-06-29T16:34:11.607555: step 2376, loss 0.0267287, acc 1\n",
      "2019-06-29T16:34:11.895869: step 2377, loss 0.0107909, acc 1\n",
      "2019-06-29T16:34:12.178505: step 2378, loss 0.0253944, acc 1\n",
      "2019-06-29T16:34:12.466453: step 2379, loss 0.0287627, acc 0.984375\n",
      "2019-06-29T16:34:12.757015: step 2380, loss 0.0390658, acc 0.984375\n",
      "2019-06-29T16:34:13.038215: step 2381, loss 0.0401171, acc 0.984375\n",
      "2019-06-29T16:34:13.309993: step 2382, loss 0.0215776, acc 1\n",
      "2019-06-29T16:34:13.659700: step 2383, loss 0.0578588, acc 0.984375\n",
      "2019-06-29T16:34:13.969406: step 2384, loss 0.0601729, acc 0.984375\n",
      "2019-06-29T16:34:14.252373: step 2385, loss 0.0511897, acc 1\n",
      "2019-06-29T16:34:14.539309: step 2386, loss 0.0846057, acc 0.96875\n",
      "2019-06-29T16:34:14.826432: step 2387, loss 0.0316861, acc 1\n",
      "2019-06-29T16:34:15.108322: step 2388, loss 0.0586764, acc 0.984375\n",
      "2019-06-29T16:34:15.401532: step 2389, loss 0.180196, acc 0.953125\n",
      "2019-06-29T16:34:15.813824: step 2390, loss 0.0423989, acc 0.984375\n",
      "2019-06-29T16:34:16.170079: step 2391, loss 0.0445723, acc 0.984375\n",
      "2019-06-29T16:34:16.477297: step 2392, loss 0.0714826, acc 0.96875\n",
      "2019-06-29T16:34:16.776504: step 2393, loss 0.0275732, acc 1\n",
      "2019-06-29T16:34:17.066712: step 2394, loss 0.0443382, acc 0.984375\n",
      "2019-06-29T16:34:17.348259: step 2395, loss 0.0173588, acc 1\n",
      "2019-06-29T16:34:17.681496: step 2396, loss 0.0241404, acc 1\n",
      "2019-06-29T16:34:17.972403: step 2397, loss 0.0613864, acc 0.96875\n",
      "2019-06-29T16:34:18.255982: step 2398, loss 0.0573605, acc 0.984375\n",
      "2019-06-29T16:34:18.537235: step 2399, loss 0.0327963, acc 0.984375\n",
      "2019-06-29T16:34:18.824263: step 2400, loss 0.0396224, acc 0.983333\n",
      "\n",
      "Evaluation:\n",
      "2019-06-29T16:34:19.513139: step 2400, loss 0.848153, acc 0.718574\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\mao\\Desktop\\Tensorflow\\CNN\\1\\cnn-text-classification-tf-master\\runs\\1561796526\\checkpoints\\model-2400\n",
      "\n",
      "2019-06-29T16:34:20.619162: step 2401, loss 0.0320447, acc 0.984375\n",
      "2019-06-29T16:34:21.052255: step 2402, loss 0.039667, acc 0.984375\n",
      "2019-06-29T16:34:21.316748: step 2403, loss 0.0373187, acc 0.984375\n",
      "2019-06-29T16:34:21.582375: step 2404, loss 0.00766937, acc 1\n",
      "2019-06-29T16:34:21.849852: step 2405, loss 0.012703, acc 1\n",
      "2019-06-29T16:34:22.109115: step 2406, loss 0.0246346, acc 0.984375\n",
      "2019-06-29T16:34:22.392554: step 2407, loss 0.119077, acc 0.96875\n",
      "2019-06-29T16:34:22.634140: step 2408, loss 0.0425869, acc 0.984375\n",
      "2019-06-29T16:34:22.906168: step 2409, loss 0.0111686, acc 1\n",
      "2019-06-29T16:34:23.171797: step 2410, loss 0.0122608, acc 1\n",
      "2019-06-29T16:34:23.446301: step 2411, loss 0.036532, acc 1\n",
      "2019-06-29T16:34:23.711930: step 2412, loss 0.0152356, acc 1\n",
      "2019-06-29T16:34:23.979969: step 2413, loss 0.0366664, acc 0.96875\n",
      "2019-06-29T16:34:24.229972: step 2414, loss 0.0137634, acc 1\n",
      "2019-06-29T16:34:24.497881: step 2415, loss 0.0148218, acc 1\n",
      "2019-06-29T16:34:24.773692: step 2416, loss 0.0798306, acc 0.96875\n",
      "2019-06-29T16:34:25.026013: step 2417, loss 0.0313791, acc 0.984375\n",
      "2019-06-29T16:34:25.308281: step 2418, loss 0.0299239, acc 1\n",
      "2019-06-29T16:34:25.582212: step 2419, loss 0.018091, acc 1\n",
      "2019-06-29T16:34:25.850170: step 2420, loss 0.162045, acc 0.9375\n",
      "2019-06-29T16:34:26.105375: step 2421, loss 0.0976553, acc 0.984375\n",
      "2019-06-29T16:34:26.376382: step 2422, loss 0.0432965, acc 0.984375\n",
      "2019-06-29T16:34:26.661133: step 2423, loss 0.0198053, acc 1\n",
      "2019-06-29T16:34:26.966267: step 2424, loss 0.102419, acc 0.96875\n",
      "2019-06-29T16:34:27.252955: step 2425, loss 0.0691664, acc 0.96875\n",
      "2019-06-29T16:34:27.535354: step 2426, loss 0.0338936, acc 0.984375\n",
      "2019-06-29T16:34:27.845225: step 2427, loss 0.0237229, acc 1\n",
      "2019-06-29T16:34:28.137385: step 2428, loss 0.019552, acc 1\n",
      "2019-06-29T16:34:28.404278: step 2429, loss 0.0279624, acc 0.984375\n",
      "2019-06-29T16:34:28.685708: step 2430, loss 0.0175835, acc 1\n",
      "2019-06-29T16:34:28.976713: step 2431, loss 0.0160401, acc 1\n",
      "2019-06-29T16:34:29.255632: step 2432, loss 0.0327879, acc 0.984375\n",
      "2019-06-29T16:34:29.534573: step 2433, loss 0.0812871, acc 0.96875\n",
      "2019-06-29T16:34:29.811347: step 2434, loss 0.0241267, acc 0.984375\n",
      "2019-06-29T16:34:30.143544: step 2435, loss 0.0232836, acc 1\n",
      "2019-06-29T16:34:30.424797: step 2436, loss 0.0318648, acc 0.984375\n",
      "2019-06-29T16:34:30.714169: step 2437, loss 0.0778533, acc 0.984375\n",
      "2019-06-29T16:34:31.032310: step 2438, loss 0.0686808, acc 0.96875\n",
      "2019-06-29T16:34:31.344815: step 2439, loss 0.0436596, acc 0.96875\n",
      "2019-06-29T16:34:31.626067: step 2440, loss 0.0340749, acc 0.984375\n",
      "2019-06-29T16:34:31.960267: step 2441, loss 0.0645571, acc 0.96875\n",
      "2019-06-29T16:34:32.267748: step 2442, loss 0.0207986, acc 1\n",
      "2019-06-29T16:34:32.564626: step 2443, loss 0.0474494, acc 0.984375\n",
      "2019-06-29T16:34:32.845881: step 2444, loss 0.0329892, acc 1\n",
      "2019-06-29T16:34:33.132772: step 2445, loss 0.0677821, acc 0.984375\n",
      "2019-06-29T16:34:33.460900: step 2446, loss 0.00501116, acc 1\n",
      "2019-06-29T16:34:33.742153: step 2447, loss 0.027845, acc 0.984375\n",
      "2019-06-29T16:34:34.013380: step 2448, loss 0.0437182, acc 0.984375\n",
      "2019-06-29T16:34:34.310260: step 2449, loss 0.0528214, acc 0.96875\n",
      "2019-06-29T16:34:34.638390: step 2450, loss 0.0548571, acc 0.953125\n",
      "2019-06-29T16:34:35.052256: step 2451, loss 0.019109, acc 1\n",
      "2019-06-29T16:34:35.396011: step 2452, loss 0.0368186, acc 0.984375\n",
      "2019-06-29T16:34:35.661640: step 2453, loss 0.00866185, acc 1\n",
      "2019-06-29T16:34:35.932754: step 2454, loss 0.0153509, acc 1\n",
      "2019-06-29T16:34:36.198382: step 2455, loss 0.0141011, acc 1\n",
      "2019-06-29T16:34:36.464012: step 2456, loss 0.0182334, acc 1\n",
      "2019-06-29T16:34:36.729640: step 2457, loss 0.0110904, acc 1\n",
      "2019-06-29T16:34:37.017390: step 2458, loss 0.0168888, acc 1\n",
      "2019-06-29T16:34:37.298643: step 2459, loss 0.0360867, acc 0.984375\n",
      "2019-06-29T16:34:37.579897: step 2460, loss 0.0478215, acc 0.984375\n",
      "2019-06-29T16:34:37.845526: step 2461, loss 0.0306842, acc 0.984375\n",
      "2019-06-29T16:34:38.100948: step 2462, loss 0.0740308, acc 0.953125\n",
      "2019-06-29T16:34:38.433083: step 2463, loss 0.0290016, acc 0.984375\n",
      "2019-06-29T16:34:38.774093: step 2464, loss 0.017523, acc 1\n",
      "2019-06-29T16:34:39.042459: step 2465, loss 0.157188, acc 0.9375\n",
      "2019-06-29T16:34:39.333371: step 2466, loss 0.0163059, acc 1\n",
      "2019-06-29T16:34:39.599560: step 2467, loss 0.037095, acc 0.984375\n",
      "2019-06-29T16:34:39.886763: step 2468, loss 0.039192, acc 0.984375\n",
      "2019-06-29T16:34:40.188938: step 2469, loss 0.0213155, acc 1\n",
      "2019-06-29T16:34:40.503161: step 2470, loss 0.04167, acc 0.984375\n",
      "2019-06-29T16:34:40.837399: step 2471, loss 0.0367927, acc 0.984375\n",
      "2019-06-29T16:34:41.180645: step 2472, loss 0.0181654, acc 1\n",
      "2019-06-29T16:34:41.596940: step 2473, loss 0.0410702, acc 0.96875\n",
      "2019-06-29T16:34:41.883577: step 2474, loss 0.00823106, acc 1\n",
      "2019-06-29T16:34:42.152332: step 2475, loss 0.0117712, acc 1\n",
      "2019-06-29T16:34:42.423526: step 2476, loss 0.0337784, acc 0.984375\n",
      "2019-06-29T16:34:42.729744: step 2477, loss 0.0138398, acc 1\n",
      "2019-06-29T16:34:43.040552: step 2478, loss 0.00945181, acc 1\n",
      "2019-06-29T16:34:43.524896: step 2479, loss 0.0577021, acc 0.96875\n",
      "2019-06-29T16:34:43.786016: step 2480, loss 0.0471986, acc 0.984375\n",
      "2019-06-29T16:34:44.074127: step 2481, loss 0.02499, acc 1\n",
      "2019-06-29T16:34:44.416641: step 2482, loss 0.10176, acc 0.96875\n",
      "2019-06-29T16:34:44.725228: step 2483, loss 0.0133244, acc 1\n",
      "2019-06-29T16:34:45.030180: step 2484, loss 0.0326388, acc 1\n",
      "2019-06-29T16:34:45.347802: step 2485, loss 0.0130165, acc 1\n",
      "2019-06-29T16:34:45.628716: step 2486, loss 0.0209567, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-29T16:34:45.916051: step 2487, loss 0.0238156, acc 1\n",
      "2019-06-29T16:34:46.197640: step 2488, loss 0.050281, acc 0.96875\n",
      "2019-06-29T16:34:46.603420: step 2489, loss 0.0925736, acc 0.96875\n",
      "2019-06-29T16:34:46.880371: step 2490, loss 0.0649949, acc 0.984375\n",
      "2019-06-29T16:34:47.164825: step 2491, loss 0.0412876, acc 0.984375\n",
      "2019-06-29T16:34:47.477520: step 2492, loss 0.0210362, acc 1\n",
      "2019-06-29T16:34:47.784279: step 2493, loss 0.0315123, acc 1\n",
      "2019-06-29T16:34:48.062834: step 2494, loss 0.0713232, acc 0.984375\n",
      "2019-06-29T16:34:48.367269: step 2495, loss 0.0328529, acc 0.984375\n",
      "2019-06-29T16:34:48.678471: step 2496, loss 0.101413, acc 0.9375\n",
      "2019-06-29T16:34:48.964622: step 2497, loss 0.0305034, acc 1\n",
      "2019-06-29T16:34:49.301788: step 2498, loss 0.0580122, acc 0.984375\n",
      "2019-06-29T16:34:49.583042: step 2499, loss 0.023461, acc 0.984375\n",
      "2019-06-29T16:34:49.879921: step 2500, loss 0.0424162, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2019-06-29T16:34:50.645945: step 2500, loss 0.878361, acc 0.710131\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\mao\\Desktop\\Tensorflow\\CNN\\1\\cnn-text-classification-tf-master\\runs\\1561796526\\checkpoints\\model-2500\n",
      "\n",
      "2019-06-29T16:34:51.759944: step 2501, loss 0.0261812, acc 0.984375\n",
      "2019-06-29T16:34:52.255133: step 2502, loss 0.0910752, acc 0.984375\n",
      "2019-06-29T16:34:52.536387: step 2503, loss 0.0247717, acc 1\n",
      "2019-06-29T16:34:52.817642: step 2504, loss 0.0265668, acc 1\n",
      "2019-06-29T16:34:53.088722: step 2505, loss 0.0348851, acc 0.984375\n",
      "2019-06-29T16:34:53.363728: step 2506, loss 0.0417676, acc 0.984375\n",
      "2019-06-29T16:34:53.644981: step 2507, loss 0.0156823, acc 1\n",
      "2019-06-29T16:34:53.917428: step 2508, loss 0.0499103, acc 1\n",
      "2019-06-29T16:34:54.198681: step 2509, loss 0.0453195, acc 0.984375\n",
      "2019-06-29T16:34:54.479936: step 2510, loss 0.0371985, acc 0.984375\n",
      "2019-06-29T16:34:54.761190: step 2511, loss 0.0261345, acc 1\n",
      "2019-06-29T16:34:55.033964: step 2512, loss 0.0224029, acc 1\n",
      "2019-06-29T16:34:55.299594: step 2513, loss 0.0218292, acc 1\n",
      "2019-06-29T16:34:55.580848: step 2514, loss 0.0419731, acc 0.984375\n",
      "2019-06-29T16:34:55.846477: step 2515, loss 0.0209621, acc 1\n",
      "2019-06-29T16:34:56.117582: step 2516, loss 0.0243267, acc 1\n",
      "2019-06-29T16:34:56.383211: step 2517, loss 0.0791421, acc 0.953125\n",
      "2019-06-29T16:34:56.648840: step 2518, loss 0.0251469, acc 0.984375\n",
      "2019-06-29T16:34:56.919733: step 2519, loss 0.0234383, acc 1\n",
      "2019-06-29T16:34:57.232238: step 2520, loss 0.0460991, acc 0.96875\n",
      "2019-06-29T16:34:57.513492: step 2521, loss 0.0110827, acc 1\n",
      "2019-06-29T16:34:57.779121: step 2522, loss 0.0308628, acc 1\n",
      "2019-06-29T16:34:58.049957: step 2523, loss 0.0557064, acc 0.96875\n",
      "2019-06-29T16:34:58.315585: step 2524, loss 0.0338634, acc 1\n",
      "2019-06-29T16:34:58.659342: step 2525, loss 0.0281025, acc 1\n",
      "2019-06-29T16:34:58.961947: step 2526, loss 0.0197419, acc 1\n",
      "2019-06-29T16:34:59.253601: step 2527, loss 0.0531263, acc 0.984375\n",
      "2019-06-29T16:34:59.547464: step 2528, loss 0.0313998, acc 0.984375\n",
      "2019-06-29T16:34:59.813093: step 2529, loss 0.0474176, acc 0.984375\n",
      "2019-06-29T16:35:00.085676: step 2530, loss 0.0819295, acc 0.96875\n",
      "2019-06-29T16:35:00.424886: step 2531, loss 0.0566586, acc 0.96875\n",
      "2019-06-29T16:35:00.706138: step 2532, loss 0.0200876, acc 1\n",
      "2019-06-29T16:35:00.992857: step 2533, loss 0.0443053, acc 1\n",
      "2019-06-29T16:35:01.258485: step 2534, loss 0.0506446, acc 0.984375\n",
      "2019-06-29T16:35:01.539741: step 2535, loss 0.0489571, acc 0.984375\n",
      "2019-06-29T16:35:01.820994: step 2536, loss 0.0236884, acc 1\n",
      "2019-06-29T16:35:02.109537: step 2537, loss 0.0127003, acc 1\n",
      "2019-06-29T16:35:02.390790: step 2538, loss 0.00910987, acc 1\n",
      "2019-06-29T16:35:02.672045: step 2539, loss 0.129114, acc 0.984375\n",
      "2019-06-29T16:35:02.943162: step 2540, loss 0.0150498, acc 1\n",
      "2019-06-29T16:35:03.224418: step 2541, loss 0.0439282, acc 0.96875\n",
      "2019-06-29T16:35:03.505671: step 2542, loss 0.0526039, acc 0.984375\n",
      "2019-06-29T16:35:03.813941: step 2543, loss 0.0154398, acc 1\n",
      "2019-06-29T16:35:04.100505: step 2544, loss 0.0446642, acc 0.984375\n",
      "2019-06-29T16:35:04.366133: step 2545, loss 0.0296676, acc 1\n",
      "2019-06-29T16:35:04.663014: step 2546, loss 0.0369157, acc 0.984375\n",
      "2019-06-29T16:35:04.965291: step 2547, loss 0.0260499, acc 1\n",
      "2019-06-29T16:35:05.243937: step 2548, loss 0.0146728, acc 1\n",
      "2019-06-29T16:35:05.525191: step 2549, loss 0.0156811, acc 0.984375\n",
      "2019-06-29T16:35:05.790820: step 2550, loss 0.0134407, acc 1\n",
      "2019-06-29T16:35:06.093104: step 2551, loss 0.0194128, acc 1\n",
      "2019-06-29T16:35:06.374358: step 2552, loss 0.0293194, acc 0.984375\n",
      "2019-06-29T16:35:06.671238: step 2553, loss 0.0180372, acc 1\n",
      "2019-06-29T16:35:07.084474: step 2554, loss 0.0273399, acc 0.984375\n",
      "2019-06-29T16:35:07.396979: step 2555, loss 0.0354271, acc 0.984375\n",
      "2019-06-29T16:35:07.771984: step 2556, loss 0.0308693, acc 1\n",
      "2019-06-29T16:35:08.074520: step 2557, loss 0.016981, acc 1\n",
      "2019-06-29T16:35:08.355774: step 2558, loss 0.0280147, acc 0.984375\n",
      "2019-06-29T16:35:08.652654: step 2559, loss 0.029471, acc 0.984375\n",
      "2019-06-29T16:35:08.984564: step 2560, loss 0.0824144, acc 0.953125\n",
      "2019-06-29T16:35:09.406445: step 2561, loss 0.0572292, acc 0.984375\n",
      "2019-06-29T16:35:09.703324: step 2562, loss 0.0261718, acc 1\n",
      "2019-06-29T16:35:10.006852: step 2563, loss 0.0133908, acc 1\n",
      "2019-06-29T16:35:10.319357: step 2564, loss 0.0194691, acc 1\n",
      "2019-06-29T16:35:10.600611: step 2565, loss 0.0134045, acc 1\n",
      "2019-06-29T16:35:10.887270: step 2566, loss 0.00859789, acc 1\n",
      "2019-06-29T16:35:11.168524: step 2567, loss 0.0796104, acc 0.96875\n",
      "2019-06-29T16:35:11.512280: step 2568, loss 0.0393911, acc 0.984375\n",
      "2019-06-29T16:35:11.793534: step 2569, loss 0.0144859, acc 1\n",
      "2019-06-29T16:35:12.080327: step 2570, loss 0.046195, acc 0.984375\n",
      "2019-06-29T16:35:12.377207: step 2571, loss 0.0350284, acc 0.984375\n",
      "2019-06-29T16:35:12.674087: step 2572, loss 0.0598629, acc 0.96875\n",
      "2019-06-29T16:35:12.945156: step 2573, loss 0.0508829, acc 0.96875\n",
      "2019-06-29T16:35:13.210786: step 2574, loss 0.0162829, acc 1\n",
      "2019-06-29T16:35:13.476415: step 2575, loss 0.0150607, acc 1\n",
      "2019-06-29T16:35:13.762058: step 2576, loss 0.00743737, acc 1\n",
      "2019-06-29T16:35:14.033148: step 2577, loss 0.0126079, acc 1\n",
      "2019-06-29T16:35:14.298778: step 2578, loss 0.0125938, acc 1\n",
      "2019-06-29T16:35:14.611282: step 2579, loss 0.00775035, acc 1\n",
      "2019-06-29T16:35:14.898126: step 2580, loss 0.0123996, acc 1\n",
      "2019-06-29T16:35:15.195006: step 2581, loss 0.0263926, acc 1\n",
      "2019-06-29T16:35:15.496661: step 2582, loss 0.054422, acc 0.984375\n",
      "2019-06-29T16:35:15.780921: step 2583, loss 0.00671426, acc 1\n",
      "2019-06-29T16:35:16.100065: step 2584, loss 0.0445665, acc 0.984375\n",
      "2019-06-29T16:35:16.381320: step 2585, loss 0.00907116, acc 1\n",
      "2019-06-29T16:35:16.662573: step 2586, loss 0.0516891, acc 0.984375\n",
      "2019-06-29T16:35:16.950623: step 2587, loss 0.0318239, acc 0.984375\n",
      "2019-06-29T16:35:17.231877: step 2588, loss 0.0303908, acc 1\n",
      "2019-06-29T16:35:17.497506: step 2589, loss 0.0143765, acc 1\n",
      "2019-06-29T16:35:17.794386: step 2590, loss 0.0594843, acc 0.984375\n",
      "2019-06-29T16:35:18.082342: step 2591, loss 0.0149088, acc 1\n",
      "2019-06-29T16:35:18.363598: step 2592, loss 0.0165004, acc 1\n",
      "2019-06-29T16:35:18.629225: step 2593, loss 0.0442706, acc 0.96875\n",
      "2019-06-29T16:35:18.916219: step 2594, loss 0.0232394, acc 1\n",
      "2019-06-29T16:35:19.213101: step 2595, loss 0.0418201, acc 0.984375\n",
      "2019-06-29T16:35:19.494353: step 2596, loss 0.0133744, acc 1\n",
      "2019-06-29T16:35:19.775607: step 2597, loss 0.0412057, acc 0.984375\n",
      "2019-06-29T16:35:20.062458: step 2598, loss 0.00624771, acc 1\n",
      "2019-06-29T16:35:20.343712: step 2599, loss 0.00775694, acc 1\n",
      "2019-06-29T16:35:20.632493: step 2600, loss 0.0109725, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-06-29T16:35:21.387033: step 2600, loss 0.914783, acc 0.71576\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\mao\\Desktop\\Tensorflow\\CNN\\1\\cnn-text-classification-tf-master\\runs\\1561796526\\checkpoints\\model-2600\n",
      "\n",
      "2019-06-29T16:35:22.625516: step 2601, loss 0.0761358, acc 0.953125\n",
      "2019-06-29T16:35:23.041732: step 2602, loss 0.0156769, acc 1\n",
      "2019-06-29T16:35:23.307361: step 2603, loss 0.0203398, acc 0.984375\n",
      "2019-06-29T16:35:23.588616: step 2604, loss 0.0178669, acc 1\n",
      "2019-06-29T16:35:23.854245: step 2605, loss 0.0151597, acc 1\n",
      "2019-06-29T16:35:24.110061: step 2606, loss 0.032601, acc 0.984375\n",
      "2019-06-29T16:35:24.375691: step 2607, loss 0.0137718, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-29T16:35:24.656944: step 2608, loss 0.00471947, acc 1\n",
      "2019-06-29T16:35:24.912607: step 2609, loss 0.00858212, acc 1\n",
      "2019-06-29T16:35:25.178237: step 2610, loss 0.0310047, acc 1\n",
      "2019-06-29T16:35:25.443864: step 2611, loss 0.0205614, acc 1\n",
      "2019-06-29T16:35:25.725119: step 2612, loss 0.0106112, acc 1\n",
      "2019-06-29T16:35:25.980361: step 2613, loss 0.0115169, acc 1\n",
      "2019-06-29T16:35:26.245991: step 2614, loss 0.0181013, acc 1\n",
      "2019-06-29T16:35:26.511619: step 2615, loss 0.00961282, acc 1\n",
      "2019-06-29T16:35:26.792873: step 2616, loss 0.102211, acc 0.96875\n",
      "2019-06-29T16:35:27.047991: step 2617, loss 0.0678772, acc 0.96875\n",
      "2019-06-29T16:35:27.313620: step 2618, loss 0.0106787, acc 1\n",
      "2019-06-29T16:35:27.579248: step 2619, loss 0.0305929, acc 0.984375\n",
      "2019-06-29T16:35:27.860503: step 2620, loss 0.0450209, acc 0.984375\n",
      "2019-06-29T16:35:28.131434: step 2621, loss 0.0259887, acc 1\n",
      "2019-06-29T16:35:28.397062: step 2622, loss 0.0156909, acc 1\n",
      "2019-06-29T16:35:28.662691: step 2623, loss 0.0256455, acc 0.984375\n",
      "2019-06-29T16:35:28.932518: step 2624, loss 0.0239081, acc 1\n",
      "2019-06-29T16:35:29.198145: step 2625, loss 0.0074481, acc 1\n",
      "2019-06-29T16:35:29.463774: step 2626, loss 0.071599, acc 0.984375\n",
      "2019-06-29T16:35:29.729233: step 2627, loss 0.00682328, acc 1\n",
      "2019-06-29T16:35:30.000288: step 2628, loss 0.0175464, acc 1\n",
      "2019-06-29T16:35:30.265917: step 2629, loss 0.0181821, acc 1\n",
      "2019-06-29T16:35:30.547172: step 2630, loss 0.0454909, acc 0.984375\n",
      "2019-06-29T16:35:30.828426: step 2631, loss 0.0584216, acc 0.96875\n",
      "2019-06-29T16:35:31.172914: step 2632, loss 0.0337636, acc 0.984375\n",
      "2019-06-29T16:35:31.666538: step 2633, loss 0.0424856, acc 0.984375\n",
      "2019-06-29T16:35:31.939542: step 2634, loss 0.023583, acc 1\n",
      "2019-06-29T16:35:32.220797: step 2635, loss 0.0337087, acc 0.984375\n",
      "2019-06-29T16:35:32.502050: step 2636, loss 0.0135554, acc 1\n",
      "2019-06-29T16:35:32.767679: step 2637, loss 0.0245588, acc 1\n",
      "2019-06-29T16:35:33.038525: step 2638, loss 0.0195258, acc 1\n",
      "2019-06-29T16:35:33.319780: step 2639, loss 0.0115542, acc 1\n",
      "2019-06-29T16:35:33.585409: step 2640, loss 0.0147676, acc 1\n",
      "2019-06-29T16:35:33.866663: step 2641, loss 0.0215838, acc 1\n",
      "2019-06-29T16:35:34.137627: step 2642, loss 0.0341402, acc 0.96875\n",
      "2019-06-29T16:35:34.450132: step 2643, loss 0.0265063, acc 0.984375\n",
      "2019-06-29T16:35:34.731387: step 2644, loss 0.0585657, acc 0.984375\n",
      "2019-06-29T16:35:35.002459: step 2645, loss 0.0593858, acc 0.96875\n",
      "2019-06-29T16:35:35.283714: step 2646, loss 0.0151366, acc 1\n",
      "2019-06-29T16:35:35.580592: step 2647, loss 0.0512518, acc 0.984375\n",
      "2019-06-29T16:35:35.877472: step 2648, loss 0.0450066, acc 0.984375\n",
      "2019-06-29T16:35:36.179846: step 2649, loss 0.0178549, acc 1\n",
      "2019-06-29T16:35:36.461101: step 2650, loss 0.0043967, acc 1\n",
      "2019-06-29T16:35:36.742355: step 2651, loss 0.0859727, acc 0.953125\n",
      "2019-06-29T16:35:37.029185: step 2652, loss 0.0306143, acc 1\n",
      "2019-06-29T16:35:37.326064: step 2653, loss 0.0109778, acc 1\n",
      "2019-06-29T16:35:37.638568: step 2654, loss 0.0114264, acc 1\n",
      "2019-06-29T16:35:37.972304: step 2655, loss 0.0365369, acc 0.984375\n",
      "2019-06-29T16:35:38.299601: step 2656, loss 0.0653519, acc 0.96875\n",
      "2019-06-29T16:35:38.627731: step 2657, loss 0.00965631, acc 1\n",
      "2019-06-29T16:35:38.929887: step 2658, loss 0.060506, acc 0.953125\n",
      "2019-06-29T16:35:39.211142: step 2659, loss 0.0338302, acc 0.984375\n",
      "2019-06-29T16:35:39.492395: step 2660, loss 0.0255524, acc 0.984375\n",
      "2019-06-29T16:35:39.773649: step 2661, loss 0.0368066, acc 0.984375\n",
      "2019-06-29T16:35:40.075868: step 2662, loss 0.0108564, acc 1\n",
      "2019-06-29T16:35:40.372748: step 2663, loss 0.0146905, acc 1\n",
      "2019-06-29T16:35:40.716502: step 2664, loss 0.0353981, acc 1\n",
      "2019-06-29T16:35:41.005203: step 2665, loss 0.0476812, acc 0.984375\n",
      "2019-06-29T16:35:41.274395: step 2666, loss 0.080048, acc 0.984375\n",
      "2019-06-29T16:35:41.540585: step 2667, loss 0.00735308, acc 1\n",
      "2019-06-29T16:35:41.807403: step 2668, loss 0.0391746, acc 0.984375\n",
      "2019-06-29T16:35:42.101792: step 2669, loss 0.0112996, acc 1\n",
      "2019-06-29T16:35:42.369982: step 2670, loss 0.0343449, acc 0.984375\n",
      "2019-06-29T16:35:42.639174: step 2671, loss 0.0255908, acc 1\n",
      "2019-06-29T16:35:42.913369: step 2672, loss 0.00448004, acc 1\n",
      "2019-06-29T16:35:43.182562: step 2673, loss 0.00673136, acc 1\n",
      "2019-06-29T16:35:43.448751: step 2674, loss 0.0705792, acc 0.96875\n",
      "2019-06-29T16:35:43.717942: step 2675, loss 0.0065106, acc 1\n",
      "2019-06-29T16:35:43.982131: step 2676, loss 0.0149062, acc 1\n",
      "2019-06-29T16:35:44.250321: step 2677, loss 0.00977744, acc 1\n",
      "2019-06-29T16:35:44.504888: step 2678, loss 0.057408, acc 0.984375\n",
      "2019-06-29T16:35:44.770516: step 2679, loss 0.01411, acc 1\n",
      "2019-06-29T16:35:45.041571: step 2680, loss 0.0466712, acc 0.953125\n",
      "2019-06-29T16:35:45.322826: step 2681, loss 0.0138493, acc 1\n",
      "2019-06-29T16:35:45.588456: step 2682, loss 0.0201529, acc 0.984375\n",
      "2019-06-29T16:35:45.850242: step 2683, loss 0.0828693, acc 0.96875\n",
      "2019-06-29T16:35:46.109275: step 2684, loss 0.0525504, acc 0.984375\n",
      "2019-06-29T16:35:46.374904: step 2685, loss 0.0138758, acc 1\n",
      "2019-06-29T16:35:46.718660: step 2686, loss 0.0556616, acc 0.96875\n",
      "2019-06-29T16:35:47.006752: step 2687, loss 0.00764873, acc 1\n",
      "2019-06-29T16:35:47.359471: step 2688, loss 0.0240987, acc 0.984375\n",
      "2019-06-29T16:35:47.694942: step 2689, loss 0.00728598, acc 1\n",
      "2019-06-29T16:35:48.023176: step 2690, loss 0.0424099, acc 0.984375\n",
      "2019-06-29T16:35:48.339401: step 2691, loss 0.0358097, acc 1\n",
      "2019-06-29T16:35:48.690651: step 2692, loss 0.0292171, acc 1\n",
      "2019-06-29T16:35:49.002873: step 2693, loss 0.0385171, acc 0.984375\n",
      "2019-06-29T16:35:49.319098: step 2694, loss 0.0105595, acc 1\n",
      "2019-06-29T16:35:49.638799: step 2695, loss 0.0327528, acc 0.984375\n",
      "2019-06-29T16:35:49.965508: step 2696, loss 0.0207721, acc 1\n",
      "2019-06-29T16:35:50.281789: step 2697, loss 0.014887, acc 1\n",
      "2019-06-29T16:35:50.625544: step 2698, loss 0.0277903, acc 1\n",
      "2019-06-29T16:35:50.991499: step 2699, loss 0.00618691, acc 1\n",
      "2019-06-29T16:35:51.335255: step 2700, loss 0.00985013, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-06-29T16:35:52.293845: step 2700, loss 0.925786, acc 0.723265\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\mao\\Desktop\\Tensorflow\\CNN\\1\\cnn-text-classification-tf-master\\runs\\1561796526\\checkpoints\\model-2700\n",
      "\n",
      "2019-06-29T16:35:54.054467: step 2701, loss 0.0277003, acc 1\n",
      "2019-06-29T16:35:54.382597: step 2702, loss 0.00369319, acc 1\n",
      "2019-06-29T16:35:54.710727: step 2703, loss 0.0320709, acc 0.984375\n",
      "2019-06-29T16:35:55.023581: step 2704, loss 0.0118616, acc 1\n",
      "2019-06-29T16:35:55.367336: step 2705, loss 0.0251028, acc 0.984375\n",
      "2019-06-29T16:35:55.695466: step 2706, loss 0.0103877, acc 1\n",
      "2019-06-29T16:35:55.997329: step 2707, loss 0.0147451, acc 1\n",
      "2019-06-29T16:35:56.294541: step 2708, loss 0.0152817, acc 1\n",
      "2019-06-29T16:35:56.620773: step 2709, loss 0.0120167, acc 1\n",
      "2019-06-29T16:35:56.913982: step 2710, loss 0.0193232, acc 0.984375\n",
      "2019-06-29T16:35:57.202187: step 2711, loss 0.00942272, acc 1\n",
      "2019-06-29T16:35:57.498399: step 2712, loss 0.0117749, acc 1\n",
      "2019-06-29T16:35:57.787604: step 2713, loss 0.0276598, acc 1\n",
      "2019-06-29T16:35:58.068440: step 2714, loss 0.0276405, acc 0.984375\n",
      "2019-06-29T16:35:58.365319: step 2715, loss 0.00469601, acc 1\n",
      "2019-06-29T16:35:58.662198: step 2716, loss 0.0075827, acc 1\n",
      "2019-06-29T16:35:58.964632: step 2717, loss 0.0171292, acc 1\n",
      "2019-06-29T16:35:59.257299: step 2718, loss 0.0366951, acc 0.96875\n",
      "2019-06-29T16:35:59.616679: step 2719, loss 0.0133048, acc 1\n",
      "2019-06-29T16:35:59.956279: step 2720, loss 0.0215714, acc 0.984375\n",
      "2019-06-29T16:36:00.259526: step 2721, loss 0.0102994, acc 1\n",
      "2019-06-29T16:36:00.561741: step 2722, loss 0.025335, acc 1\n",
      "2019-06-29T16:36:00.860954: step 2723, loss 0.011259, acc 1\n",
      "2019-06-29T16:36:01.229217: step 2724, loss 0.0519969, acc 0.984375\n",
      "2019-06-29T16:36:01.543073: step 2725, loss 0.024619, acc 0.984375\n",
      "2019-06-29T16:36:01.834280: step 2726, loss 0.0272679, acc 1\n",
      "2019-06-29T16:36:02.130491: step 2727, loss 0.0330766, acc 0.96875\n",
      "2019-06-29T16:36:02.426018: step 2728, loss 0.0662433, acc 0.984375\n",
      "2019-06-29T16:36:02.754144: step 2729, loss 0.0501988, acc 0.96875\n",
      "2019-06-29T16:36:03.111090: step 2730, loss 0.00740451, acc 1\n",
      "2019-06-29T16:36:03.439220: step 2731, loss 0.0125566, acc 1\n",
      "2019-06-29T16:36:03.785468: step 2732, loss 0.0125944, acc 1\n",
      "2019-06-29T16:36:04.112414: step 2733, loss 0.0136087, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-29T16:36:04.444734: step 2734, loss 0.00955597, acc 1\n",
      "2019-06-29T16:36:04.723933: step 2735, loss 0.0031258, acc 1\n",
      "2019-06-29T16:36:05.004133: step 2736, loss 0.00543254, acc 1\n",
      "2019-06-29T16:36:05.301344: step 2737, loss 0.0287301, acc 0.984375\n",
      "2019-06-29T16:36:05.596555: step 2738, loss 0.0517885, acc 0.96875\n",
      "2019-06-29T16:36:05.885760: step 2739, loss 0.0119738, acc 1\n",
      "2019-06-29T16:36:06.158954: step 2740, loss 0.0078146, acc 1\n",
      "2019-06-29T16:36:06.462170: step 2741, loss 0.0120386, acc 1\n",
      "2019-06-29T16:36:06.765403: step 2742, loss 0.0162819, acc 1\n",
      "2019-06-29T16:36:07.070190: step 2743, loss 0.0138364, acc 1\n",
      "2019-06-29T16:36:07.398320: step 2744, loss 0.0497462, acc 0.96875\n",
      "2019-06-29T16:36:07.726449: step 2745, loss 0.0241281, acc 0.984375\n",
      "2019-06-29T16:36:08.045470: step 2746, loss 0.0193026, acc 0.984375\n",
      "2019-06-29T16:36:08.357974: step 2747, loss 0.0385484, acc 0.984375\n",
      "2019-06-29T16:36:08.717354: step 2748, loss 0.0151329, acc 1\n",
      "2019-06-29T16:36:09.036465: step 2749, loss 0.00609082, acc 1\n",
      "2019-06-29T16:36:09.364595: step 2750, loss 0.0156407, acc 1\n",
      "2019-06-29T16:36:09.723975: step 2751, loss 0.0173832, acc 0.984375\n",
      "2019-06-29T16:36:10.083356: step 2752, loss 0.00394169, acc 1\n",
      "2019-06-29T16:36:10.427110: step 2753, loss 0.0240531, acc 1\n",
      "2019-06-29T16:36:10.786491: step 2754, loss 0.0121201, acc 1\n",
      "2019-06-29T16:36:11.124419: step 2755, loss 0.0439131, acc 0.984375\n",
      "2019-06-29T16:36:11.436924: step 2756, loss 0.0352382, acc 0.984375\n",
      "2019-06-29T16:36:11.733805: step 2757, loss 0.00322715, acc 1\n",
      "2019-06-29T16:36:12.036240: step 2758, loss 0.0551556, acc 0.953125\n",
      "2019-06-29T16:36:12.317494: step 2759, loss 0.0112099, acc 1\n",
      "2019-06-29T16:36:12.614374: step 2760, loss 0.025026, acc 1\n",
      "2019-06-29T16:36:12.902569: step 2761, loss 0.00843121, acc 1\n",
      "2019-06-29T16:36:13.183824: step 2762, loss 0.0133668, acc 1\n",
      "2019-06-29T16:36:13.480703: step 2763, loss 0.0143935, acc 1\n",
      "2019-06-29T16:36:13.761956: step 2764, loss 0.0162827, acc 1\n",
      "2019-06-29T16:36:14.112777: step 2765, loss 0.035677, acc 0.984375\n",
      "2019-06-29T16:36:14.409656: step 2766, loss 0.0187708, acc 1\n",
      "2019-06-29T16:36:14.675285: step 2767, loss 0.0104045, acc 1\n",
      "2019-06-29T16:36:14.971131: step 2768, loss 0.0185988, acc 0.984375\n",
      "2019-06-29T16:36:15.268011: step 2769, loss 0.0073494, acc 1\n",
      "2019-06-29T16:36:15.564889: step 2770, loss 0.00645092, acc 1\n",
      "2019-06-29T16:36:15.861769: step 2771, loss 0.0369179, acc 0.96875\n",
      "2019-06-29T16:36:16.148469: step 2772, loss 0.0724128, acc 0.984375\n",
      "2019-06-29T16:36:16.429721: step 2773, loss 0.011018, acc 1\n",
      "2019-06-29T16:36:16.726601: step 2774, loss 0.0136157, acc 1\n",
      "2019-06-29T16:36:17.015363: step 2775, loss 0.00896466, acc 1\n",
      "2019-06-29T16:36:17.296617: step 2776, loss 0.0253957, acc 0.984375\n",
      "2019-06-29T16:36:17.593497: step 2777, loss 0.013616, acc 1\n",
      "2019-06-29T16:36:17.890549: step 2778, loss 0.0252089, acc 1\n",
      "2019-06-29T16:36:18.172136: step 2779, loss 0.0452022, acc 0.984375\n",
      "2019-06-29T16:36:18.500267: step 2780, loss 0.0376981, acc 0.984375\n",
      "2019-06-29T16:36:18.844022: step 2781, loss 0.0567095, acc 0.984375\n",
      "2019-06-29T16:36:19.169055: step 2782, loss 0.0499533, acc 0.984375\n",
      "2019-06-29T16:36:19.555263: step 2783, loss 0.016494, acc 1\n",
      "2019-06-29T16:36:19.867767: step 2784, loss 0.0752248, acc 0.984375\n",
      "2019-06-29T16:36:20.172667: step 2785, loss 0.0241653, acc 1\n",
      "2019-06-29T16:36:20.500797: step 2786, loss 0.0137674, acc 1\n",
      "2019-06-29T16:36:20.813300: step 2787, loss 0.0330125, acc 0.984375\n",
      "2019-06-29T16:36:21.126845: step 2788, loss 0.0101838, acc 1\n",
      "2019-06-29T16:36:21.439351: step 2789, loss 0.0276771, acc 1\n",
      "2019-06-29T16:36:21.814356: step 2790, loss 0.015913, acc 1\n",
      "2019-06-29T16:36:22.120203: step 2791, loss 0.0426933, acc 0.984375\n",
      "2019-06-29T16:36:22.432707: step 2792, loss 0.151815, acc 0.984375\n",
      "2019-06-29T16:36:22.745212: step 2793, loss 0.00416201, acc 1\n",
      "2019-06-29T16:36:23.048960: step 2794, loss 0.00556564, acc 1\n",
      "2019-06-29T16:36:23.408342: step 2795, loss 0.0345321, acc 0.984375\n",
      "2019-06-29T16:36:23.767722: step 2796, loss 0.00886589, acc 1\n",
      "2019-06-29T16:36:24.134690: step 2797, loss 0.052403, acc 0.984375\n",
      "2019-06-29T16:36:24.639938: step 2798, loss 0.00700512, acc 1\n",
      "2019-06-29T16:36:24.955568: step 2799, loss 0.0310405, acc 0.984375\n",
      "2019-06-29T16:36:25.268073: step 2800, loss 0.0178929, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2019-06-29T16:36:26.176768: step 2800, loss 0.951015, acc 0.723265\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\mao\\Desktop\\Tensorflow\\CNN\\1\\cnn-text-classification-tf-master\\runs\\1561796526\\checkpoints\\model-2800\n",
      "\n",
      "2019-06-29T16:36:28.320286: step 2801, loss 0.00563857, acc 1\n",
      "2019-06-29T16:36:28.664041: step 2802, loss 0.00349581, acc 1\n",
      "2019-06-29T16:36:28.950714: step 2803, loss 0.0390821, acc 0.984375\n",
      "2019-06-29T16:36:29.247594: step 2804, loss 0.0489499, acc 0.984375\n",
      "2019-06-29T16:36:29.528848: step 2805, loss 0.0184291, acc 1\n",
      "2019-06-29T16:36:29.825727: step 2806, loss 0.0177637, acc 0.984375\n",
      "2019-06-29T16:36:30.112546: step 2807, loss 0.0617529, acc 0.984375\n",
      "2019-06-29T16:36:30.409426: step 2808, loss 0.0339835, acc 0.984375\n",
      "2019-06-29T16:36:30.706305: step 2809, loss 0.00597511, acc 1\n",
      "2019-06-29T16:36:30.993188: step 2810, loss 0.00742213, acc 1\n",
      "2019-06-29T16:36:31.290066: step 2811, loss 0.0132667, acc 1\n",
      "2019-06-29T16:36:31.586946: step 2812, loss 0.0281675, acc 1\n",
      "2019-06-29T16:36:31.852575: step 2813, loss 0.0129612, acc 1\n",
      "2019-06-29T16:36:32.123525: step 2814, loss 0.0280374, acc 1\n",
      "2019-06-29T16:36:32.389154: step 2815, loss 0.0120643, acc 1\n",
      "2019-06-29T16:36:32.654782: step 2816, loss 0.0101455, acc 1\n",
      "2019-06-29T16:36:32.957217: step 2817, loss 0.022257, acc 1\n",
      "2019-06-29T16:36:33.238472: step 2818, loss 0.0866671, acc 0.96875\n",
      "2019-06-29T16:36:33.504100: step 2819, loss 0.0228101, acc 1\n",
      "2019-06-29T16:36:33.785355: step 2820, loss 0.0163828, acc 1\n",
      "2019-06-29T16:36:34.063384: step 2821, loss 0.045902, acc 0.96875\n",
      "2019-06-29T16:36:34.344639: step 2822, loss 0.043488, acc 0.984375\n",
      "2019-06-29T16:36:34.704019: step 2823, loss 0.0188191, acc 1\n",
      "2019-06-29T16:36:35.086060: step 2824, loss 0.0141639, acc 1\n",
      "2019-06-29T16:36:35.429816: step 2825, loss 0.0351038, acc 0.984375\n",
      "2019-06-29T16:36:35.757944: step 2826, loss 0.00460401, acc 1\n",
      "2019-06-29T16:36:36.079535: step 2827, loss 0.0691482, acc 0.984375\n",
      "2019-06-29T16:36:36.407665: step 2828, loss 0.0869798, acc 0.96875\n",
      "2019-06-29T16:36:36.719643: step 2829, loss 0.0632341, acc 0.96875\n",
      "2019-06-29T16:36:37.019008: step 2830, loss 0.0160387, acc 1\n",
      "2019-06-29T16:36:37.378389: step 2831, loss 0.0108091, acc 1\n",
      "2019-06-29T16:36:37.747599: step 2832, loss 0.0216097, acc 0.984375\n",
      "2019-06-29T16:36:38.032802: step 2833, loss 0.0259568, acc 0.984375\n",
      "2019-06-29T16:36:38.309999: step 2834, loss 0.057619, acc 0.984375\n",
      "2019-06-29T16:36:38.602837: step 2835, loss 0.016633, acc 1\n",
      "2019-06-29T16:36:38.900839: step 2836, loss 0.00593356, acc 1\n",
      "2019-06-29T16:36:39.177036: step 2837, loss 0.0458642, acc 0.96875\n",
      "2019-06-29T16:36:39.472519: step 2838, loss 0.0129437, acc 1\n",
      "2019-06-29T16:36:39.753280: step 2839, loss 0.0215453, acc 1\n",
      "2019-06-29T16:36:40.046488: step 2840, loss 0.0346268, acc 0.984375\n",
      "2019-06-29T16:36:40.325687: step 2841, loss 0.0169445, acc 1\n",
      "2019-06-29T16:36:40.614893: step 2842, loss 0.00488995, acc 1\n",
      "2019-06-29T16:36:40.897094: step 2843, loss 0.0069174, acc 1\n",
      "2019-06-29T16:36:41.174153: step 2844, loss 0.0140499, acc 1\n",
      "2019-06-29T16:36:41.486658: step 2845, loss 0.0202553, acc 1\n",
      "2019-06-29T16:36:41.814787: step 2846, loss 0.0392338, acc 0.984375\n",
      "2019-06-29T16:36:42.103082: step 2847, loss 0.0240518, acc 0.984375\n",
      "2019-06-29T16:36:42.399961: step 2848, loss 0.0444122, acc 0.984375\n",
      "2019-06-29T16:36:42.696841: step 2849, loss 0.0757714, acc 0.96875\n",
      "2019-06-29T16:36:43.000321: step 2850, loss 0.00902287, acc 1\n",
      "2019-06-29T16:36:43.281575: step 2851, loss 0.00967373, acc 1\n",
      "2019-06-29T16:36:43.547205: step 2852, loss 0.0421437, acc 0.984375\n",
      "2019-06-29T16:36:43.828459: step 2853, loss 0.0157601, acc 1\n",
      "2019-06-29T16:36:44.099908: step 2854, loss 0.0224684, acc 1\n",
      "2019-06-29T16:36:44.412411: step 2855, loss 0.0185914, acc 0.984375\n",
      "2019-06-29T16:36:44.678040: step 2856, loss 0.00973886, acc 1\n",
      "2019-06-29T16:36:44.965578: step 2857, loss 0.010207, acc 1\n",
      "2019-06-29T16:36:45.231206: step 2858, loss 0.0525646, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-29T16:36:45.496835: step 2859, loss 0.00388274, acc 1\n",
      "2019-06-29T16:36:45.762463: step 2860, loss 0.0198095, acc 1\n",
      "2019-06-29T16:36:46.033376: step 2861, loss 0.0397403, acc 0.984375\n",
      "2019-06-29T16:36:46.345881: step 2862, loss 0.0134106, acc 1\n",
      "2019-06-29T16:36:46.611509: step 2863, loss 0.00456973, acc 1\n",
      "2019-06-29T16:36:46.882384: step 2864, loss 0.0113565, acc 1\n",
      "2019-06-29T16:36:47.148014: step 2865, loss 0.0457083, acc 0.984375\n",
      "2019-06-29T16:36:47.413644: step 2866, loss 0.00797172, acc 1\n",
      "2019-06-29T16:36:47.679272: step 2867, loss 0.0292666, acc 0.984375\n",
      "2019-06-29T16:36:47.934454: step 2868, loss 0.00610188, acc 1\n",
      "2019-06-29T16:36:48.231334: step 2869, loss 0.0261544, acc 0.984375\n",
      "2019-06-29T16:36:48.496962: step 2870, loss 0.00527796, acc 1\n",
      "2019-06-29T16:36:48.762590: step 2871, loss 0.020647, acc 1\n",
      "2019-06-29T16:36:49.017939: step 2872, loss 0.0168324, acc 1\n",
      "2019-06-29T16:36:49.283568: step 2873, loss 0.00373178, acc 1\n",
      "2019-06-29T16:36:49.549197: step 2874, loss 0.00242855, acc 1\n",
      "2019-06-29T16:36:49.814827: step 2875, loss 0.0118001, acc 1\n",
      "2019-06-29T16:36:50.070091: step 2876, loss 0.0327911, acc 0.984375\n",
      "2019-06-29T16:36:50.340624: step 2877, loss 0.0480933, acc 0.984375\n",
      "2019-06-29T16:36:50.656637: step 2878, loss 0.0145948, acc 0.984375\n",
      "2019-06-29T16:36:50.943305: step 2879, loss 0.157211, acc 0.953125\n",
      "2019-06-29T16:36:51.240185: step 2880, loss 0.0162345, acc 1\n",
      "2019-06-29T16:36:51.528382: step 2881, loss 0.0182164, acc 0.984375\n",
      "2019-06-29T16:36:51.794011: step 2882, loss 0.00566889, acc 1\n",
      "2019-06-29T16:36:52.064975: step 2883, loss 0.00562287, acc 1\n",
      "2019-06-29T16:36:52.346230: step 2884, loss 0.0095351, acc 1\n",
      "2019-06-29T16:36:52.627485: step 2885, loss 0.0282032, acc 0.984375\n",
      "2019-06-29T16:36:52.898387: step 2886, loss 0.0142932, acc 1\n",
      "2019-06-29T16:36:53.195267: step 2887, loss 0.0243672, acc 0.984375\n",
      "2019-06-29T16:36:53.476521: step 2888, loss 0.0112247, acc 1\n",
      "2019-06-29T16:36:53.757775: step 2889, loss 0.00758669, acc 1\n",
      "2019-06-29T16:36:54.044574: step 2890, loss 0.0731195, acc 0.96875\n",
      "2019-06-29T16:36:54.310203: step 2891, loss 0.00572772, acc 1\n",
      "2019-06-29T16:36:54.591457: step 2892, loss 0.0164682, acc 1\n",
      "2019-06-29T16:36:54.899427: step 2893, loss 0.0790476, acc 0.96875\n",
      "2019-06-29T16:36:55.170660: step 2894, loss 0.00940939, acc 1\n",
      "2019-06-29T16:36:55.451912: step 2895, loss 0.0140027, acc 1\n",
      "2019-06-29T16:36:55.733167: step 2896, loss 0.0119013, acc 1\n",
      "2019-06-29T16:36:56.040322: step 2897, loss 0.0393366, acc 0.984375\n",
      "2019-06-29T16:36:56.321577: step 2898, loss 0.0466475, acc 0.96875\n",
      "2019-06-29T16:36:56.602831: step 2899, loss 0.00954335, acc 1\n",
      "2019-06-29T16:36:56.873730: step 2900, loss 0.0265437, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2019-06-29T16:36:57.624717: step 2900, loss 0.961127, acc 0.726079\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\mao\\Desktop\\Tensorflow\\CNN\\1\\cnn-text-classification-tf-master\\runs\\1561796526\\checkpoints\\model-2900\n",
      "\n",
      "2019-06-29T16:36:58.737756: step 2901, loss 0.0135106, acc 1\n",
      "2019-06-29T16:36:59.203943: step 2902, loss 0.0241258, acc 0.984375\n",
      "2019-06-29T16:36:59.453946: step 2903, loss 0.00617693, acc 1\n",
      "2019-06-29T16:36:59.719575: step 2904, loss 0.00857209, acc 1\n",
      "2019-06-29T16:36:59.990580: step 2905, loss 0.0177008, acc 1\n",
      "2019-06-29T16:37:00.251135: step 2906, loss 0.00880354, acc 1\n",
      "2019-06-29T16:37:00.516763: step 2907, loss 0.019393, acc 1\n",
      "2019-06-29T16:37:00.782392: step 2908, loss 0.0075083, acc 1\n",
      "2019-06-29T16:37:01.053560: step 2909, loss 0.00562853, acc 1\n",
      "2019-06-29T16:37:01.319189: step 2910, loss 0.0119077, acc 1\n",
      "2019-06-29T16:37:01.600443: step 2911, loss 0.0275262, acc 0.984375\n",
      "2019-06-29T16:37:01.871571: step 2912, loss 0.00769344, acc 1\n",
      "2019-06-29T16:37:02.121575: step 2913, loss 0.00882348, acc 1\n",
      "2019-06-29T16:37:02.387205: step 2914, loss 0.0557563, acc 0.96875\n",
      "2019-06-29T16:37:02.652833: step 2915, loss 0.0102882, acc 1\n",
      "2019-06-29T16:37:02.923790: step 2916, loss 0.0119846, acc 1\n",
      "2019-06-29T16:37:03.189419: step 2917, loss 0.0461922, acc 0.984375\n",
      "2019-06-29T16:37:03.455049: step 2918, loss 0.0148686, acc 1\n",
      "2019-06-29T16:37:03.736302: step 2919, loss 0.00710411, acc 1\n",
      "2019-06-29T16:37:04.043491: step 2920, loss 0.0110856, acc 1\n",
      "2019-06-29T16:37:04.309121: step 2921, loss 0.00487553, acc 1\n",
      "2019-06-29T16:37:04.590373: step 2922, loss 0.0115013, acc 1\n",
      "2019-06-29T16:37:04.856002: step 2923, loss 0.00851627, acc 1\n",
      "2019-06-29T16:37:05.126251: step 2924, loss 0.0202196, acc 1\n",
      "2019-06-29T16:37:05.376259: step 2925, loss 0.0104066, acc 1\n",
      "2019-06-29T16:37:05.657166: step 2926, loss 0.0230418, acc 1\n",
      "2019-06-29T16:37:05.928143: step 2927, loss 0.0123805, acc 1\n",
      "2019-06-29T16:37:06.193772: step 2928, loss 0.0226823, acc 1\n",
      "2019-06-29T16:37:06.459402: step 2929, loss 0.0529963, acc 0.984375\n",
      "2019-06-29T16:37:06.756281: step 2930, loss 0.0100958, acc 1\n",
      "2019-06-29T16:37:07.043064: step 2931, loss 0.0330183, acc 1\n",
      "2019-06-29T16:37:07.339942: step 2932, loss 0.00252745, acc 1\n",
      "2019-06-29T16:37:07.599397: step 2933, loss 0.0148665, acc 0.984375\n",
      "2019-06-29T16:37:07.902726: step 2934, loss 0.010222, acc 1\n",
      "2019-06-29T16:37:08.168354: step 2935, loss 0.0311141, acc 1\n",
      "2019-06-29T16:37:08.449609: step 2936, loss 0.0565479, acc 0.984375\n",
      "2019-06-29T16:37:08.730862: step 2937, loss 0.0599801, acc 0.984375\n",
      "2019-06-29T16:37:09.033258: step 2938, loss 0.0258253, acc 1\n",
      "2019-06-29T16:37:09.314513: step 2939, loss 0.0540253, acc 0.96875\n",
      "2019-06-29T16:37:09.595767: step 2940, loss 0.00660132, acc 1\n",
      "2019-06-29T16:37:09.882465: step 2941, loss 0.00940078, acc 1\n",
      "2019-06-29T16:37:10.179344: step 2942, loss 0.0110017, acc 1\n",
      "2019-06-29T16:37:10.460598: step 2943, loss 0.0713745, acc 0.984375\n",
      "2019-06-29T16:37:10.741852: step 2944, loss 0.00279772, acc 1\n",
      "2019-06-29T16:37:11.013046: step 2945, loss 0.0197091, acc 1\n",
      "2019-06-29T16:37:11.309928: step 2946, loss 0.0246906, acc 1\n",
      "2019-06-29T16:37:11.606805: step 2947, loss 0.00501782, acc 1\n",
      "2019-06-29T16:37:11.920334: step 2948, loss 0.0312418, acc 0.984375\n",
      "2019-06-29T16:37:12.223627: step 2949, loss 0.00635721, acc 1\n",
      "2019-06-29T16:37:12.520505: step 2950, loss 0.00880039, acc 1\n",
      "2019-06-29T16:37:12.817385: step 2951, loss 0.014461, acc 1\n",
      "2019-06-29T16:37:13.114557: step 2952, loss 0.0382414, acc 0.984375\n",
      "2019-06-29T16:37:13.458312: step 2953, loss 0.019573, acc 1\n",
      "2019-06-29T16:37:13.755192: step 2954, loss 0.0364545, acc 0.984375\n",
      "2019-06-29T16:37:14.057800: step 2955, loss 0.0380378, acc 0.96875\n",
      "2019-06-29T16:37:14.354682: step 2956, loss 0.0333647, acc 1\n",
      "2019-06-29T16:37:14.635933: step 2957, loss 0.00891166, acc 1\n",
      "2019-06-29T16:37:14.906980: step 2958, loss 0.0103734, acc 1\n",
      "2019-06-29T16:37:15.172608: step 2959, loss 0.00829913, acc 1\n",
      "2019-06-29T16:37:15.453863: step 2960, loss 0.00781359, acc 1\n",
      "2019-06-29T16:37:15.719491: step 2961, loss 0.0104984, acc 1\n",
      "2019-06-29T16:37:15.990497: step 2962, loss 0.0263737, acc 0.984375\n",
      "2019-06-29T16:37:16.256126: step 2963, loss 0.0146525, acc 1\n",
      "2019-06-29T16:37:16.521756: step 2964, loss 0.0100541, acc 1\n",
      "2019-06-29T16:37:16.803009: step 2965, loss 0.0134271, acc 1\n",
      "2019-06-29T16:37:17.058486: step 2966, loss 0.0262214, acc 0.984375\n",
      "2019-06-29T16:37:17.339741: step 2967, loss 0.0092158, acc 1\n",
      "2019-06-29T16:37:17.605370: step 2968, loss 0.0228938, acc 1\n",
      "2019-06-29T16:37:17.876442: step 2969, loss 0.00696875, acc 1\n",
      "2019-06-29T16:37:18.142070: step 2970, loss 0.00666433, acc 1\n",
      "2019-06-29T16:37:18.407701: step 2971, loss 0.017005, acc 1\n",
      "2019-06-29T16:37:18.673328: step 2972, loss 0.00610201, acc 1\n",
      "2019-06-29T16:37:18.944220: step 2973, loss 0.0709562, acc 0.984375\n",
      "2019-06-29T16:37:19.209849: step 2974, loss 0.0288925, acc 0.984375\n",
      "2019-06-29T16:37:19.475477: step 2975, loss 0.00404088, acc 1\n",
      "2019-06-29T16:37:19.741107: step 2976, loss 0.0431245, acc 0.984375\n",
      "2019-06-29T16:37:20.012572: step 2977, loss 0.0279949, acc 0.984375\n",
      "2019-06-29T16:37:20.278199: step 2978, loss 0.0138343, acc 1\n",
      "2019-06-29T16:37:20.547572: step 2979, loss 0.0120455, acc 1\n",
      "2019-06-29T16:37:20.813201: step 2980, loss 0.0450429, acc 0.984375\n",
      "2019-06-29T16:37:21.099981: step 2981, loss 0.00611344, acc 1\n",
      "2019-06-29T16:37:21.396859: step 2982, loss 0.0440006, acc 1\n",
      "2019-06-29T16:37:21.678114: step 2983, loss 0.0082464, acc 1\n",
      "2019-06-29T16:37:21.953292: step 2984, loss 0.0295603, acc 1\n",
      "2019-06-29T16:37:22.218920: step 2985, loss 0.00831871, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-29T16:37:22.484550: step 2986, loss 0.00667326, acc 1\n",
      "2019-06-29T16:37:22.781429: step 2987, loss 0.0403732, acc 0.984375\n",
      "2019-06-29T16:37:23.068102: step 2988, loss 0.0116332, acc 1\n",
      "2019-06-29T16:37:23.380607: step 2989, loss 0.00752793, acc 1\n",
      "2019-06-29T16:37:23.663171: step 2990, loss 0.0176727, acc 0.984375\n",
      "2019-06-29T16:37:23.934053: step 2991, loss 0.00519953, acc 1\n",
      "2019-06-29T16:37:24.215308: step 2992, loss 0.0131459, acc 1\n",
      "2019-06-29T16:37:24.512187: step 2993, loss 0.0168223, acc 1\n",
      "2019-06-29T16:37:24.793441: step 2994, loss 0.0315476, acc 0.984375\n",
      "2019-06-29T16:37:25.064722: step 2995, loss 0.0121895, acc 1\n",
      "2019-06-29T16:37:25.330351: step 2996, loss 0.0121475, acc 1\n",
      "2019-06-29T16:37:25.627230: step 2997, loss 0.00791959, acc 1\n",
      "2019-06-29T16:37:25.913860: step 2998, loss 0.00759091, acc 1\n",
      "2019-06-29T16:37:26.195114: step 2999, loss 0.0260674, acc 0.984375\n",
      "2019-06-29T16:37:26.476369: step 3000, loss 0.0134956, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-06-29T16:37:27.179520: step 3000, loss 0.991832, acc 0.730769\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\mao\\Desktop\\Tensorflow\\CNN\\1\\cnn-text-classification-tf-master\\runs\\1561796526\\checkpoints\\model-3000\n",
      "\n",
      "2019-06-29T16:37:28.740239: step 3001, loss 0.00362867, acc 1\n",
      "2019-06-29T16:37:29.026805: step 3002, loss 0.0058761, acc 1\n",
      "2019-06-29T16:37:29.323684: step 3003, loss 0.01458, acc 1\n",
      "2019-06-29T16:37:29.604938: step 3004, loss 0.00300871, acc 1\n",
      "2019-06-29T16:37:29.892382: step 3005, loss 0.0132013, acc 1\n",
      "2019-06-29T16:37:30.210626: step 3006, loss 0.00486081, acc 1\n",
      "2019-06-29T16:37:30.476249: step 3007, loss 0.00839304, acc 1\n",
      "2019-06-29T16:37:30.741878: step 3008, loss 0.0090157, acc 1\n",
      "2019-06-29T16:37:31.072219: step 3009, loss 0.0118069, acc 1\n",
      "2019-06-29T16:37:31.337847: step 3010, loss 0.0103454, acc 1\n",
      "2019-06-29T16:37:31.603476: step 3011, loss 0.00882655, acc 1\n",
      "2019-06-29T16:37:31.858888: step 3012, loss 0.00652624, acc 1\n",
      "2019-06-29T16:37:32.140141: step 3013, loss 0.0113114, acc 1\n",
      "2019-06-29T16:37:32.405771: step 3014, loss 0.00319416, acc 1\n",
      "2019-06-29T16:37:32.687023: step 3015, loss 0.0249716, acc 1\n",
      "2019-06-29T16:37:32.958798: step 3016, loss 0.0197889, acc 1\n",
      "2019-06-29T16:37:33.224427: step 3017, loss 0.0235635, acc 0.984375\n",
      "2019-06-29T16:37:33.490055: step 3018, loss 0.00474409, acc 1\n",
      "2019-06-29T16:37:33.755684: step 3019, loss 0.014725, acc 1\n",
      "2019-06-29T16:37:34.011015: step 3020, loss 0.0083337, acc 1\n",
      "2019-06-29T16:37:34.292270: step 3021, loss 0.00518944, acc 1\n",
      "2019-06-29T16:37:34.557899: step 3022, loss 0.0183877, acc 0.984375\n",
      "2019-06-29T16:37:34.823527: step 3023, loss 0.0263588, acc 0.984375\n",
      "2019-06-29T16:37:35.095462: step 3024, loss 0.00654764, acc 1\n",
      "2019-06-29T16:37:35.392341: step 3025, loss 0.00376492, acc 1\n",
      "2019-06-29T16:37:35.673595: step 3026, loss 0.0192535, acc 0.984375\n",
      "2019-06-29T16:37:35.944518: step 3027, loss 0.0159107, acc 1\n",
      "2019-06-29T16:37:36.210148: step 3028, loss 0.020885, acc 1\n",
      "2019-06-29T16:37:36.538051: step 3029, loss 0.0233049, acc 1\n",
      "2019-06-29T16:37:36.819305: step 3030, loss 0.00478957, acc 1\n",
      "2019-06-29T16:37:37.074648: step 3031, loss 0.00611074, acc 1\n",
      "2019-06-29T16:37:37.355903: step 3032, loss 0.0161165, acc 1\n",
      "2019-06-29T16:37:37.643693: step 3033, loss 0.00654847, acc 1\n",
      "2019-06-29T16:37:37.930372: step 3034, loss 0.0184146, acc 0.984375\n",
      "2019-06-29T16:37:38.180375: step 3035, loss 0.0114843, acc 1\n",
      "2019-06-29T16:37:38.446006: step 3036, loss 0.00944274, acc 1\n",
      "2019-06-29T16:37:38.789760: step 3037, loss 0.0282699, acc 1\n",
      "2019-06-29T16:37:39.094257: step 3038, loss 0.0159702, acc 1\n",
      "2019-06-29T16:37:39.391137: step 3039, loss 0.00341234, acc 1\n",
      "2019-06-29T16:37:39.708346: step 3040, loss 0.018985, acc 0.984375\n",
      "2019-06-29T16:37:40.010556: step 3041, loss 0.0123792, acc 1\n",
      "2019-06-29T16:37:40.276186: step 3042, loss 0.0596639, acc 0.96875\n",
      "2019-06-29T16:37:40.557440: step 3043, loss 0.00616764, acc 1\n",
      "2019-06-29T16:37:40.838694: step 3044, loss 0.00435464, acc 1\n",
      "2019-06-29T16:37:41.125468: step 3045, loss 0.0299115, acc 0.984375\n",
      "2019-06-29T16:37:41.406723: step 3046, loss 0.00529955, acc 1\n",
      "2019-06-29T16:37:41.687977: step 3047, loss 0.0428732, acc 0.984375\n",
      "2019-06-29T16:37:41.993778: step 3048, loss 0.00593036, acc 1\n",
      "2019-06-29T16:37:42.259406: step 3049, loss 0.019908, acc 1\n",
      "2019-06-29T16:37:42.540661: step 3050, loss 0.117239, acc 0.96875\n",
      "2019-06-29T16:37:42.821915: step 3051, loss 0.00905396, acc 1\n",
      "2019-06-29T16:37:43.124324: step 3052, loss 0.00573724, acc 1\n",
      "2019-06-29T16:37:43.405577: step 3053, loss 0.0226898, acc 0.984375\n",
      "2019-06-29T16:37:43.686832: step 3054, loss 0.00998283, acc 1\n",
      "2019-06-29T16:37:43.973464: step 3055, loss 0.00994199, acc 1\n",
      "2019-06-29T16:37:44.289390: step 3056, loss 0.0138555, acc 1\n",
      "2019-06-29T16:37:44.570644: step 3057, loss 0.0210179, acc 1\n",
      "2019-06-29T16:37:45.023776: step 3058, loss 0.0038881, acc 1\n",
      "2019-06-29T16:37:45.391912: step 3059, loss 0.0259463, acc 0.984375\n",
      "2019-06-29T16:37:45.845042: step 3060, loss 0.0122673, acc 1\n",
      "2019-06-29T16:37:46.503063: step 3061, loss 0.00329618, acc 1\n",
      "2019-06-29T16:37:46.815567: step 3062, loss 0.00920813, acc 1\n",
      "2019-06-29T16:37:47.117888: step 3063, loss 0.00696833, acc 1\n",
      "2019-06-29T16:37:47.399143: step 3064, loss 0.0129357, acc 1\n",
      "2019-06-29T16:37:47.727273: step 3065, loss 0.010677, acc 1\n",
      "2019-06-29T16:37:48.030140: step 3066, loss 0.00238071, acc 1\n",
      "2019-06-29T16:37:48.327019: step 3067, loss 0.00898742, acc 1\n",
      "2019-06-29T16:37:48.655149: step 3068, loss 0.0148913, acc 1\n",
      "2019-06-29T16:37:48.926201: step 3069, loss 0.0213005, acc 1\n",
      "2019-06-29T16:37:49.191829: step 3070, loss 0.0175551, acc 0.984375\n",
      "2019-06-29T16:37:49.473083: step 3071, loss 0.00491221, acc 1\n",
      "2019-06-29T16:37:49.769964: step 3072, loss 0.00412147, acc 1\n",
      "2019-06-29T16:37:50.038941: step 3073, loss 0.0235285, acc 0.984375\n",
      "2019-06-29T16:37:50.304569: step 3074, loss 0.0122011, acc 0.984375\n",
      "2019-06-29T16:37:50.578032: step 3075, loss 0.0957754, acc 0.984375\n",
      "2019-06-29T16:37:50.859285: step 3076, loss 0.00866192, acc 1\n",
      "2019-06-29T16:37:51.114647: step 3077, loss 0.00766689, acc 1\n",
      "2019-06-29T16:37:51.395902: step 3078, loss 0.00753548, acc 1\n",
      "2019-06-29T16:37:51.661530: step 3079, loss 0.0230029, acc 0.984375\n",
      "2019-06-29T16:37:51.948344: step 3080, loss 0.016165, acc 1\n",
      "2019-06-29T16:37:52.213974: step 3081, loss 0.00523565, acc 1\n",
      "2019-06-29T16:37:52.479602: step 3082, loss 0.00739606, acc 1\n",
      "2019-06-29T16:37:52.745232: step 3083, loss 0.0104281, acc 1\n",
      "2019-06-29T16:37:53.016281: step 3084, loss 0.00443103, acc 1\n",
      "2019-06-29T16:37:53.297536: step 3085, loss 0.082071, acc 0.984375\n",
      "2019-06-29T16:37:53.563165: step 3086, loss 0.0392627, acc 0.984375\n",
      "2019-06-29T16:37:53.848053: step 3087, loss 0.0138091, acc 1\n",
      "2019-06-29T16:37:54.119184: step 3088, loss 0.0023881, acc 1\n",
      "2019-06-29T16:37:54.587942: step 3089, loss 0.0359956, acc 0.984375\n",
      "2019-06-29T16:37:54.884821: step 3090, loss 0.00750139, acc 1\n",
      "2019-06-29T16:37:55.188986: step 3091, loss 0.00841403, acc 1\n",
      "2019-06-29T16:37:55.497378: step 3092, loss 0.00130616, acc 1\n",
      "2019-06-29T16:37:55.791811: step 3093, loss 0.0188702, acc 0.984375\n",
      "2019-06-29T16:37:56.078612: step 3094, loss 0.00498293, acc 1\n",
      "2019-06-29T16:37:56.406743: step 3095, loss 0.00411513, acc 1\n",
      "2019-06-29T16:37:56.719247: step 3096, loss 0.00589111, acc 1\n",
      "2019-06-29T16:37:56.990564: step 3097, loss 0.0099058, acc 1\n",
      "2019-06-29T16:37:57.271819: step 3098, loss 0.00600498, acc 1\n",
      "2019-06-29T16:37:57.553073: step 3099, loss 0.00622836, acc 1\n",
      "2019-06-29T16:37:57.834326: step 3100, loss 0.0204757, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2019-06-29T16:37:58.558169: step 3100, loss 1.02699, acc 0.728893\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\mao\\Desktop\\Tensorflow\\CNN\\1\\cnn-text-classification-tf-master\\runs\\1561796526\\checkpoints\\model-3100\n",
      "\n",
      "2019-06-29T16:38:00.096800: step 3101, loss 0.0144858, acc 1\n",
      "2019-06-29T16:38:00.378055: step 3102, loss 0.0359416, acc 0.984375\n",
      "2019-06-29T16:38:00.706184: step 3103, loss 0.0376855, acc 0.984375\n",
      "2019-06-29T16:38:00.977271: step 3104, loss 0.0130319, acc 1\n",
      "2019-06-29T16:38:01.254090: step 3105, loss 0.00969265, acc 1\n",
      "2019-06-29T16:38:01.535344: step 3106, loss 0.0109543, acc 1\n",
      "2019-06-29T16:38:01.832222: step 3107, loss 0.0065428, acc 1\n",
      "2019-06-29T16:38:02.103383: step 3108, loss 0.0116487, acc 1\n",
      "2019-06-29T16:38:02.369011: step 3109, loss 0.00335384, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-29T16:38:02.650265: step 3110, loss 0.0277592, acc 0.984375\n",
      "2019-06-29T16:38:02.921700: step 3111, loss 0.0156483, acc 1\n",
      "2019-06-29T16:38:03.171704: step 3112, loss 0.00241799, acc 1\n",
      "2019-06-29T16:38:03.437333: step 3113, loss 0.0180985, acc 0.984375\n",
      "2019-06-29T16:38:03.702961: step 3114, loss 0.0130967, acc 1\n",
      "2019-06-29T16:38:03.989515: step 3115, loss 0.0200844, acc 0.984375\n",
      "2019-06-29T16:38:04.239518: step 3116, loss 0.0367304, acc 0.984375\n",
      "2019-06-29T16:38:04.505148: step 3117, loss 0.0260171, acc 1\n",
      "2019-06-29T16:38:04.755150: step 3118, loss 0.017181, acc 1\n",
      "2019-06-29T16:38:05.026232: step 3119, loss 0.00687919, acc 1\n",
      "2019-06-29T16:38:05.291861: step 3120, loss 0.0568607, acc 0.984375\n",
      "2019-06-29T16:38:05.557490: step 3121, loss 0.0390853, acc 0.96875\n",
      "2019-06-29T16:38:05.823120: step 3122, loss 0.0232604, acc 0.984375\n",
      "2019-06-29T16:38:06.094340: step 3123, loss 0.0118395, acc 1\n",
      "2019-06-29T16:38:06.359969: step 3124, loss 0.00304953, acc 1\n",
      "2019-06-29T16:38:06.625598: step 3125, loss 0.007854, acc 1\n",
      "2019-06-29T16:38:06.880867: step 3126, loss 0.00697574, acc 1\n",
      "2019-06-29T16:38:07.146496: step 3127, loss 0.0133058, acc 1\n",
      "2019-06-29T16:38:07.412125: step 3128, loss 0.0362965, acc 0.984375\n",
      "2019-06-29T16:38:07.681464: step 3129, loss 0.0338965, acc 0.96875\n",
      "2019-06-29T16:38:07.952313: step 3130, loss 0.0304923, acc 0.984375\n",
      "2019-06-29T16:38:08.217942: step 3131, loss 0.0033795, acc 1\n",
      "2019-06-29T16:38:08.483570: step 3132, loss 0.0205626, acc 0.984375\n",
      "2019-06-29T16:38:08.749199: step 3133, loss 0.00569051, acc 1\n",
      "2019-06-29T16:38:09.004457: step 3134, loss 0.0069519, acc 1\n",
      "2019-06-29T16:38:09.285711: step 3135, loss 0.0208235, acc 1\n",
      "2019-06-29T16:38:09.555062: step 3136, loss 0.013522, acc 1\n",
      "2019-06-29T16:38:09.820692: step 3137, loss 0.0116948, acc 1\n",
      "2019-06-29T16:38:10.075995: step 3138, loss 0.0171074, acc 1\n",
      "2019-06-29T16:38:10.341625: step 3139, loss 0.0155374, acc 1\n",
      "2019-06-29T16:38:10.638505: step 3140, loss 0.0208068, acc 0.984375\n",
      "2019-06-29T16:38:10.925477: step 3141, loss 0.00798139, acc 1\n",
      "2019-06-29T16:38:11.222357: step 3142, loss 0.0202386, acc 0.984375\n",
      "2019-06-29T16:38:11.532352: step 3143, loss 0.00505365, acc 1\n",
      "2019-06-29T16:38:11.785283: step 3144, loss 0.00243609, acc 1\n",
      "2019-06-29T16:38:12.056322: step 3145, loss 0.011556, acc 1\n",
      "2019-06-29T16:38:12.337575: step 3146, loss 0.00488377, acc 1\n",
      "2019-06-29T16:38:12.650080: step 3147, loss 0.00970993, acc 1\n",
      "2019-06-29T16:38:12.921060: step 3148, loss 0.00738317, acc 1\n",
      "2019-06-29T16:38:13.202315: step 3149, loss 0.00714446, acc 1\n",
      "2019-06-29T16:38:13.467943: step 3150, loss 0.0230995, acc 0.983333\n",
      "2019-06-29T16:38:13.764822: step 3151, loss 0.0406746, acc 0.984375\n",
      "2019-06-29T16:38:14.064321: step 3152, loss 0.0306872, acc 0.984375\n",
      "2019-06-29T16:38:14.345575: step 3153, loss 0.0111104, acc 1\n",
      "2019-06-29T16:38:14.611204: step 3154, loss 0.00839298, acc 1\n",
      "2019-06-29T16:38:14.907052: step 3155, loss 0.015572, acc 1\n",
      "2019-06-29T16:38:15.203936: step 3156, loss 0.0128582, acc 1\n",
      "2019-06-29T16:38:15.500811: step 3157, loss 0.00437969, acc 1\n",
      "2019-06-29T16:38:15.766438: step 3158, loss 0.029231, acc 0.984375\n",
      "2019-06-29T16:38:16.070268: step 3159, loss 0.0172346, acc 0.984375\n",
      "2019-06-29T16:38:16.367148: step 3160, loss 0.00773448, acc 1\n",
      "2019-06-29T16:38:16.648402: step 3161, loss 0.041806, acc 0.984375\n",
      "2019-06-29T16:38:16.949931: step 3162, loss 0.0266523, acc 0.984375\n",
      "2019-06-29T16:38:17.231184: step 3163, loss 0.0167777, acc 1\n",
      "2019-06-29T16:38:17.528063: step 3164, loss 0.00801821, acc 1\n",
      "2019-06-29T16:38:17.838602: step 3165, loss 0.0327657, acc 0.984375\n",
      "2019-06-29T16:38:18.126703: step 3166, loss 0.0113741, acc 1\n",
      "2019-06-29T16:38:18.423583: step 3167, loss 0.0146556, acc 0.984375\n",
      "2019-06-29T16:38:18.689211: step 3168, loss 0.0691614, acc 0.984375\n",
      "2019-06-29T16:38:18.960176: step 3169, loss 0.0028207, acc 1\n",
      "2019-06-29T16:38:19.272682: step 3170, loss 0.0192663, acc 1\n",
      "2019-06-29T16:38:19.569562: step 3171, loss 0.00309705, acc 1\n",
      "2019-06-29T16:38:19.850815: step 3172, loss 0.00519084, acc 1\n",
      "2019-06-29T16:38:20.290173: step 3173, loss 0.0096872, acc 1\n",
      "2019-06-29T16:38:20.555802: step 3174, loss 0.00854502, acc 1\n",
      "2019-06-29T16:38:20.828021: step 3175, loss 0.00376162, acc 1\n",
      "2019-06-29T16:38:21.099066: step 3176, loss 0.0107034, acc 1\n",
      "2019-06-29T16:38:21.364695: step 3177, loss 0.0239443, acc 0.984375\n",
      "2019-06-29T16:38:21.645948: step 3178, loss 0.0148264, acc 0.984375\n",
      "2019-06-29T16:38:21.917158: step 3179, loss 0.0138044, acc 1\n",
      "2019-06-29T16:38:22.182787: step 3180, loss 0.0124197, acc 1\n",
      "2019-06-29T16:38:22.448416: step 3181, loss 0.00712585, acc 1\n",
      "2019-06-29T16:38:22.714044: step 3182, loss 0.00792994, acc 1\n",
      "2019-06-29T16:38:22.984970: step 3183, loss 0.0910728, acc 0.96875\n",
      "2019-06-29T16:38:23.250599: step 3184, loss 0.0215761, acc 0.984375\n",
      "2019-06-29T16:38:23.516228: step 3185, loss 0.0138461, acc 0.984375\n",
      "2019-06-29T16:38:23.781856: step 3186, loss 0.0201837, acc 1\n",
      "2019-06-29T16:38:24.052884: step 3187, loss 0.0279984, acc 0.984375\n",
      "2019-06-29T16:38:24.318513: step 3188, loss 0.00681025, acc 1\n",
      "2019-06-29T16:38:24.568519: step 3189, loss 0.00348107, acc 1\n",
      "2019-06-29T16:38:24.834146: step 3190, loss 0.00210806, acc 1\n",
      "2019-06-29T16:38:25.105165: step 3191, loss 0.00994835, acc 1\n",
      "2019-06-29T16:38:25.370794: step 3192, loss 0.0030537, acc 1\n",
      "2019-06-29T16:38:25.667674: step 3193, loss 0.0207109, acc 1\n",
      "2019-06-29T16:38:25.942452: step 3194, loss 0.00446112, acc 1\n",
      "2019-06-29T16:38:26.239333: step 3195, loss 0.00570344, acc 1\n",
      "2019-06-29T16:38:26.520586: step 3196, loss 0.0129215, acc 1\n",
      "2019-06-29T16:38:26.848717: step 3197, loss 0.00771733, acc 1\n",
      "2019-06-29T16:38:27.135733: step 3198, loss 0.0105565, acc 1\n",
      "2019-06-29T16:38:27.435841: step 3199, loss 0.0144055, acc 1\n",
      "2019-06-29T16:38:27.696398: step 3200, loss 0.0289226, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2019-06-29T16:38:28.435421: step 3200, loss 1.02978, acc 0.733584\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\mao\\Desktop\\Tensorflow\\CNN\\1\\cnn-text-classification-tf-master\\runs\\1561796526\\checkpoints\\model-3200\n",
      "\n",
      "2019-06-29T16:38:29.907878: step 3201, loss 0.00252309, acc 1\n",
      "2019-06-29T16:38:30.210171: step 3202, loss 0.0655478, acc 0.96875\n",
      "2019-06-29T16:38:30.507052: step 3203, loss 0.00667, acc 1\n",
      "2019-06-29T16:38:30.788305: step 3204, loss 0.0127663, acc 1\n",
      "2019-06-29T16:38:31.075021: step 3205, loss 0.00631067, acc 1\n",
      "2019-06-29T16:38:31.356276: step 3206, loss 0.0368666, acc 0.984375\n",
      "2019-06-29T16:38:31.668780: step 3207, loss 0.00508087, acc 1\n",
      "2019-06-29T16:38:31.955409: step 3208, loss 0.0082581, acc 1\n",
      "2019-06-29T16:38:32.236664: step 3209, loss 0.028996, acc 1\n",
      "2019-06-29T16:38:32.502294: step 3210, loss 0.00356915, acc 1\n",
      "2019-06-29T16:38:32.799171: step 3211, loss 0.00428345, acc 1\n",
      "2019-06-29T16:38:33.070608: step 3212, loss 0.00382772, acc 1\n",
      "2019-06-29T16:38:33.367488: step 3213, loss 0.0110375, acc 1\n",
      "2019-06-29T16:38:33.679993: step 3214, loss 0.00693942, acc 1\n",
      "2019-06-29T16:38:33.951097: step 3215, loss 0.0112907, acc 1\n",
      "2019-06-29T16:38:34.232351: step 3216, loss 0.0105386, acc 1\n",
      "2019-06-29T16:38:34.497981: step 3217, loss 0.00239757, acc 1\n",
      "2019-06-29T16:38:34.763609: step 3218, loss 0.0141662, acc 1\n",
      "2019-06-29T16:38:35.018832: step 3219, loss 0.0178761, acc 0.984375\n",
      "2019-06-29T16:38:35.342311: step 3220, loss 0.00985747, acc 1\n",
      "2019-06-29T16:38:35.675548: step 3221, loss 0.0118514, acc 1\n",
      "2019-06-29T16:38:35.980766: step 3222, loss 0.041304, acc 0.984375\n",
      "2019-06-29T16:38:36.259966: step 3223, loss 0.0107004, acc 1\n",
      "2019-06-29T16:38:36.537321: step 3224, loss 0.00569482, acc 1\n",
      "2019-06-29T16:38:36.841537: step 3225, loss 0.00209355, acc 1\n",
      "2019-06-29T16:38:37.122229: step 3226, loss 0.0127564, acc 1\n",
      "2019-06-29T16:38:37.397948: step 3227, loss 0.0120935, acc 1\n",
      "2019-06-29T16:38:37.667736: step 3228, loss 0.0201269, acc 1\n",
      "2019-06-29T16:38:37.950135: step 3229, loss 0.0252411, acc 0.984375\n",
      "2019-06-29T16:38:38.310572: step 3230, loss 0.00772055, acc 1\n",
      "2019-06-29T16:38:38.590730: step 3231, loss 0.015346, acc 1\n",
      "2019-06-29T16:38:38.870123: step 3232, loss 0.0169842, acc 0.984375\n",
      "2019-06-29T16:38:39.147398: step 3233, loss 0.00736773, acc 1\n",
      "2019-06-29T16:38:39.454617: step 3234, loss 0.0221515, acc 0.984375\n",
      "2019-06-29T16:38:39.762836: step 3235, loss 0.00290672, acc 1\n",
      "2019-06-29T16:38:40.044424: step 3236, loss 0.0149297, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-29T16:38:40.330699: step 3237, loss 0.035399, acc 0.96875\n",
      "2019-06-29T16:38:40.679950: step 3238, loss 0.0403881, acc 0.984375\n",
      "2019-06-29T16:38:41.100247: step 3239, loss 0.0121091, acc 1\n",
      "2019-06-29T16:38:41.404807: step 3240, loss 0.0051448, acc 1\n",
      "2019-06-29T16:38:41.682599: step 3241, loss 0.0119253, acc 1\n",
      "2019-06-29T16:38:41.956645: step 3242, loss 0.0488609, acc 0.984375\n",
      "2019-06-29T16:38:42.229082: step 3243, loss 0.00595599, acc 1\n",
      "2019-06-29T16:38:42.539848: step 3244, loss 0.0646477, acc 0.984375\n",
      "2019-06-29T16:38:42.838060: step 3245, loss 0.00494027, acc 1\n",
      "2019-06-29T16:38:43.141276: step 3246, loss 0.03433, acc 0.984375\n",
      "2019-06-29T16:38:43.427503: step 3247, loss 0.00606206, acc 1\n",
      "2019-06-29T16:38:43.730988: step 3248, loss 0.0287882, acc 0.984375\n",
      "2019-06-29T16:38:44.045801: step 3249, loss 0.00299319, acc 1\n",
      "2019-06-29T16:38:44.346888: step 3250, loss 0.00330285, acc 1\n",
      "2019-06-29T16:38:44.685544: step 3251, loss 0.0599415, acc 0.984375\n",
      "2019-06-29T16:38:44.987330: step 3252, loss 0.0146929, acc 1\n",
      "2019-06-29T16:38:45.276535: step 3253, loss 0.00704522, acc 1\n",
      "2019-06-29T16:38:45.568743: step 3254, loss 0.0184711, acc 0.984375\n",
      "2019-06-29T16:38:45.852536: step 3255, loss 0.00604929, acc 1\n",
      "2019-06-29T16:38:46.117401: step 3256, loss 0.00472839, acc 1\n",
      "2019-06-29T16:38:46.428013: step 3257, loss 0.00211673, acc 1\n",
      "2019-06-29T16:38:46.744124: step 3258, loss 0.0468685, acc 0.984375\n",
      "2019-06-29T16:38:47.088369: step 3259, loss 0.00816053, acc 1\n",
      "2019-06-29T16:38:47.392588: step 3260, loss 0.00211867, acc 1\n",
      "2019-06-29T16:38:47.824894: step 3261, loss 0.0415293, acc 0.984375\n",
      "2019-06-29T16:38:48.145122: step 3262, loss 0.0491981, acc 0.984375\n",
      "2019-06-29T16:38:48.430325: step 3263, loss 0.00430235, acc 1\n",
      "2019-06-29T16:38:48.710525: step 3264, loss 0.0226232, acc 0.984375\n",
      "2019-06-29T16:38:48.998730: step 3265, loss 0.0113033, acc 1\n",
      "2019-06-29T16:38:49.326963: step 3266, loss 0.0034739, acc 1\n",
      "2019-06-29T16:38:49.631181: step 3267, loss 0.00530797, acc 1\n",
      "2019-06-29T16:38:49.918384: step 3268, loss 0.00268314, acc 1\n",
      "2019-06-29T16:38:50.202587: step 3269, loss 0.0088385, acc 1\n",
      "2019-06-29T16:38:50.494796: step 3270, loss 0.00711016, acc 1\n",
      "2019-06-29T16:38:50.774994: step 3271, loss 0.0462259, acc 0.984375\n",
      "2019-06-29T16:38:51.054193: step 3272, loss 0.0068904, acc 1\n",
      "2019-06-29T16:38:51.327387: step 3273, loss 0.00785724, acc 1\n",
      "2019-06-29T16:38:51.594579: step 3274, loss 0.016901, acc 1\n",
      "2019-06-29T16:38:51.864770: step 3275, loss 0.00764562, acc 1\n",
      "2019-06-29T16:38:52.137966: step 3276, loss 0.0216719, acc 0.984375\n",
      "2019-06-29T16:38:52.411159: step 3277, loss 0.00380239, acc 1\n",
      "2019-06-29T16:38:52.686355: step 3278, loss 0.00596573, acc 1\n",
      "2019-06-29T16:38:52.955547: step 3279, loss 0.0228907, acc 1\n",
      "2019-06-29T16:38:53.222738: step 3280, loss 0.0125099, acc 1\n",
      "2019-06-29T16:38:53.497934: step 3281, loss 0.00577489, acc 1\n",
      "2019-06-29T16:38:53.762121: step 3282, loss 0.0231878, acc 0.984375\n",
      "2019-06-29T16:38:54.033315: step 3283, loss 0.00324276, acc 1\n",
      "2019-06-29T16:38:54.301505: step 3284, loss 0.047932, acc 0.96875\n",
      "2019-06-29T16:38:54.637744: step 3285, loss 0.0170947, acc 0.984375\n",
      "2019-06-29T16:38:54.906937: step 3286, loss 0.00247848, acc 1\n",
      "2019-06-29T16:38:55.174127: step 3287, loss 0.0200092, acc 0.984375\n",
      "2019-06-29T16:38:55.444319: step 3288, loss 0.0155401, acc 1\n",
      "2019-06-29T16:38:55.855613: step 3289, loss 0.0996315, acc 0.984375\n",
      "2019-06-29T16:38:56.147820: step 3290, loss 0.0486388, acc 0.96875\n",
      "2019-06-29T16:38:56.493303: step 3291, loss 0.0129306, acc 1\n",
      "2019-06-29T16:38:56.801522: step 3292, loss 0.0107855, acc 1\n",
      "2019-06-29T16:38:57.120748: step 3293, loss 0.0539027, acc 0.984375\n",
      "2019-06-29T16:38:57.423966: step 3294, loss 0.00733873, acc 1\n",
      "2019-06-29T16:38:57.728181: step 3295, loss 0.0103608, acc 1\n",
      "2019-06-29T16:38:58.055467: step 3296, loss 0.00228133, acc 1\n",
      "2019-06-29T16:38:58.321656: step 3297, loss 0.0551972, acc 0.984375\n",
      "2019-06-29T16:38:58.655895: step 3298, loss 0.025841, acc 0.984375\n",
      "2019-06-29T16:38:58.951105: step 3299, loss 0.0315713, acc 1\n",
      "2019-06-29T16:38:59.247315: step 3300, loss 0.0114123, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-06-29T16:38:59.995413: step 3300, loss 1.04956, acc 0.724203\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\mao\\Desktop\\Tensorflow\\CNN\\1\\cnn-text-classification-tf-master\\runs\\1561796526\\checkpoints\\model-3300\n",
      "\n",
      "2019-06-29T16:39:01.627103: step 3301, loss 0.00632655, acc 1\n",
      "2019-06-29T16:39:01.914307: step 3302, loss 0.00621508, acc 1\n",
      "2019-06-29T16:39:02.197509: step 3303, loss 0.00172833, acc 1\n",
      "2019-06-29T16:39:02.478710: step 3304, loss 0.00521709, acc 1\n",
      "2019-06-29T16:39:02.770919: step 3305, loss 0.00433594, acc 1\n",
      "2019-06-29T16:39:03.064126: step 3306, loss 0.00763898, acc 1\n",
      "2019-06-29T16:39:03.349329: step 3307, loss 0.0284649, acc 0.984375\n",
      "2019-06-29T16:39:03.649542: step 3308, loss 0.0548, acc 0.984375\n",
      "2019-06-29T16:39:03.932744: step 3309, loss 0.00591414, acc 1\n",
      "2019-06-29T16:39:04.242965: step 3310, loss 0.00793934, acc 1\n",
      "2019-06-29T16:39:04.528168: step 3311, loss 0.027317, acc 0.984375\n",
      "2019-06-29T16:39:04.842392: step 3312, loss 0.00555439, acc 1\n",
      "2019-06-29T16:39:05.123593: step 3313, loss 0.0341245, acc 0.984375\n",
      "2019-06-29T16:39:05.414800: step 3314, loss 0.0162349, acc 1\n",
      "2019-06-29T16:39:05.706007: step 3315, loss 0.00311377, acc 1\n",
      "2019-06-29T16:39:06.000216: step 3316, loss 0.00481253, acc 1\n",
      "2019-06-29T16:39:06.279415: step 3317, loss 0.00647573, acc 1\n",
      "2019-06-29T16:39:06.555612: step 3318, loss 0.0123507, acc 1\n",
      "2019-06-29T16:39:06.827806: step 3319, loss 0.00305008, acc 1\n",
      "2019-06-29T16:39:07.121014: step 3320, loss 0.0585183, acc 0.984375\n",
      "2019-06-29T16:39:07.392207: step 3321, loss 0.120991, acc 0.96875\n",
      "2019-06-29T16:39:07.673408: step 3322, loss 0.00270693, acc 1\n",
      "2019-06-29T16:39:07.942599: step 3323, loss 0.00649713, acc 1\n",
      "2019-06-29T16:39:08.229803: step 3324, loss 0.0274426, acc 0.984375\n",
      "2019-06-29T16:39:08.503999: step 3325, loss 0.00361927, acc 1\n",
      "2019-06-29T16:39:08.767188: step 3326, loss 0.0353257, acc 0.984375\n",
      "2019-06-29T16:39:09.035377: step 3327, loss 0.00447413, acc 1\n",
      "2019-06-29T16:39:09.315576: step 3328, loss 0.00709509, acc 1\n",
      "2019-06-29T16:39:09.585769: step 3329, loss 0.00738305, acc 1\n",
      "2019-06-29T16:39:09.856962: step 3330, loss 0.00717637, acc 1\n",
      "2019-06-29T16:39:10.129156: step 3331, loss 0.00623929, acc 1\n",
      "2019-06-29T16:39:10.410356: step 3332, loss 0.0117014, acc 1\n",
      "2019-06-29T16:39:10.685552: step 3333, loss 0.00661342, acc 1\n",
      "2019-06-29T16:39:10.950741: step 3334, loss 0.0167985, acc 0.984375\n",
      "2019-06-29T16:39:11.221935: step 3335, loss 0.0254028, acc 1\n",
      "2019-06-29T16:39:11.534156: step 3336, loss 0.00306176, acc 1\n",
      "2019-06-29T16:39:11.824364: step 3337, loss 0.00448833, acc 1\n",
      "2019-06-29T16:39:12.096556: step 3338, loss 0.00428334, acc 1\n",
      "2019-06-29T16:39:12.368750: step 3339, loss 0.0214721, acc 0.984375\n",
      "2019-06-29T16:39:12.685976: step 3340, loss 0.00396582, acc 1\n",
      "2019-06-29T16:39:12.959171: step 3341, loss 0.00761912, acc 1\n",
      "2019-06-29T16:39:13.226573: step 3342, loss 0.00398863, acc 1\n",
      "2019-06-29T16:39:13.490762: step 3343, loss 0.0214437, acc 0.984375\n",
      "2019-06-29T16:39:13.784971: step 3344, loss 0.015832, acc 1\n",
      "2019-06-29T16:39:14.059167: step 3345, loss 0.00355529, acc 1\n",
      "2019-06-29T16:39:14.327357: step 3346, loss 0.0032834, acc 1\n",
      "2019-06-29T16:39:14.663597: step 3347, loss 0.0498149, acc 0.984375\n",
      "2019-06-29T16:39:14.985826: step 3348, loss 0.0102049, acc 1\n",
      "2019-06-29T16:39:15.290044: step 3349, loss 0.00884379, acc 1\n",
      "2019-06-29T16:39:15.587478: step 3350, loss 0.00250738, acc 1\n",
      "2019-06-29T16:39:15.867678: step 3351, loss 0.0028503, acc 1\n",
      "2019-06-29T16:39:16.150880: step 3352, loss 0.00322935, acc 1\n",
      "2019-06-29T16:39:16.449093: step 3353, loss 0.00130815, acc 1\n",
      "2019-06-29T16:39:16.764316: step 3354, loss 0.0094368, acc 1\n",
      "2019-06-29T16:39:17.068533: step 3355, loss 0.0121063, acc 1\n",
      "2019-06-29T16:39:17.352736: step 3356, loss 0.00869304, acc 1\n",
      "2019-06-29T16:39:17.671963: step 3357, loss 0.0111296, acc 1\n",
      "2019-06-29T16:39:17.957165: step 3358, loss 0.00883909, acc 1\n",
      "2019-06-29T16:39:18.287400: step 3359, loss 0.0151675, acc 1\n",
      "2019-06-29T16:39:18.566599: step 3360, loss 0.0077707, acc 1\n",
      "2019-06-29T16:39:18.865812: step 3361, loss 0.0335431, acc 0.96875\n",
      "2019-06-29T16:39:19.152016: step 3362, loss 0.00646862, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-29T16:39:19.445226: step 3363, loss 0.00495337, acc 1\n",
      "2019-06-29T16:39:19.736432: step 3364, loss 0.00745461, acc 1\n",
      "2019-06-29T16:39:20.017632: step 3365, loss 0.0101489, acc 1\n",
      "2019-06-29T16:39:20.324851: step 3366, loss 0.00218953, acc 1\n",
      "2019-06-29T16:39:20.612056: step 3367, loss 0.00823543, acc 1\n",
      "2019-06-29T16:39:20.904264: step 3368, loss 0.00393413, acc 1\n",
      "2019-06-29T16:39:21.206480: step 3369, loss 0.00395283, acc 1\n",
      "2019-06-29T16:39:21.511696: step 3370, loss 0.00726495, acc 1\n",
      "2019-06-29T16:39:21.799902: step 3371, loss 0.0157305, acc 1\n",
      "2019-06-29T16:39:22.090108: step 3372, loss 0.00277474, acc 1\n",
      "2019-06-29T16:39:22.368306: step 3373, loss 0.115111, acc 0.96875\n",
      "2019-06-29T16:39:22.682530: step 3374, loss 0.00586898, acc 1\n",
      "2019-06-29T16:39:22.955725: step 3375, loss 0.00636331, acc 1\n",
      "2019-06-29T16:39:23.227918: step 3376, loss 0.0165011, acc 1\n",
      "2019-06-29T16:39:23.503115: step 3377, loss 0.00530096, acc 1\n",
      "2019-06-29T16:39:23.808332: step 3378, loss 0.00431169, acc 1\n",
      "2019-06-29T16:39:24.073520: step 3379, loss 0.00523905, acc 1\n",
      "2019-06-29T16:39:24.345714: step 3380, loss 0.00430112, acc 1\n",
      "2019-06-29T16:39:24.611904: step 3381, loss 0.00221171, acc 1\n",
      "2019-06-29T16:39:25.001231: step 3382, loss 0.021215, acc 1\n",
      "2019-06-29T16:39:25.290302: step 3383, loss 0.00373029, acc 1\n",
      "2019-06-29T16:39:25.615534: step 3384, loss 0.0171617, acc 1\n",
      "2019-06-29T16:39:25.899737: step 3385, loss 0.00345937, acc 1\n",
      "2019-06-29T16:39:26.223968: step 3386, loss 0.00310127, acc 1\n",
      "2019-06-29T16:39:26.530185: step 3387, loss 0.0495597, acc 0.984375\n",
      "2019-06-29T16:39:26.804380: step 3388, loss 0.0242256, acc 1\n",
      "2019-06-29T16:39:27.092586: step 3389, loss 0.0400796, acc 0.96875\n",
      "2019-06-29T16:39:27.366780: step 3390, loss 0.00170071, acc 1\n",
      "2019-06-29T16:39:27.637974: step 3391, loss 0.00498158, acc 1\n",
      "2019-06-29T16:39:27.911169: step 3392, loss 0.0106288, acc 1\n",
      "2019-06-29T16:39:28.200374: step 3393, loss 0.00363258, acc 1\n",
      "2019-06-29T16:39:28.465564: step 3394, loss 0.0217186, acc 0.984375\n",
      "2019-06-29T16:39:28.732754: step 3395, loss 0.0129061, acc 1\n",
      "2019-06-29T16:39:29.003947: step 3396, loss 0.0213619, acc 0.984375\n",
      "2019-06-29T16:39:29.283145: step 3397, loss 0.00590272, acc 1\n",
      "2019-06-29T16:39:29.560343: step 3398, loss 0.004829, acc 1\n",
      "2019-06-29T16:39:29.832536: step 3399, loss 0.00893023, acc 1\n",
      "2019-06-29T16:39:30.102729: step 3400, loss 0.00616148, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-06-29T16:39:30.817239: step 3400, loss 1.07465, acc 0.729831\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\mao\\Desktop\\Tensorflow\\CNN\\1\\cnn-text-classification-tf-master\\runs\\1561796526\\checkpoints\\model-3400\n",
      "\n",
      "2019-06-29T16:39:32.447398: step 3401, loss 0.00567164, acc 1\n",
      "2019-06-29T16:39:32.739606: step 3402, loss 0.0167906, acc 1\n",
      "2019-06-29T16:39:33.023808: step 3403, loss 0.0215737, acc 1\n",
      "2019-06-29T16:39:33.312013: step 3404, loss 0.0068239, acc 1\n",
      "2019-06-29T16:39:33.637246: step 3405, loss 0.0113525, acc 1\n",
      "2019-06-29T16:39:33.922449: step 3406, loss 0.0284845, acc 1\n",
      "2019-06-29T16:39:34.208806: step 3407, loss 0.00541588, acc 1\n",
      "2019-06-29T16:39:34.499012: step 3408, loss 0.0179793, acc 0.984375\n",
      "2019-06-29T16:39:34.808766: step 3409, loss 0.0286654, acc 0.984375\n",
      "2019-06-29T16:39:35.108979: step 3410, loss 0.00265634, acc 1\n",
      "2019-06-29T16:39:35.412195: step 3411, loss 0.00951787, acc 1\n",
      "2019-06-29T16:39:35.715411: step 3412, loss 0.00942133, acc 1\n",
      "2019-06-29T16:39:36.005617: step 3413, loss 0.00480253, acc 1\n",
      "2019-06-29T16:39:36.295825: step 3414, loss 0.006857, acc 1\n",
      "2019-06-29T16:39:36.589034: step 3415, loss 0.00171976, acc 1\n",
      "2019-06-29T16:39:36.881241: step 3416, loss 0.0132558, acc 1\n",
      "2019-06-29T16:39:37.171448: step 3417, loss 0.0106081, acc 1\n",
      "2019-06-29T16:39:37.461654: step 3418, loss 0.00639693, acc 1\n",
      "2019-06-29T16:39:37.749859: step 3419, loss 0.00138578, acc 1\n",
      "2019-06-29T16:39:38.061081: step 3420, loss 0.0384191, acc 0.96875\n",
      "2019-06-29T16:39:38.345283: step 3421, loss 0.0215059, acc 0.984375\n",
      "2019-06-29T16:39:38.639493: step 3422, loss 0.0013148, acc 1\n",
      "2019-06-29T16:39:38.909686: step 3423, loss 0.0158477, acc 1\n",
      "2019-06-29T16:39:39.192887: step 3424, loss 0.00990728, acc 1\n",
      "2019-06-29T16:39:39.470084: step 3425, loss 0.0084063, acc 1\n",
      "2019-06-29T16:39:39.747281: step 3426, loss 0.00317233, acc 1\n",
      "2019-06-29T16:39:40.016473: step 3427, loss 0.00822729, acc 1\n",
      "2019-06-29T16:39:40.306680: step 3428, loss 0.00280202, acc 1\n",
      "2019-06-29T16:39:40.579874: step 3429, loss 0.0208775, acc 0.984375\n",
      "2019-06-29T16:39:40.848065: step 3430, loss 0.0045005, acc 1\n",
      "2019-06-29T16:39:41.125262: step 3431, loss 0.0106924, acc 1\n",
      "2019-06-29T16:39:41.416470: step 3432, loss 0.0684748, acc 0.984375\n",
      "2019-06-29T16:39:41.692666: step 3433, loss 0.00490176, acc 1\n",
      "2019-06-29T16:39:41.962859: step 3434, loss 0.0233117, acc 0.984375\n",
      "2019-06-29T16:39:42.231050: step 3435, loss 0.00290586, acc 1\n",
      "2019-06-29T16:39:42.531263: step 3436, loss 0.0219755, acc 1\n",
      "2019-06-29T16:39:42.803458: step 3437, loss 0.00915546, acc 1\n",
      "2019-06-29T16:39:43.074651: step 3438, loss 0.00304502, acc 1\n",
      "2019-06-29T16:39:43.341840: step 3439, loss 0.0135137, acc 1\n",
      "2019-06-29T16:39:43.628045: step 3440, loss 0.00861538, acc 1\n",
      "2019-06-29T16:39:43.898237: step 3441, loss 0.011744, acc 1\n",
      "2019-06-29T16:39:44.171432: step 3442, loss 0.0159397, acc 0.984375\n",
      "2019-06-29T16:39:44.447628: step 3443, loss 0.00477609, acc 1\n",
      "2019-06-29T16:39:44.738836: step 3444, loss 0.0063196, acc 1\n",
      "2019-06-29T16:39:45.040051: step 3445, loss 0.00982989, acc 1\n",
      "2019-06-29T16:39:45.335260: step 3446, loss 0.013965, acc 0.984375\n",
      "2019-06-29T16:39:45.610217: step 3447, loss 0.00788125, acc 1\n",
      "2019-06-29T16:39:45.882410: step 3448, loss 0.0060609, acc 1\n",
      "2019-06-29T16:39:46.146600: step 3449, loss 0.00868156, acc 1\n",
      "2019-06-29T16:39:46.463611: step 3450, loss 0.0800857, acc 0.983333\n",
      "2019-06-29T16:39:46.835139: step 3451, loss 0.00335351, acc 1\n",
      "2019-06-29T16:39:47.202401: step 3452, loss 0.00541194, acc 1\n",
      "2019-06-29T16:39:47.608690: step 3453, loss 0.002233, acc 1\n",
      "2019-06-29T16:39:47.953948: step 3454, loss 0.0086963, acc 1\n",
      "2019-06-29T16:39:48.267170: step 3455, loss 0.00156691, acc 1\n",
      "2019-06-29T16:39:48.556377: step 3456, loss 0.00228109, acc 1\n",
      "2019-06-29T16:39:48.846583: step 3457, loss 0.00960935, acc 1\n",
      "2019-06-29T16:39:49.157806: step 3458, loss 0.00307728, acc 1\n",
      "2019-06-29T16:39:49.443007: step 3459, loss 0.0123508, acc 1\n",
      "2019-06-29T16:39:49.731213: step 3460, loss 0.00643757, acc 1\n",
      "2019-06-29T16:39:50.012414: step 3461, loss 0.0170109, acc 1\n",
      "2019-06-29T16:39:50.316630: step 3462, loss 0.00475274, acc 1\n",
      "2019-06-29T16:39:50.602834: step 3463, loss 0.00445156, acc 1\n",
      "2019-06-29T16:39:50.888037: step 3464, loss 0.00257969, acc 1\n",
      "2019-06-29T16:39:51.175242: step 3465, loss 0.00279705, acc 1\n",
      "2019-06-29T16:39:51.462446: step 3466, loss 0.00819912, acc 1\n",
      "2019-06-29T16:39:51.758656: step 3467, loss 0.0214143, acc 0.984375\n",
      "2019-06-29T16:39:52.060872: step 3468, loss 0.00978333, acc 1\n",
      "2019-06-29T16:39:52.414123: step 3469, loss 0.0396841, acc 0.984375\n",
      "2019-06-29T16:39:52.713336: step 3470, loss 0.0254867, acc 0.984375\n",
      "2019-06-29T16:39:52.996537: step 3471, loss 0.0249603, acc 0.984375\n",
      "2019-06-29T16:39:53.314764: step 3472, loss 0.0133201, acc 1\n",
      "2019-06-29T16:39:53.622985: step 3473, loss 0.00374432, acc 1\n",
      "2019-06-29T16:39:53.911189: step 3474, loss 0.00621528, acc 1\n",
      "2019-06-29T16:39:54.231417: step 3475, loss 0.0319214, acc 0.984375\n",
      "2019-06-29T16:39:54.526628: step 3476, loss 0.0025723, acc 1\n",
      "2019-06-29T16:39:54.818835: step 3477, loss 0.00549403, acc 1\n",
      "2019-06-29T16:39:55.111044: step 3478, loss 0.00703287, acc 1\n",
      "2019-06-29T16:39:55.414259: step 3479, loss 0.00295111, acc 1\n",
      "2019-06-29T16:39:55.697460: step 3480, loss 0.00743981, acc 1\n",
      "2019-06-29T16:39:55.971657: step 3481, loss 0.0336075, acc 0.984375\n",
      "2019-06-29T16:39:56.277874: step 3482, loss 0.0242839, acc 0.984375\n",
      "2019-06-29T16:39:56.553070: step 3483, loss 0.00122124, acc 1\n",
      "2019-06-29T16:39:56.862290: step 3484, loss 0.00500164, acc 1\n",
      "2019-06-29T16:39:57.138487: step 3485, loss 0.000780642, acc 1\n",
      "2019-06-29T16:39:57.454713: step 3486, loss 0.00148325, acc 1\n",
      "2019-06-29T16:39:57.739915: step 3487, loss 0.0291039, acc 0.984375\n",
      "2019-06-29T16:39:58.071152: step 3488, loss 0.00132445, acc 1\n",
      "2019-06-29T16:39:58.352351: step 3489, loss 0.00391677, acc 1\n",
      "2019-06-29T16:39:58.628548: step 3490, loss 0.0156053, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-29T16:39:58.897739: step 3491, loss 0.00662284, acc 1\n",
      "2019-06-29T16:39:59.197954: step 3492, loss 0.00924441, acc 1\n",
      "2019-06-29T16:39:59.480153: step 3493, loss 0.0100065, acc 1\n",
      "2019-06-29T16:39:59.769359: step 3494, loss 0.00361877, acc 1\n",
      "2019-06-29T16:40:00.053579: step 3495, loss 0.00778287, acc 1\n",
      "2019-06-29T16:40:00.357804: step 3496, loss 0.00259035, acc 1\n",
      "2019-06-29T16:40:00.631999: step 3497, loss 0.0260354, acc 0.984375\n",
      "2019-06-29T16:40:00.902191: step 3498, loss 0.0140184, acc 1\n",
      "2019-06-29T16:40:01.167380: step 3499, loss 0.00512668, acc 1\n",
      "2019-06-29T16:40:01.465592: step 3500, loss 0.00329696, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-06-29T16:40:02.115586: step 3500, loss 1.11874, acc 0.72045\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\mao\\Desktop\\Tensorflow\\CNN\\1\\cnn-text-classification-tf-master\\runs\\1561796526\\checkpoints\\model-3500\n",
      "\n",
      "2019-06-29T16:40:03.868366: step 3501, loss 0.00309459, acc 1\n",
      "2019-06-29T16:40:04.150567: step 3502, loss 0.00458868, acc 1\n",
      "2019-06-29T16:40:04.431767: step 3503, loss 0.00812634, acc 1\n",
      "2019-06-29T16:40:04.727979: step 3504, loss 0.0465497, acc 0.984375\n",
      "2019-06-29T16:40:05.038199: step 3505, loss 0.0166349, acc 0.984375\n",
      "2019-06-29T16:40:05.322401: step 3506, loss 0.00265252, acc 1\n",
      "2019-06-29T16:40:05.612608: step 3507, loss 0.0210528, acc 0.984375\n",
      "2019-06-29T16:40:05.922829: step 3508, loss 0.0112261, acc 1\n",
      "2019-06-29T16:40:06.223042: step 3509, loss 0.0105728, acc 1\n",
      "2019-06-29T16:40:06.508245: step 3510, loss 0.00763162, acc 1\n",
      "2019-06-29T16:40:06.811461: step 3511, loss 0.00488426, acc 1\n",
      "2019-06-29T16:40:07.155707: step 3512, loss 0.0251665, acc 0.984375\n",
      "2019-06-29T16:40:07.447916: step 3513, loss 0.00971288, acc 1\n",
      "2019-06-29T16:40:07.745126: step 3514, loss 0.00535072, acc 1\n",
      "2019-06-29T16:40:08.034332: step 3515, loss 0.00486541, acc 1\n",
      "2019-06-29T16:40:08.367569: step 3516, loss 0.021553, acc 1\n",
      "2019-06-29T16:40:08.672786: step 3517, loss 0.00739833, acc 1\n",
      "2019-06-29T16:40:08.957990: step 3518, loss 0.0046859, acc 1\n",
      "2019-06-29T16:40:09.255201: step 3519, loss 0.00455711, acc 1\n",
      "2019-06-29T16:40:09.552413: step 3520, loss 0.00162556, acc 1\n",
      "2019-06-29T16:40:09.843620: step 3521, loss 0.00937392, acc 1\n",
      "2019-06-29T16:40:10.132826: step 3522, loss 0.00662034, acc 1\n",
      "2019-06-29T16:40:10.452053: step 3523, loss 0.00209631, acc 1\n",
      "2019-06-29T16:40:10.750265: step 3524, loss 0.0281787, acc 0.984375\n",
      "2019-06-29T16:40:11.038471: step 3525, loss 0.0154404, acc 1\n",
      "2019-06-29T16:40:11.360701: step 3526, loss 0.00223127, acc 1\n",
      "2019-06-29T16:40:11.639899: step 3527, loss 0.00374947, acc 1\n",
      "2019-06-29T16:40:11.925102: step 3528, loss 0.00263204, acc 1\n",
      "2019-06-29T16:40:12.235323: step 3529, loss 0.0179349, acc 0.984375\n",
      "2019-06-29T16:40:12.535536: step 3530, loss 0.00417055, acc 1\n",
      "2019-06-29T16:40:12.808731: step 3531, loss 0.0119622, acc 1\n",
      "2019-06-29T16:40:13.075921: step 3532, loss 0.00234916, acc 1\n",
      "2019-06-29T16:40:13.359123: step 3533, loss 0.0120243, acc 1\n",
      "2019-06-29T16:40:13.639322: step 3534, loss 0.0289726, acc 0.984375\n",
      "2019-06-29T16:40:13.929530: step 3535, loss 0.00475703, acc 1\n",
      "2019-06-29T16:40:14.208728: step 3536, loss 0.0100663, acc 1\n",
      "2019-06-29T16:40:14.474918: step 3537, loss 0.0119271, acc 1\n",
      "2019-06-29T16:40:14.756117: step 3538, loss 0.0267864, acc 0.984375\n",
      "2019-06-29T16:40:15.046325: step 3539, loss 0.0257496, acc 0.984375\n",
      "2019-06-29T16:40:15.334530: step 3540, loss 0.00803136, acc 1\n",
      "2019-06-29T16:40:15.601719: step 3541, loss 0.0274825, acc 0.984375\n",
      "2019-06-29T16:40:15.909939: step 3542, loss 0.0446841, acc 0.984375\n",
      "2019-06-29T16:40:16.181132: step 3543, loss 0.00283158, acc 1\n",
      "2019-06-29T16:40:16.459330: step 3544, loss 0.00625151, acc 1\n",
      "2019-06-29T16:40:16.732525: step 3545, loss 0.0195644, acc 1\n",
      "2019-06-29T16:40:17.040744: step 3546, loss 0.00558616, acc 1\n",
      "2019-06-29T16:40:17.309936: step 3547, loss 0.0040718, acc 1\n",
      "2019-06-29T16:40:17.574124: step 3548, loss 0.00709534, acc 1\n",
      "2019-06-29T16:40:17.847997: step 3549, loss 0.00309582, acc 1\n",
      "2019-06-29T16:40:18.144715: step 3550, loss 0.00153557, acc 1\n",
      "2019-06-29T16:40:18.417910: step 3551, loss 0.00705594, acc 1\n",
      "2019-06-29T16:40:18.733134: step 3552, loss 0.0034944, acc 1\n",
      "2019-06-29T16:40:19.060369: step 3553, loss 0.023713, acc 0.984375\n",
      "2019-06-29T16:40:19.389604: step 3554, loss 0.0097151, acc 1\n",
      "2019-06-29T16:40:19.692656: step 3555, loss 0.00513307, acc 1\n",
      "2019-06-29T16:40:20.046908: step 3556, loss 0.0111522, acc 1\n",
      "2019-06-29T16:40:20.361131: step 3557, loss 0.0137219, acc 1\n",
      "2019-06-29T16:40:20.661344: step 3558, loss 0.00624097, acc 1\n",
      "2019-06-29T16:40:20.938542: step 3559, loss 0.00288217, acc 1\n",
      "2019-06-29T16:40:21.222745: step 3560, loss 0.00287821, acc 1\n",
      "2019-06-29T16:40:21.531965: step 3561, loss 0.00518454, acc 1\n",
      "2019-06-29T16:40:21.812164: step 3562, loss 0.00611965, acc 1\n",
      "2019-06-29T16:40:22.090362: step 3563, loss 0.00539254, acc 1\n",
      "2019-06-29T16:40:22.376566: step 3564, loss 0.00224108, acc 1\n",
      "2019-06-29T16:40:22.707801: step 3565, loss 0.00140495, acc 1\n",
      "2019-06-29T16:40:22.988001: step 3566, loss 0.0274571, acc 0.984375\n",
      "2019-06-29T16:40:23.292218: step 3567, loss 0.00666724, acc 1\n",
      "2019-06-29T16:40:23.590430: step 3568, loss 0.00898318, acc 1\n",
      "2019-06-29T16:40:23.917663: step 3569, loss 0.00909517, acc 1\n",
      "2019-06-29T16:40:24.201865: step 3570, loss 0.003913, acc 1\n",
      "2019-06-29T16:40:24.491072: step 3571, loss 0.0120232, acc 1\n",
      "2019-06-29T16:40:24.783279: step 3572, loss 0.00206179, acc 1\n",
      "2019-06-29T16:40:25.068482: step 3573, loss 0.0351413, acc 0.984375\n",
      "2019-06-29T16:40:25.354686: step 3574, loss 0.00198353, acc 1\n",
      "2019-06-29T16:40:25.640890: step 3575, loss 0.00393397, acc 1\n",
      "2019-06-29T16:40:25.950110: step 3576, loss 0.0346753, acc 0.984375\n",
      "2019-06-29T16:40:26.250324: step 3577, loss 0.00171315, acc 1\n",
      "2019-06-29T16:40:26.544533: step 3578, loss 0.00768401, acc 1\n",
      "2019-06-29T16:40:26.838743: step 3579, loss 0.00373658, acc 1\n",
      "2019-06-29T16:40:27.129950: step 3580, loss 0.00592278, acc 1\n",
      "2019-06-29T16:40:27.426161: step 3581, loss 0.00706742, acc 1\n",
      "2019-06-29T16:40:27.724373: step 3582, loss 0.00933774, acc 1\n",
      "2019-06-29T16:40:28.011578: step 3583, loss 0.0016154, acc 1\n",
      "2019-06-29T16:40:28.288775: step 3584, loss 0.00333076, acc 1\n",
      "2019-06-29T16:40:28.578982: step 3585, loss 0.00407424, acc 1\n",
      "2019-06-29T16:40:28.844171: step 3586, loss 0.00657634, acc 1\n",
      "2019-06-29T16:40:29.123372: step 3587, loss 0.00855257, acc 1\n",
      "2019-06-29T16:40:29.420582: step 3588, loss 0.00290135, acc 1\n",
      "2019-06-29T16:40:29.688772: step 3589, loss 0.00391385, acc 1\n",
      "2019-06-29T16:40:29.962967: step 3590, loss 0.00848581, acc 1\n",
      "2019-06-29T16:40:30.241165: step 3591, loss 0.0299912, acc 0.984375\n",
      "2019-06-29T16:40:30.531372: step 3592, loss 0.0156272, acc 1\n",
      "2019-06-29T16:40:30.802566: step 3593, loss 0.00197423, acc 1\n",
      "2019-06-29T16:40:31.073758: step 3594, loss 0.0286788, acc 0.984375\n",
      "2019-06-29T16:40:31.361963: step 3595, loss 0.00156792, acc 1\n",
      "2019-06-29T16:40:31.720219: step 3596, loss 0.00542915, acc 1\n",
      "2019-06-29T16:40:32.002420: step 3597, loss 0.00712495, acc 1\n",
      "2019-06-29T16:40:32.270610: step 3598, loss 0.0230944, acc 0.984375\n",
      "2019-06-29T16:40:32.552812: step 3599, loss 0.00899269, acc 1\n",
      "2019-06-29T16:40:32.832010: step 3600, loss 0.0028293, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-06-29T16:40:33.485539: step 3600, loss 1.13234, acc 0.725141\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\mao\\Desktop\\Tensorflow\\CNN\\1\\cnn-text-classification-tf-master\\runs\\1561796526\\checkpoints\\model-3600\n",
      "\n",
      "2019-06-29T16:40:35.188602: step 3601, loss 0.0306744, acc 0.984375\n",
      "2019-06-29T16:40:35.497822: step 3602, loss 0.00512674, acc 1\n",
      "2019-06-29T16:40:35.795215: step 3603, loss 0.0361689, acc 0.984375\n",
      "2019-06-29T16:40:36.072792: step 3604, loss 0.00139546, acc 1\n",
      "2019-06-29T16:40:36.374006: step 3605, loss 0.00703025, acc 1\n",
      "2019-06-29T16:40:36.685228: step 3606, loss 0.00440264, acc 1\n",
      "2019-06-29T16:40:36.965427: step 3607, loss 0.00485946, acc 1\n",
      "2019-06-29T16:40:37.264640: step 3608, loss 0.0479243, acc 0.984375\n",
      "2019-06-29T16:40:37.562852: step 3609, loss 0.00269231, acc 1\n",
      "2019-06-29T16:40:37.907697: step 3610, loss 0.0028847, acc 1\n",
      "2019-06-29T16:40:38.200906: step 3611, loss 0.0174134, acc 0.984375\n",
      "2019-06-29T16:40:38.544397: step 3612, loss 0.00182145, acc 1\n",
      "2019-06-29T16:40:38.841607: step 3613, loss 0.00557699, acc 1\n",
      "2019-06-29T16:40:39.128813: step 3614, loss 0.0162272, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-29T16:40:39.408012: step 3615, loss 0.0319959, acc 1\n",
      "2019-06-29T16:40:39.905393: step 3616, loss 0.0095953, acc 1\n",
      "2019-06-29T16:40:40.210609: step 3617, loss 0.00311373, acc 1\n",
      "2019-06-29T16:40:40.583875: step 3618, loss 0.017087, acc 1\n",
      "2019-06-29T16:40:40.947133: step 3619, loss 0.00228031, acc 1\n",
      "2019-06-29T16:40:41.250349: step 3620, loss 0.0226407, acc 0.984375\n",
      "2019-06-29T16:40:41.552565: step 3621, loss 0.0228242, acc 0.984375\n",
      "2019-06-29T16:40:41.846774: step 3622, loss 0.03278, acc 0.984375\n",
      "2019-06-29T16:40:42.149989: step 3623, loss 0.00609125, acc 1\n",
      "2019-06-29T16:40:42.439196: step 3624, loss 0.00303516, acc 1\n",
      "2019-06-29T16:40:42.736408: step 3625, loss 0.0106678, acc 1\n",
      "2019-06-29T16:40:43.022611: step 3626, loss 0.0171271, acc 1\n",
      "2019-06-29T16:40:43.295806: step 3627, loss 0.00175061, acc 1\n",
      "2019-06-29T16:40:43.566998: step 3628, loss 0.0114006, acc 1\n",
      "2019-06-29T16:40:43.830186: step 3629, loss 0.00290337, acc 1\n",
      "2019-06-29T16:40:44.109384: step 3630, loss 0.0119394, acc 1\n",
      "2019-06-29T16:40:44.383580: step 3631, loss 0.00308769, acc 1\n",
      "2019-06-29T16:40:44.653773: step 3632, loss 0.00449956, acc 1\n",
      "2019-06-29T16:40:44.922964: step 3633, loss 0.00412035, acc 1\n",
      "2019-06-29T16:40:45.197160: step 3634, loss 0.00221146, acc 1\n",
      "2019-06-29T16:40:45.471354: step 3635, loss 0.00843825, acc 1\n",
      "2019-06-29T16:40:45.744549: step 3636, loss 0.00801708, acc 1\n",
      "2019-06-29T16:40:46.018745: step 3637, loss 0.00272651, acc 1\n",
      "2019-06-29T16:40:46.314956: step 3638, loss 0.00743293, acc 1\n",
      "2019-06-29T16:40:46.607791: step 3639, loss 0.00843754, acc 1\n",
      "2019-06-29T16:40:46.927020: step 3640, loss 0.00647557, acc 1\n",
      "2019-06-29T16:40:47.230235: step 3641, loss 0.0126808, acc 1\n",
      "2019-06-29T16:40:47.579483: step 3642, loss 0.0607801, acc 0.984375\n",
      "2019-06-29T16:40:47.891707: step 3643, loss 0.0079424, acc 1\n",
      "2019-06-29T16:40:48.180911: step 3644, loss 0.00202328, acc 1\n",
      "2019-06-29T16:40:48.454106: step 3645, loss 0.00404966, acc 1\n",
      "2019-06-29T16:40:48.759323: step 3646, loss 0.00548861, acc 1\n",
      "2019-06-29T16:40:49.029515: step 3647, loss 0.00277485, acc 1\n",
      "2019-06-29T16:40:49.302710: step 3648, loss 0.00743428, acc 1\n",
      "2019-06-29T16:40:49.591916: step 3649, loss 0.00584195, acc 1\n",
      "2019-06-29T16:40:49.859106: step 3650, loss 0.00573525, acc 1\n",
      "2019-06-29T16:40:50.150880: step 3651, loss 0.00252783, acc 1\n",
      "2019-06-29T16:40:50.423590: step 3652, loss 0.0115108, acc 1\n",
      "2019-06-29T16:40:50.731809: step 3653, loss 0.00612503, acc 1\n",
      "2019-06-29T16:40:51.033024: step 3654, loss 0.00226573, acc 1\n",
      "2019-06-29T16:40:51.325232: step 3655, loss 0.00623203, acc 1\n",
      "2019-06-29T16:40:51.626235: step 3656, loss 0.0272223, acc 1\n",
      "2019-06-29T16:40:51.946463: step 3657, loss 0.00717426, acc 1\n",
      "2019-06-29T16:40:52.242674: step 3658, loss 0.00402634, acc 1\n",
      "2019-06-29T16:40:52.525877: step 3659, loss 0.00248005, acc 1\n",
      "2019-06-29T16:40:52.805075: step 3660, loss 0.00951884, acc 1\n",
      "2019-06-29T16:40:53.161328: step 3661, loss 0.00619016, acc 1\n",
      "2019-06-29T16:40:53.442528: step 3662, loss 0.0134087, acc 1\n",
      "2019-06-29T16:40:53.722729: step 3663, loss 0.00719124, acc 1\n",
      "2019-06-29T16:40:54.004930: step 3664, loss 0.00481553, acc 1\n",
      "2019-06-29T16:40:54.298138: step 3665, loss 0.00551966, acc 1\n",
      "2019-06-29T16:40:54.579338: step 3666, loss 0.0142507, acc 1\n",
      "2019-06-29T16:40:54.858537: step 3667, loss 0.00544829, acc 1\n",
      "2019-06-29T16:40:55.144741: step 3668, loss 0.00162788, acc 1\n",
      "2019-06-29T16:40:55.432946: step 3669, loss 0.0089873, acc 1\n",
      "2019-06-29T16:40:55.721151: step 3670, loss 0.0137637, acc 1\n",
      "2019-06-29T16:40:56.015360: step 3671, loss 0.0141128, acc 1\n",
      "2019-06-29T16:40:56.302566: step 3672, loss 0.0120445, acc 1\n",
      "2019-06-29T16:40:56.593772: step 3673, loss 0.043353, acc 0.96875\n",
      "2019-06-29T16:40:56.882978: step 3674, loss 0.00154736, acc 1\n",
      "2019-06-29T16:40:57.163177: step 3675, loss 0.00326001, acc 1\n",
      "2019-06-29T16:40:57.452383: step 3676, loss 0.00527481, acc 1\n",
      "2019-06-29T16:40:57.743591: step 3677, loss 0.00413691, acc 1\n",
      "2019-06-29T16:40:58.040802: step 3678, loss 0.00382112, acc 1\n",
      "2019-06-29T16:40:58.332010: step 3679, loss 0.011119, acc 1\n",
      "2019-06-29T16:40:58.681258: step 3680, loss 0.0254651, acc 0.984375\n",
      "2019-06-29T16:40:58.995482: step 3681, loss 0.00359997, acc 1\n",
      "2019-06-29T16:40:59.280685: step 3682, loss 0.00605148, acc 1\n",
      "2019-06-29T16:40:59.567891: step 3683, loss 0.00316404, acc 1\n",
      "2019-06-29T16:40:59.835081: step 3684, loss 0.00616703, acc 1\n",
      "2019-06-29T16:41:00.108293: step 3685, loss 0.00767978, acc 1\n",
      "2019-06-29T16:41:00.381487: step 3686, loss 0.0100646, acc 1\n",
      "2019-06-29T16:41:00.685704: step 3687, loss 0.00191819, acc 1\n",
      "2019-06-29T16:41:00.956897: step 3688, loss 0.00943744, acc 1\n",
      "2019-06-29T16:41:01.229091: step 3689, loss 0.00768164, acc 1\n",
      "2019-06-29T16:41:01.498282: step 3690, loss 0.000641969, acc 1\n",
      "2019-06-29T16:41:01.776480: step 3691, loss 0.0038768, acc 1\n",
      "2019-06-29T16:41:02.053678: step 3692, loss 0.00652721, acc 1\n",
      "2019-06-29T16:41:02.317867: step 3693, loss 0.00200588, acc 1\n",
      "2019-06-29T16:41:02.592061: step 3694, loss 0.00684551, acc 1\n",
      "2019-06-29T16:41:02.867256: step 3695, loss 0.014706, acc 1\n",
      "2019-06-29T16:41:03.135448: step 3696, loss 0.00600986, acc 1\n",
      "2019-06-29T16:41:03.403638: step 3697, loss 0.00493802, acc 1\n",
      "2019-06-29T16:41:03.679835: step 3698, loss 0.00165401, acc 1\n",
      "2019-06-29T16:41:03.953029: step 3699, loss 0.00166381, acc 1\n",
      "2019-06-29T16:41:04.221220: step 3700, loss 0.00254791, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-06-29T16:41:04.875690: step 3700, loss 1.14282, acc 0.72045\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\mao\\Desktop\\Tensorflow\\CNN\\1\\cnn-text-classification-tf-master\\runs\\1561796526\\checkpoints\\model-3700\n",
      "\n",
      "2019-06-29T16:41:06.019695: step 3701, loss 0.00171444, acc 1\n",
      "2019-06-29T16:41:06.559078: step 3702, loss 0.00434491, acc 1\n",
      "2019-06-29T16:41:06.865297: step 3703, loss 0.0102395, acc 1\n",
      "2019-06-29T16:41:07.161508: step 3704, loss 0.00239343, acc 1\n",
      "2019-06-29T16:41:07.487744: step 3705, loss 0.00972409, acc 1\n",
      "2019-06-29T16:41:07.757287: step 3706, loss 0.00423933, acc 1\n",
      "2019-06-29T16:41:08.039488: step 3707, loss 0.00848428, acc 1\n",
      "2019-06-29T16:41:08.324691: step 3708, loss 0.00227053, acc 1\n",
      "2019-06-29T16:41:08.631910: step 3709, loss 0.00363394, acc 1\n",
      "2019-06-29T16:41:08.923119: step 3710, loss 0.00542379, acc 1\n",
      "2019-06-29T16:41:09.202316: step 3711, loss 0.00182923, acc 1\n",
      "2019-06-29T16:41:09.496526: step 3712, loss 0.0168131, acc 1\n",
      "2019-06-29T16:41:09.799742: step 3713, loss 0.00249155, acc 1\n",
      "2019-06-29T16:41:10.086945: step 3714, loss 0.00367119, acc 1\n",
      "2019-06-29T16:41:10.368146: step 3715, loss 0.00648703, acc 1\n",
      "2019-06-29T16:41:10.663356: step 3716, loss 0.00258201, acc 1\n",
      "2019-06-29T16:41:10.945557: step 3717, loss 0.0120932, acc 1\n",
      "2019-06-29T16:41:11.233764: step 3718, loss 0.00182389, acc 1\n",
      "2019-06-29T16:41:11.529973: step 3719, loss 0.00864675, acc 1\n",
      "2019-06-29T16:41:11.832189: step 3720, loss 0.00915506, acc 1\n",
      "2019-06-29T16:41:12.115389: step 3721, loss 0.00204744, acc 1\n",
      "2019-06-29T16:41:12.404595: step 3722, loss 0.00789692, acc 1\n",
      "2019-06-29T16:41:12.705810: step 3723, loss 0.00209876, acc 1\n",
      "2019-06-29T16:41:13.020034: step 3724, loss 0.00112955, acc 1\n",
      "2019-06-29T16:41:13.304236: step 3725, loss 0.0359679, acc 0.984375\n",
      "2019-06-29T16:41:13.592442: step 3726, loss 0.00189061, acc 1\n",
      "2019-06-29T16:41:13.882648: step 3727, loss 0.00230069, acc 1\n",
      "2019-06-29T16:41:14.182861: step 3728, loss 0.00782847, acc 1\n",
      "2019-06-29T16:41:14.468066: step 3729, loss 0.00924302, acc 1\n",
      "2019-06-29T16:41:14.752267: step 3730, loss 0.0103388, acc 1\n",
      "2019-06-29T16:41:15.038471: step 3731, loss 0.00757895, acc 1\n",
      "2019-06-29T16:41:15.313667: step 3732, loss 0.0270518, acc 0.984375\n",
      "2019-06-29T16:41:15.587862: step 3733, loss 0.00456772, acc 1\n",
      "2019-06-29T16:41:15.850048: step 3734, loss 0.00263964, acc 1\n",
      "2019-06-29T16:41:16.117240: step 3735, loss 0.00346674, acc 1\n",
      "2019-06-29T16:41:16.391434: step 3736, loss 0.033547, acc 0.984375\n",
      "2019-06-29T16:41:16.673635: step 3737, loss 0.00331645, acc 1\n",
      "2019-06-29T16:41:16.948831: step 3738, loss 0.00335109, acc 1\n",
      "2019-06-29T16:41:17.223027: step 3739, loss 0.0136149, acc 1\n",
      "2019-06-29T16:41:17.492218: step 3740, loss 0.0176395, acc 1\n",
      "2019-06-29T16:41:17.772418: step 3741, loss 0.00167938, acc 1\n",
      "2019-06-29T16:41:18.043610: step 3742, loss 0.00932767, acc 1\n",
      "2019-06-29T16:41:18.313803: step 3743, loss 0.0311221, acc 0.984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-29T16:41:18.588999: step 3744, loss 0.00265725, acc 1\n",
      "2019-06-29T16:41:18.855188: step 3745, loss 0.0020126, acc 1\n",
      "2019-06-29T16:41:19.131385: step 3746, loss 0.00251971, acc 1\n",
      "2019-06-29T16:41:19.403579: step 3747, loss 0.00264404, acc 1\n",
      "2019-06-29T16:41:19.677774: step 3748, loss 0.00947598, acc 1\n",
      "2019-06-29T16:41:19.948968: step 3749, loss 0.00658432, acc 1\n",
      "2019-06-29T16:41:20.207151: step 3750, loss 0.0134069, acc 1\n",
      "2019-06-29T16:41:20.484349: step 3751, loss 0.00346213, acc 1\n",
      "2019-06-29T16:41:20.773556: step 3752, loss 0.00804606, acc 1\n",
      "2019-06-29T16:41:21.053753: step 3753, loss 0.00272885, acc 1\n",
      "2019-06-29T16:41:21.388992: step 3754, loss 0.00170025, acc 1\n",
      "2019-06-29T16:41:21.683202: step 3755, loss 0.0111747, acc 1\n",
      "2019-06-29T16:41:21.963401: step 3756, loss 0.00118243, acc 1\n",
      "2019-06-29T16:41:22.234627: step 3757, loss 0.00259779, acc 1\n",
      "2019-06-29T16:41:22.513825: step 3758, loss 0.00325835, acc 1\n",
      "2019-06-29T16:41:22.839057: step 3759, loss 0.00788098, acc 1\n",
      "2019-06-29T16:41:23.133267: step 3760, loss 0.00441458, acc 1\n",
      "2019-06-29T16:41:23.428477: step 3761, loss 0.00307811, acc 1\n",
      "2019-06-29T16:41:23.706149: step 3762, loss 0.00168133, acc 1\n",
      "2019-06-29T16:41:24.013367: step 3763, loss 0.00151042, acc 1\n",
      "2019-06-29T16:41:24.292566: step 3764, loss 0.00329833, acc 1\n",
      "2019-06-29T16:41:24.573766: step 3765, loss 0.017087, acc 1\n",
      "2019-06-29T16:41:24.855967: step 3766, loss 0.00088484, acc 1\n",
      "2019-06-29T16:41:25.149176: step 3767, loss 0.00140077, acc 1\n",
      "2019-06-29T16:41:25.431376: step 3768, loss 0.000460578, acc 1\n",
      "2019-06-29T16:41:25.708574: step 3769, loss 0.000778695, acc 1\n",
      "2019-06-29T16:41:25.982770: step 3770, loss 0.00380977, acc 1\n",
      "2019-06-29T16:41:26.271975: step 3771, loss 0.00307953, acc 1\n",
      "2019-06-29T16:41:26.555177: step 3772, loss 0.00438391, acc 1\n",
      "2019-06-29T16:41:26.833376: step 3773, loss 0.00903907, acc 1\n",
      "2019-06-29T16:41:27.119579: step 3774, loss 0.00485718, acc 1\n",
      "2019-06-29T16:41:27.408786: step 3775, loss 0.00771486, acc 1\n",
      "2019-06-29T16:41:27.701993: step 3776, loss 0.0073856, acc 1\n",
      "2019-06-29T16:41:27.989198: step 3777, loss 0.00776149, acc 1\n",
      "2019-06-29T16:41:28.280405: step 3778, loss 0.0105578, acc 1\n",
      "2019-06-29T16:41:28.571612: step 3779, loss 0.00107277, acc 1\n",
      "2019-06-29T16:41:28.858817: step 3780, loss 0.00402562, acc 1\n",
      "2019-06-29T16:41:29.140017: step 3781, loss 0.0094219, acc 1\n",
      "2019-06-29T16:41:29.462246: step 3782, loss 0.00140658, acc 1\n",
      "2019-06-29T16:41:29.747450: step 3783, loss 0.00525882, acc 1\n",
      "2019-06-29T16:41:30.033654: step 3784, loss 0.00292406, acc 1\n",
      "2019-06-29T16:41:30.321858: step 3785, loss 0.00246569, acc 1\n",
      "2019-06-29T16:41:30.649092: step 3786, loss 0.0014252, acc 1\n",
      "2019-06-29T16:41:30.912279: step 3787, loss 0.0138771, acc 0.984375\n",
      "2019-06-29T16:41:31.200485: step 3788, loss 0.00390527, acc 1\n",
      "2019-06-29T16:41:31.478682: step 3789, loss 0.00624352, acc 1\n",
      "2019-06-29T16:41:31.753878: step 3790, loss 0.00288498, acc 1\n",
      "2019-06-29T16:41:32.024070: step 3791, loss 0.0113776, acc 1\n",
      "2019-06-29T16:41:32.293262: step 3792, loss 0.00239469, acc 1\n",
      "2019-06-29T16:41:32.558451: step 3793, loss 0.00341448, acc 1\n",
      "2019-06-29T16:41:32.836649: step 3794, loss 0.0345756, acc 0.984375\n",
      "2019-06-29T16:41:33.102838: step 3795, loss 0.0307625, acc 0.984375\n",
      "2019-06-29T16:41:33.370029: step 3796, loss 0.0054718, acc 1\n",
      "2019-06-29T16:41:33.641222: step 3797, loss 0.00123601, acc 1\n",
      "2019-06-29T16:41:33.919420: step 3798, loss 0.0046017, acc 1\n",
      "2019-06-29T16:41:34.188611: step 3799, loss 0.00228239, acc 1\n",
      "2019-06-29T16:41:34.452799: step 3800, loss 0.0103697, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-06-29T16:41:35.129282: step 3800, loss 1.20033, acc 0.724203\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\mao\\Desktop\\Tensorflow\\CNN\\1\\cnn-text-classification-tf-master\\runs\\1561796526\\checkpoints\\model-3800\n",
      "\n",
      "2019-06-29T16:41:36.237996: step 3801, loss 0.00112217, acc 1\n",
      "2019-06-29T16:41:36.743356: step 3802, loss 0.00157561, acc 1\n",
      "2019-06-29T16:41:37.024556: step 3803, loss 0.00118725, acc 1\n",
      "2019-06-29T16:41:37.290746: step 3804, loss 0.00273246, acc 1\n",
      "2019-06-29T16:41:37.560938: step 3805, loss 0.00225682, acc 1\n",
      "2019-06-29T16:41:37.832485: step 3806, loss 0.00140703, acc 1\n",
      "2019-06-29T16:41:38.106680: step 3807, loss 0.00505436, acc 1\n",
      "2019-06-29T16:41:38.369867: step 3808, loss 0.00610502, acc 1\n",
      "2019-06-29T16:41:38.671083: step 3809, loss 0.0335098, acc 0.984375\n",
      "2019-06-29T16:41:38.966293: step 3810, loss 0.00371394, acc 1\n",
      "2019-06-29T16:41:39.274511: step 3811, loss 0.00100444, acc 1\n",
      "2019-06-29T16:41:39.606623: step 3812, loss 0.00152801, acc 1\n",
      "2019-06-29T16:41:39.889825: step 3813, loss 0.0251115, acc 0.984375\n",
      "2019-06-29T16:41:40.201048: step 3814, loss 0.00564572, acc 1\n",
      "2019-06-29T16:41:40.505264: step 3815, loss 0.00221002, acc 1\n",
      "2019-06-29T16:41:40.783463: step 3816, loss 0.00263741, acc 1\n",
      "2019-06-29T16:41:41.068666: step 3817, loss 0.00611797, acc 1\n",
      "2019-06-29T16:41:41.346863: step 3818, loss 0.00359318, acc 1\n",
      "2019-06-29T16:41:41.643073: step 3819, loss 0.0151932, acc 1\n",
      "2019-06-29T16:41:41.920271: step 3820, loss 0.00153252, acc 1\n",
      "2019-06-29T16:41:42.204473: step 3821, loss 0.00469824, acc 1\n",
      "2019-06-29T16:41:42.480671: step 3822, loss 0.00451147, acc 1\n",
      "2019-06-29T16:41:42.771878: step 3823, loss 0.0514143, acc 0.984375\n",
      "2019-06-29T16:41:43.059082: step 3824, loss 0.00874103, acc 1\n",
      "2019-06-29T16:41:43.348288: step 3825, loss 0.0488311, acc 0.984375\n",
      "2019-06-29T16:41:43.648502: step 3826, loss 0.00180588, acc 1\n",
      "2019-06-29T16:41:43.928701: step 3827, loss 0.07884, acc 0.96875\n",
      "2019-06-29T16:41:44.220909: step 3828, loss 0.00153681, acc 1\n",
      "2019-06-29T16:41:44.510115: step 3829, loss 0.0063872, acc 1\n",
      "2019-06-29T16:41:44.805325: step 3830, loss 0.00382658, acc 1\n",
      "2019-06-29T16:41:45.092529: step 3831, loss 0.00294545, acc 1\n",
      "2019-06-29T16:41:45.375731: step 3832, loss 0.00231238, acc 1\n",
      "2019-06-29T16:41:45.672942: step 3833, loss 0.00786515, acc 1\n",
      "2019-06-29T16:41:45.987167: step 3834, loss 0.00214734, acc 1\n",
      "2019-06-29T16:41:46.269367: step 3835, loss 0.00187943, acc 1\n",
      "2019-06-29T16:41:46.553569: step 3836, loss 0.00452801, acc 1\n",
      "2019-06-29T16:41:46.824763: step 3837, loss 0.00216214, acc 1\n",
      "2019-06-29T16:41:47.106963: step 3838, loss 0.00414544, acc 1\n",
      "2019-06-29T16:41:47.376155: step 3839, loss 0.00468381, acc 1\n",
      "2019-06-29T16:41:47.657355: step 3840, loss 0.00721818, acc 1\n",
      "2019-06-29T16:41:47.941558: step 3841, loss 0.00432505, acc 1\n",
      "2019-06-29T16:41:48.220757: step 3842, loss 0.018279, acc 0.984375\n",
      "2019-06-29T16:41:48.489949: step 3843, loss 0.00478161, acc 1\n",
      "2019-06-29T16:41:48.765145: step 3844, loss 0.00269493, acc 1\n",
      "2019-06-29T16:41:49.030333: step 3845, loss 0.0175613, acc 1\n",
      "2019-06-29T16:41:49.314536: step 3846, loss 0.0315285, acc 0.984375\n",
      "2019-06-29T16:41:49.587730: step 3847, loss 0.00124335, acc 1\n",
      "2019-06-29T16:41:49.849917: step 3848, loss 0.012896, acc 1\n",
      "2019-06-29T16:41:50.121109: step 3849, loss 0.00187285, acc 1\n",
      "2019-06-29T16:41:50.409315: step 3850, loss 0.0022241, acc 1\n",
      "2019-06-29T16:41:50.684511: step 3851, loss 0.0055067, acc 1\n",
      "2019-06-29T16:41:50.946698: step 3852, loss 0.00344302, acc 1\n",
      "2019-06-29T16:41:51.205882: step 3853, loss 0.00103411, acc 1\n",
      "2019-06-29T16:41:51.493087: step 3854, loss 0.00196275, acc 1\n",
      "2019-06-29T16:41:51.757275: step 3855, loss 0.0403699, acc 0.96875\n",
      "2019-06-29T16:41:52.026466: step 3856, loss 0.0292881, acc 0.984375\n",
      "2019-06-29T16:41:52.297659: step 3857, loss 0.00405305, acc 1\n",
      "2019-06-29T16:41:52.575857: step 3858, loss 0.00415145, acc 1\n",
      "2019-06-29T16:41:52.842047: step 3859, loss 0.000957869, acc 1\n",
      "2019-06-29T16:41:53.110238: step 3860, loss 0.00186326, acc 1\n",
      "2019-06-29T16:41:53.375426: step 3861, loss 0.00543942, acc 1\n",
      "2019-06-29T16:41:53.666635: step 3862, loss 0.000877426, acc 1\n",
      "2019-06-29T16:41:53.934984: step 3863, loss 0.01067, acc 1\n",
      "2019-06-29T16:41:54.205176: step 3864, loss 0.00240966, acc 1\n",
      "2019-06-29T16:41:54.476370: step 3865, loss 0.00124354, acc 1\n",
      "2019-06-29T16:41:54.800601: step 3866, loss 0.00197629, acc 1\n",
      "2019-06-29T16:41:55.095811: step 3867, loss 0.00415491, acc 1\n",
      "2019-06-29T16:41:55.393022: step 3868, loss 0.00338444, acc 1\n",
      "2019-06-29T16:41:55.671471: step 3869, loss 0.00258191, acc 1\n",
      "2019-06-29T16:41:55.977688: step 3870, loss 0.0130763, acc 0.984375\n",
      "2019-06-29T16:41:56.258888: step 3871, loss 0.00972395, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-29T16:41:56.545092: step 3872, loss 0.00466702, acc 1\n",
      "2019-06-29T16:41:56.829295: step 3873, loss 0.00451504, acc 1\n",
      "2019-06-29T16:41:57.128508: step 3874, loss 0.0310061, acc 0.984375\n",
      "2019-06-29T16:41:57.410709: step 3875, loss 0.00852509, acc 1\n",
      "2019-06-29T16:41:57.696912: step 3876, loss 0.0420798, acc 0.984375\n",
      "2019-06-29T16:41:57.980114: step 3877, loss 0.0019676, acc 1\n",
      "2019-06-29T16:41:58.267319: step 3878, loss 0.00111441, acc 1\n",
      "2019-06-29T16:41:58.548519: step 3879, loss 0.00677224, acc 1\n",
      "2019-06-29T16:41:58.831721: step 3880, loss 0.00969407, acc 1\n",
      "2019-06-29T16:41:59.117925: step 3881, loss 0.0117361, acc 1\n",
      "2019-06-29T16:41:59.411133: step 3882, loss 0.0113912, acc 1\n",
      "2019-06-29T16:41:59.707344: step 3883, loss 0.00723008, acc 1\n",
      "2019-06-29T16:41:59.992547: step 3884, loss 0.00089314, acc 1\n",
      "2019-06-29T16:42:00.283285: step 3885, loss 0.0016729, acc 1\n",
      "2019-06-29T16:42:00.572490: step 3886, loss 0.0108487, acc 1\n",
      "2019-06-29T16:42:00.857692: step 3887, loss 0.00665007, acc 1\n",
      "2019-06-29T16:42:01.142896: step 3888, loss 0.00385408, acc 1\n",
      "2019-06-29T16:42:01.431101: step 3889, loss 0.0107401, acc 1\n",
      "2019-06-29T16:42:01.710300: step 3890, loss 0.0255686, acc 0.984375\n",
      "2019-06-29T16:42:01.992501: step 3891, loss 0.00334366, acc 1\n",
      "2019-06-29T16:42:02.282708: step 3892, loss 0.0625353, acc 0.96875\n",
      "2019-06-29T16:42:02.587925: step 3893, loss 0.00130232, acc 1\n",
      "2019-06-29T16:42:02.859117: step 3894, loss 0.00237364, acc 1\n",
      "2019-06-29T16:42:03.132313: step 3895, loss 0.0031664, acc 1\n",
      "2019-06-29T16:42:03.396500: step 3896, loss 0.00893034, acc 1\n",
      "2019-06-29T16:42:03.709723: step 3897, loss 0.00337332, acc 1\n",
      "2019-06-29T16:42:03.976913: step 3898, loss 0.00934098, acc 1\n",
      "2019-06-29T16:42:04.247105: step 3899, loss 0.00738191, acc 1\n",
      "2019-06-29T16:42:04.515297: step 3900, loss 0.00251003, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-06-29T16:42:05.162759: step 3900, loss 1.20681, acc 0.723265\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\mao\\Desktop\\Tensorflow\\CNN\\1\\cnn-text-classification-tf-master\\runs\\1561796526\\checkpoints\\model-3900\n",
      "\n",
      "2019-06-29T16:42:06.299648: step 3901, loss 0.00343805, acc 1\n",
      "2019-06-29T16:42:06.814013: step 3902, loss 0.00261234, acc 1\n",
      "2019-06-29T16:42:07.081203: step 3903, loss 0.00143148, acc 1\n",
      "2019-06-29T16:42:07.349394: step 3904, loss 0.00399173, acc 1\n",
      "2019-06-29T16:42:07.621588: step 3905, loss 0.00193987, acc 1\n",
      "2019-06-29T16:42:07.894783: step 3906, loss 0.0038552, acc 1\n",
      "2019-06-29T16:42:08.166976: step 3907, loss 0.00624484, acc 1\n",
      "2019-06-29T16:42:08.446175: step 3908, loss 0.00626852, acc 1\n",
      "2019-06-29T16:42:08.719370: step 3909, loss 0.00360515, acc 1\n",
      "2019-06-29T16:42:08.993565: step 3910, loss 0.000716951, acc 1\n",
      "2019-06-29T16:42:09.267760: step 3911, loss 0.00378928, acc 1\n",
      "2019-06-29T16:42:09.541955: step 3912, loss 0.00987323, acc 1\n",
      "2019-06-29T16:42:09.809059: step 3913, loss 0.00155198, acc 1\n",
      "2019-06-29T16:42:10.080252: step 3914, loss 0.00405588, acc 1\n",
      "2019-06-29T16:42:10.351445: step 3915, loss 0.013003, acc 1\n",
      "2019-06-29T16:42:10.655662: step 3916, loss 0.0145357, acc 1\n",
      "2019-06-29T16:42:10.951873: step 3917, loss 0.00565024, acc 1\n",
      "2019-06-29T16:42:11.275104: step 3918, loss 0.00645425, acc 1\n",
      "2019-06-29T16:42:11.565953: step 3919, loss 0.00887123, acc 1\n",
      "2019-06-29T16:42:11.850155: step 3920, loss 0.00518175, acc 1\n",
      "2019-06-29T16:42:12.134357: step 3921, loss 0.00217986, acc 1\n",
      "2019-06-29T16:42:12.442577: step 3922, loss 0.00177628, acc 1\n",
      "2019-06-29T16:42:12.724778: step 3923, loss 0.0135387, acc 1\n",
      "2019-06-29T16:42:13.009981: step 3924, loss 0.00212575, acc 1\n",
      "2019-06-29T16:42:13.296184: step 3925, loss 0.00521946, acc 1\n",
      "2019-06-29T16:42:13.587392: step 3926, loss 0.00355559, acc 1\n",
      "2019-06-29T16:42:13.874597: step 3927, loss 0.00448224, acc 1\n",
      "2019-06-29T16:42:14.164803: step 3928, loss 0.0119244, acc 1\n",
      "2019-06-29T16:42:14.452008: step 3929, loss 0.00530988, acc 1\n",
      "2019-06-29T16:42:14.730205: step 3930, loss 0.00955487, acc 1\n",
      "2019-06-29T16:42:15.050433: step 3931, loss 0.00135461, acc 1\n",
      "2019-06-29T16:42:15.342642: step 3932, loss 0.00687695, acc 1\n",
      "2019-06-29T16:42:15.665872: step 3933, loss 0.0037535, acc 1\n",
      "2019-06-29T16:42:15.958080: step 3934, loss 0.0229337, acc 0.984375\n",
      "2019-06-29T16:42:16.238280: step 3935, loss 0.0109954, acc 1\n",
      "2019-06-29T16:42:16.524483: step 3936, loss 0.000312354, acc 1\n",
      "2019-06-29T16:42:16.836705: step 3937, loss 0.00481848, acc 1\n",
      "2019-06-29T16:42:17.116906: step 3938, loss 0.00328093, acc 1\n",
      "2019-06-29T16:42:17.400106: step 3939, loss 0.00161684, acc 1\n",
      "2019-06-29T16:42:17.692314: step 3940, loss 0.00377142, acc 1\n",
      "2019-06-29T16:42:17.998532: step 3941, loss 0.0129409, acc 1\n",
      "2019-06-29T16:42:18.281734: step 3942, loss 0.00597131, acc 1\n",
      "2019-06-29T16:42:18.566938: step 3943, loss 0.00788875, acc 1\n",
      "2019-06-29T16:42:18.835128: step 3944, loss 0.00596191, acc 1\n",
      "2019-06-29T16:42:19.111325: step 3945, loss 0.0145539, acc 1\n",
      "2019-06-29T16:42:19.374512: step 3946, loss 0.00379696, acc 1\n",
      "2019-06-29T16:42:19.671724: step 3947, loss 0.000585095, acc 1\n",
      "2019-06-29T16:42:19.942917: step 3948, loss 0.00240944, acc 1\n",
      "2019-06-29T16:42:20.222115: step 3949, loss 0.00300442, acc 1\n",
      "2019-06-29T16:42:20.490307: step 3950, loss 0.004088, acc 1\n",
      "2019-06-29T16:42:20.759499: step 3951, loss 0.00110342, acc 1\n",
      "2019-06-29T16:42:21.022685: step 3952, loss 0.00176128, acc 1\n",
      "2019-06-29T16:42:21.303886: step 3953, loss 0.00204949, acc 1\n",
      "2019-06-29T16:42:21.575079: step 3954, loss 0.00116541, acc 1\n",
      "2019-06-29T16:42:21.843270: step 3955, loss 0.0121774, acc 1\n",
      "2019-06-29T16:42:22.108458: step 3956, loss 0.0045673, acc 1\n",
      "2019-06-29T16:42:22.400667: step 3957, loss 0.00206317, acc 1\n",
      "2019-06-29T16:42:22.674862: step 3958, loss 0.00404533, acc 1\n",
      "2019-06-29T16:42:22.944053: step 3959, loss 0.00301345, acc 1\n",
      "2019-06-29T16:42:23.207241: step 3960, loss 0.000377606, acc 1\n",
      "2019-06-29T16:42:23.488442: step 3961, loss 0.00460562, acc 1\n",
      "2019-06-29T16:42:23.749627: step 3962, loss 0.000758904, acc 1\n",
      "2019-06-29T16:42:24.013815: step 3963, loss 0.00879673, acc 1\n",
      "2019-06-29T16:42:24.283007: step 3964, loss 0.00759025, acc 1\n",
      "2019-06-29T16:42:24.565208: step 3965, loss 0.00548261, acc 1\n",
      "2019-06-29T16:42:24.833399: step 3966, loss 0.0516814, acc 0.984375\n",
      "2019-06-29T16:42:25.105592: step 3967, loss 0.00166262, acc 1\n",
      "2019-06-29T16:42:25.377786: step 3968, loss 0.00148766, acc 1\n",
      "2019-06-29T16:42:25.671996: step 3969, loss 0.00468612, acc 1\n",
      "2019-06-29T16:42:25.943503: step 3970, loss 0.00474123, acc 1\n",
      "2019-06-29T16:42:26.228706: step 3971, loss 0.00137309, acc 1\n",
      "2019-06-29T16:42:26.500900: step 3972, loss 0.00353118, acc 1\n",
      "2019-06-29T16:42:26.815124: step 3973, loss 0.00835867, acc 1\n",
      "2019-06-29T16:42:27.108334: step 3974, loss 0.00113425, acc 1\n",
      "2019-06-29T16:42:27.414552: step 3975, loss 0.00261021, acc 1\n",
      "2019-06-29T16:42:27.685381: step 3976, loss 0.000967345, acc 1\n",
      "2019-06-29T16:42:27.991599: step 3977, loss 0.00232795, acc 1\n",
      "2019-06-29T16:42:28.265795: step 3978, loss 0.0041189, acc 1\n",
      "2019-06-29T16:42:28.548996: step 3979, loss 0.00307696, acc 1\n",
      "2019-06-29T16:42:28.833198: step 3980, loss 0.0131504, acc 0.984375\n",
      "2019-06-29T16:42:29.135413: step 3981, loss 0.00338416, acc 1\n",
      "2019-06-29T16:42:29.418615: step 3982, loss 0.00170196, acc 1\n",
      "2019-06-29T16:42:29.699815: step 3983, loss 0.0142417, acc 1\n",
      "2019-06-29T16:42:29.979014: step 3984, loss 0.00700708, acc 1\n",
      "2019-06-29T16:42:30.278227: step 3985, loss 0.0232025, acc 0.984375\n",
      "2019-06-29T16:42:30.562430: step 3986, loss 0.00372315, acc 1\n",
      "2019-06-29T16:42:30.839626: step 3987, loss 0.00187739, acc 1\n",
      "2019-06-29T16:42:31.123829: step 3988, loss 0.00163887, acc 1\n",
      "2019-06-29T16:42:31.434052: step 3989, loss 0.0109952, acc 1\n",
      "2019-06-29T16:42:31.727258: step 3990, loss 0.00292827, acc 1\n",
      "2019-06-29T16:42:32.014463: step 3991, loss 0.00182979, acc 1\n",
      "2019-06-29T16:42:32.303670: step 3992, loss 0.00248398, acc 1\n",
      "2019-06-29T16:42:32.587871: step 3993, loss 0.00733674, acc 1\n",
      "2019-06-29T16:42:32.867070: step 3994, loss 0.00402969, acc 1\n",
      "2019-06-29T16:42:33.152273: step 3995, loss 0.0131694, acc 0.984375\n",
      "2019-06-29T16:42:33.436476: step 3996, loss 0.00278929, acc 1\n",
      "2019-06-29T16:42:33.724681: step 3997, loss 0.00401308, acc 1\n",
      "2019-06-29T16:42:34.007882: step 3998, loss 0.00969232, acc 1\n",
      "2019-06-29T16:42:34.291084: step 3999, loss 0.00180036, acc 1\n",
      "2019-06-29T16:42:34.600304: step 4000, loss 0.00135177, acc 1\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-29T16:42:35.341836: step 4000, loss 1.27148, acc 0.722326\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\mao\\Desktop\\Tensorflow\\CNN\\1\\cnn-text-classification-tf-master\\runs\\1561796526\\checkpoints\\model-4000\n",
      "\n",
      "2019-06-29T16:42:36.432610: step 4001, loss 0.00121615, acc 1\n",
      "2019-06-29T16:42:36.970992: step 4002, loss 0.00188656, acc 1\n",
      "2019-06-29T16:42:37.245187: step 4003, loss 0.00339027, acc 1\n",
      "2019-06-29T16:42:37.511377: step 4004, loss 0.0043059, acc 1\n",
      "2019-06-29T16:42:37.776566: step 4005, loss 0.00608671, acc 1\n",
      "2019-06-29T16:42:38.057766: step 4006, loss 0.00385668, acc 1\n",
      "2019-06-29T16:42:38.320954: step 4007, loss 0.00117497, acc 1\n",
      "2019-06-29T16:42:38.591146: step 4008, loss 0.0515545, acc 0.96875\n",
      "2019-06-29T16:42:38.859337: step 4009, loss 0.0204826, acc 0.984375\n",
      "2019-06-29T16:42:39.145540: step 4010, loss 0.00384494, acc 1\n",
      "2019-06-29T16:42:39.413732: step 4011, loss 0.00111465, acc 1\n",
      "2019-06-29T16:42:39.680922: step 4012, loss 0.00611243, acc 1\n",
      "2019-06-29T16:42:39.950113: step 4013, loss 0.000899893, acc 1\n",
      "2019-06-29T16:42:40.232314: step 4014, loss 0.0228334, acc 0.984375\n",
      "2019-06-29T16:42:40.499504: step 4015, loss 0.00456274, acc 1\n",
      "2019-06-29T16:42:40.763693: step 4016, loss 0.00189485, acc 1\n",
      "2019-06-29T16:42:41.026880: step 4017, loss 0.000760344, acc 1\n",
      "2019-06-29T16:42:41.299074: step 4018, loss 0.00387718, acc 1\n",
      "2019-06-29T16:42:41.564263: step 4019, loss 0.00313561, acc 1\n",
      "2019-06-29T16:42:41.838098: step 4020, loss 0.0076816, acc 1\n",
      "2019-06-29T16:42:42.099284: step 4021, loss 0.0137166, acc 1\n",
      "2019-06-29T16:42:42.373479: step 4022, loss 0.00104524, acc 1\n",
      "2019-06-29T16:42:42.680699: step 4023, loss 0.00348441, acc 1\n",
      "2019-06-29T16:42:42.973908: step 4024, loss 0.000706234, acc 1\n",
      "2019-06-29T16:42:43.267117: step 4025, loss 0.00058485, acc 1\n",
      "2019-06-29T16:42:43.585320: step 4026, loss 0.00406172, acc 1\n",
      "2019-06-29T16:42:43.870524: step 4027, loss 0.00333989, acc 1\n",
      "2019-06-29T16:42:44.148721: step 4028, loss 0.00696356, acc 1\n",
      "2019-06-29T16:42:44.433924: step 4029, loss 0.00249498, acc 1\n",
      "2019-06-29T16:42:44.795182: step 4030, loss 0.0104052, acc 1\n",
      "2019-06-29T16:42:45.075381: step 4031, loss 0.0172558, acc 0.984375\n",
      "2019-06-29T16:42:45.363587: step 4032, loss 0.00867875, acc 1\n",
      "2019-06-29T16:42:45.666803: step 4033, loss 0.024245, acc 0.984375\n",
      "2019-06-29T16:42:45.949004: step 4034, loss 0.00260061, acc 1\n",
      "2019-06-29T16:42:46.253221: step 4035, loss 0.00254193, acc 1\n",
      "2019-06-29T16:42:46.549431: step 4036, loss 0.00421436, acc 1\n",
      "2019-06-29T16:42:46.850645: step 4037, loss 0.00101673, acc 1\n",
      "2019-06-29T16:42:47.135848: step 4038, loss 0.00601813, acc 1\n",
      "2019-06-29T16:42:47.478093: step 4039, loss 0.0039225, acc 1\n",
      "2019-06-29T16:42:47.778306: step 4040, loss 0.0252723, acc 1\n",
      "2019-06-29T16:42:48.085524: step 4041, loss 0.00980572, acc 1\n",
      "2019-06-29T16:42:48.376732: step 4042, loss 0.00791715, acc 1\n",
      "2019-06-29T16:42:48.666938: step 4043, loss 0.0121962, acc 0.984375\n",
      "2019-06-29T16:42:48.957146: step 4044, loss 0.0432413, acc 0.984375\n",
      "2019-06-29T16:42:49.242348: step 4045, loss 0.00354193, acc 1\n",
      "2019-06-29T16:42:49.533555: step 4046, loss 0.0165343, acc 0.984375\n",
      "2019-06-29T16:42:49.820760: step 4047, loss 0.00285597, acc 1\n",
      "2019-06-29T16:42:50.106964: step 4048, loss 0.00566279, acc 1\n",
      "2019-06-29T16:42:50.387163: step 4049, loss 0.00105019, acc 1\n",
      "2019-06-29T16:42:50.671365: step 4050, loss 0.0174426, acc 0.983333\n",
      "2019-06-29T16:42:50.940557: step 4051, loss 0.0228069, acc 0.984375\n",
      "2019-06-29T16:42:51.237768: step 4052, loss 0.0012312, acc 1\n",
      "2019-06-29T16:42:51.515967: step 4053, loss 0.00530298, acc 1\n",
      "2019-06-29T16:42:51.786159: step 4054, loss 0.000979291, acc 1\n",
      "2019-06-29T16:42:52.056351: step 4055, loss 0.00505723, acc 1\n",
      "2019-06-29T16:42:52.332548: step 4056, loss 0.0267416, acc 0.984375\n",
      "2019-06-29T16:42:52.602740: step 4057, loss 0.000599314, acc 1\n",
      "2019-06-29T16:42:52.866928: step 4058, loss 0.00201099, acc 1\n",
      "2019-06-29T16:42:53.137121: step 4059, loss 0.00660913, acc 1\n",
      "2019-06-29T16:42:53.409315: step 4060, loss 0.00426385, acc 1\n",
      "2019-06-29T16:42:53.682509: step 4061, loss 0.000852586, acc 1\n",
      "2019-06-29T16:42:53.952702: step 4062, loss 0.00274706, acc 1\n",
      "2019-06-29T16:42:54.223895: step 4063, loss 0.0086796, acc 1\n",
      "2019-06-29T16:42:54.501092: step 4064, loss 0.00175405, acc 1\n",
      "2019-06-29T16:42:54.764280: step 4065, loss 0.00416139, acc 1\n",
      "2019-06-29T16:42:55.028469: step 4066, loss 0.0427343, acc 0.984375\n",
      "2019-06-29T16:42:55.302663: step 4067, loss 0.00146079, acc 1\n",
      "2019-06-29T16:42:55.574856: step 4068, loss 0.00195149, acc 1\n",
      "2019-06-29T16:42:55.843047: step 4069, loss 0.00485195, acc 1\n",
      "2019-06-29T16:42:56.112239: step 4070, loss 0.00988751, acc 1\n",
      "2019-06-29T16:42:56.384433: step 4071, loss 0.00057088, acc 1\n",
      "2019-06-29T16:42:56.659629: step 4072, loss 0.00402834, acc 1\n",
      "2019-06-29T16:42:56.930822: step 4073, loss 0.0384532, acc 0.984375\n",
      "2019-06-29T16:42:57.202015: step 4074, loss 0.00179278, acc 1\n",
      "2019-06-29T16:42:57.476210: step 4075, loss 0.00739892, acc 1\n",
      "2019-06-29T16:42:57.750084: step 4076, loss 0.00266621, acc 1\n",
      "2019-06-29T16:42:58.015781: step 4077, loss 0.00302565, acc 1\n",
      "2019-06-29T16:42:58.284973: step 4078, loss 0.00129225, acc 1\n",
      "2019-06-29T16:42:58.578183: step 4079, loss 0.00164412, acc 1\n",
      "2019-06-29T16:42:58.882398: step 4080, loss 0.0411007, acc 0.984375\n",
      "2019-06-29T16:42:59.178610: step 4081, loss 0.00699779, acc 1\n",
      "2019-06-29T16:42:59.470428: step 4082, loss 0.00671612, acc 1\n",
      "2019-06-29T16:42:59.756031: step 4083, loss 0.00114258, acc 1\n",
      "2019-06-29T16:43:00.075275: step 4084, loss 0.00528567, acc 1\n",
      "2019-06-29T16:43:00.363481: step 4085, loss 0.00235565, acc 1\n",
      "2019-06-29T16:43:00.664695: step 4086, loss 0.0124602, acc 1\n",
      "2019-06-29T16:43:00.949898: step 4087, loss 0.000219452, acc 1\n",
      "2019-06-29T16:43:01.238102: step 4088, loss 0.067141, acc 0.984375\n",
      "2019-06-29T16:43:01.521305: step 4089, loss 0.00665138, acc 1\n",
      "2019-06-29T16:43:01.807508: step 4090, loss 0.0027191, acc 1\n",
      "2019-06-29T16:43:02.085706: step 4091, loss 0.0135037, acc 1\n",
      "2019-06-29T16:43:02.375913: step 4092, loss 0.00174913, acc 1\n",
      "2019-06-29T16:43:02.667120: step 4093, loss 0.0174235, acc 0.984375\n",
      "2019-06-29T16:43:02.947321: step 4094, loss 0.00271483, acc 1\n",
      "2019-06-29T16:43:03.289564: step 4095, loss 0.00325583, acc 1\n",
      "2019-06-29T16:43:03.585774: step 4096, loss 0.00222601, acc 1\n",
      "2019-06-29T16:43:03.880985: step 4097, loss 0.0011954, acc 1\n",
      "2019-06-29T16:43:04.189204: step 4098, loss 0.00507943, acc 1\n",
      "2019-06-29T16:43:04.518438: step 4099, loss 0.00627443, acc 1\n",
      "2019-06-29T16:43:04.812648: step 4100, loss 0.000541891, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-06-29T16:43:05.564186: step 4100, loss 1.26654, acc 0.728893\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\mao\\Desktop\\Tensorflow\\CNN\\1\\cnn-text-classification-tf-master\\runs\\1561796526\\checkpoints\\model-4100\n",
      "\n",
      "2019-06-29T16:43:06.781051: step 4101, loss 0.00286586, acc 1\n",
      "2019-06-29T16:43:07.277403: step 4102, loss 0.00178454, acc 1\n",
      "2019-06-29T16:43:07.556601: step 4103, loss 0.000894351, acc 1\n",
      "2019-06-29T16:43:07.834800: step 4104, loss 0.0294228, acc 0.984375\n",
      "2019-06-29T16:43:08.102990: step 4105, loss 0.00728699, acc 1\n",
      "2019-06-29T16:43:08.373183: step 4106, loss 0.000994003, acc 1\n",
      "2019-06-29T16:43:08.651381: step 4107, loss 0.00800098, acc 1\n",
      "2019-06-29T16:43:08.934583: step 4108, loss 0.00436518, acc 1\n",
      "2019-06-29T16:43:09.197770: step 4109, loss 0.00789576, acc 1\n",
      "2019-06-29T16:43:09.466962: step 4110, loss 0.00426376, acc 1\n",
      "2019-06-29T16:43:09.741157: step 4111, loss 0.00338286, acc 1\n",
      "2019-06-29T16:43:10.028361: step 4112, loss 0.0125736, acc 1\n",
      "2019-06-29T16:43:10.298554: step 4113, loss 0.00322496, acc 1\n",
      "2019-06-29T16:43:10.569747: step 4114, loss 0.00655659, acc 1\n",
      "2019-06-29T16:43:10.844944: step 4115, loss 0.025413, acc 0.984375\n",
      "2019-06-29T16:43:11.163171: step 4116, loss 0.00053798, acc 1\n",
      "2019-06-29T16:43:11.467386: step 4117, loss 0.00202149, acc 1\n",
      "2019-06-29T16:43:11.730574: step 4118, loss 0.00172096, acc 1\n",
      "2019-06-29T16:43:11.996763: step 4119, loss 0.00739591, acc 1\n",
      "2019-06-29T16:43:12.277963: step 4120, loss 0.00870531, acc 1\n",
      "2019-06-29T16:43:12.546155: step 4121, loss 0.0272915, acc 0.984375\n",
      "2019-06-29T16:43:12.824352: step 4122, loss 0.0630913, acc 0.984375\n",
      "2019-06-29T16:43:13.131574: step 4123, loss 0.00330182, acc 1\n",
      "2019-06-29T16:43:13.415773: step 4124, loss 0.00427872, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-29T16:43:13.705980: step 4125, loss 0.0054975, acc 1\n",
      "2019-06-29T16:43:13.991234: step 4126, loss 0.00459823, acc 1\n",
      "2019-06-29T16:43:14.263430: step 4127, loss 0.00407853, acc 1\n",
      "2019-06-29T16:43:14.551633: step 4128, loss 0.00913581, acc 1\n",
      "2019-06-29T16:43:14.849846: step 4129, loss 0.00157687, acc 1\n",
      "2019-06-29T16:43:15.151061: step 4130, loss 0.00119147, acc 1\n",
      "2019-06-29T16:43:15.450273: step 4131, loss 0.017586, acc 0.984375\n",
      "2019-06-29T16:43:15.726871: step 4132, loss 0.00144997, acc 1\n",
      "2019-06-29T16:43:16.015076: step 4133, loss 0.0106403, acc 1\n",
      "2019-06-29T16:43:16.299277: step 4134, loss 0.0198985, acc 1\n",
      "2019-06-29T16:43:16.600491: step 4135, loss 0.000897891, acc 1\n",
      "2019-06-29T16:43:16.887696: step 4136, loss 0.0309875, acc 0.984375\n",
      "2019-06-29T16:43:17.172899: step 4137, loss 0.0021771, acc 1\n",
      "2019-06-29T16:43:17.457101: step 4138, loss 0.0478239, acc 0.984375\n",
      "2019-06-29T16:43:17.789339: step 4139, loss 0.00140158, acc 1\n",
      "2019-06-29T16:43:18.073540: step 4140, loss 0.000758136, acc 1\n",
      "2019-06-29T16:43:18.373754: step 4141, loss 0.000581749, acc 1\n",
      "2019-06-29T16:43:18.740015: step 4142, loss 0.00561911, acc 1\n",
      "2019-06-29T16:43:19.020214: step 4143, loss 0.00187368, acc 1\n",
      "2019-06-29T16:43:19.304417: step 4144, loss 0.0229839, acc 0.984375\n",
      "2019-06-29T16:43:19.680685: step 4145, loss 0.000690118, acc 1\n",
      "2019-06-29T16:43:19.990905: step 4146, loss 0.00486584, acc 1\n",
      "2019-06-29T16:43:20.310133: step 4147, loss 0.00888662, acc 1\n",
      "2019-06-29T16:43:20.597337: step 4148, loss 0.00116912, acc 1\n",
      "2019-06-29T16:43:20.880540: step 4149, loss 0.00363112, acc 1\n",
      "2019-06-29T16:43:21.167743: step 4150, loss 0.00330388, acc 1\n",
      "2019-06-29T16:43:21.451945: step 4151, loss 0.00529983, acc 1\n",
      "2019-06-29T16:43:21.817206: step 4152, loss 0.0050051, acc 1\n",
      "2019-06-29T16:43:22.200479: step 4153, loss 0.0119998, acc 1\n",
      "2019-06-29T16:43:22.491687: step 4154, loss 0.00419446, acc 1\n",
      "2019-06-29T16:43:22.813918: step 4155, loss 0.0152443, acc 1\n",
      "2019-06-29T16:43:23.090113: step 4156, loss 0.00636314, acc 1\n",
      "2019-06-29T16:43:23.366309: step 4157, loss 0.0605755, acc 0.984375\n",
      "2019-06-29T16:43:23.647509: step 4158, loss 0.00985339, acc 1\n",
      "2019-06-29T16:43:23.911697: step 4159, loss 0.0429134, acc 0.984375\n",
      "2019-06-29T16:43:24.184892: step 4160, loss 0.00367594, acc 1\n",
      "2019-06-29T16:43:24.460087: step 4161, loss 0.0705539, acc 0.96875\n",
      "2019-06-29T16:43:24.732281: step 4162, loss 0.00106688, acc 1\n",
      "2019-06-29T16:43:24.996469: step 4163, loss 0.0078817, acc 1\n",
      "2019-06-29T16:43:25.263660: step 4164, loss 0.00508043, acc 1\n",
      "2019-06-29T16:43:25.536854: step 4165, loss 0.0125078, acc 1\n",
      "2019-06-29T16:43:25.805045: step 4166, loss 0.00356279, acc 1\n",
      "2019-06-29T16:43:26.072235: step 4167, loss 0.00625039, acc 1\n",
      "2019-06-29T16:43:26.342428: step 4168, loss 0.00402973, acc 1\n",
      "2019-06-29T16:43:26.616623: step 4169, loss 0.00170226, acc 1\n",
      "2019-06-29T16:43:26.886815: step 4170, loss 0.00149488, acc 1\n",
      "2019-06-29T16:43:27.153004: step 4171, loss 0.0349102, acc 0.984375\n",
      "2019-06-29T16:43:27.423198: step 4172, loss 0.00543574, acc 1\n",
      "2019-06-29T16:43:27.689387: step 4173, loss 0.0591565, acc 0.984375\n",
      "2019-06-29T16:43:27.951573: step 4174, loss 0.0138024, acc 0.984375\n",
      "2019-06-29T16:43:28.221766: step 4175, loss 0.00534076, acc 1\n",
      "2019-06-29T16:43:28.493960: step 4176, loss 0.00342486, acc 1\n",
      "2019-06-29T16:43:28.759148: step 4177, loss 0.00960546, acc 1\n",
      "2019-06-29T16:43:29.023336: step 4178, loss 0.00766654, acc 1\n",
      "2019-06-29T16:43:29.287524: step 4179, loss 0.00305615, acc 1\n",
      "2019-06-29T16:43:29.558718: step 4180, loss 0.00108455, acc 1\n",
      "2019-06-29T16:43:29.831912: step 4181, loss 0.0115233, acc 1\n",
      "2019-06-29T16:43:30.097224: step 4182, loss 0.00333636, acc 1\n",
      "2019-06-29T16:43:30.365415: step 4183, loss 0.00225551, acc 1\n",
      "2019-06-29T16:43:30.671633: step 4184, loss 0.0317774, acc 0.984375\n",
      "2019-06-29T16:43:30.973849: step 4185, loss 0.00423594, acc 1\n",
      "2019-06-29T16:43:31.265056: step 4186, loss 0.00258312, acc 1\n",
      "2019-06-29T16:43:31.556873: step 4187, loss 0.00388533, acc 1\n",
      "2019-06-29T16:43:31.846079: step 4188, loss 0.00331833, acc 1\n",
      "2019-06-29T16:43:32.123276: step 4189, loss 0.00612429, acc 1\n",
      "2019-06-29T16:43:32.408479: step 4190, loss 0.00307151, acc 1\n",
      "2019-06-29T16:43:32.702689: step 4191, loss 0.0143654, acc 1\n",
      "2019-06-29T16:43:33.007906: step 4192, loss 0.001541, acc 1\n",
      "2019-06-29T16:43:33.290107: step 4193, loss 0.00284299, acc 1\n",
      "2019-06-29T16:43:33.578313: step 4194, loss 0.00302315, acc 1\n",
      "2019-06-29T16:43:33.859512: step 4195, loss 0.00179953, acc 1\n",
      "2019-06-29T16:43:34.158725: step 4196, loss 0.00205936, acc 1\n",
      "2019-06-29T16:43:34.439926: step 4197, loss 0.00262101, acc 1\n",
      "2019-06-29T16:43:34.733134: step 4198, loss 0.00197771, acc 1\n",
      "2019-06-29T16:43:35.027344: step 4199, loss 0.00307584, acc 1\n",
      "2019-06-29T16:43:35.330560: step 4200, loss 0.00202961, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-06-29T16:43:36.157152: step 4200, loss 1.28283, acc 0.728893\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\mao\\Desktop\\Tensorflow\\CNN\\1\\cnn-text-classification-tf-master\\runs\\1561796526\\checkpoints\\model-4200\n",
      "\n",
      "2019-06-29T16:43:37.960432: step 4201, loss 0.0231647, acc 0.984375\n",
      "2019-06-29T16:43:38.251640: step 4202, loss 0.00324543, acc 1\n",
      "2019-06-29T16:43:38.531839: step 4203, loss 0.00214576, acc 1\n",
      "2019-06-29T16:43:38.811037: step 4204, loss 0.0027199, acc 1\n",
      "2019-06-29T16:43:39.091237: step 4205, loss 0.00333708, acc 1\n",
      "2019-06-29T16:43:39.381444: step 4206, loss 0.00181877, acc 1\n",
      "2019-06-29T16:43:39.662644: step 4207, loss 0.0528342, acc 0.984375\n",
      "2019-06-29T16:43:39.934838: step 4208, loss 0.00130249, acc 1\n",
      "2019-06-29T16:43:40.215037: step 4209, loss 0.027895, acc 0.984375\n",
      "2019-06-29T16:43:40.497238: step 4210, loss 0.00228415, acc 1\n",
      "2019-06-29T16:43:40.774435: step 4211, loss 0.00795758, acc 1\n",
      "2019-06-29T16:43:41.066643: step 4212, loss 0.000575355, acc 1\n",
      "2019-06-29T16:43:41.339838: step 4213, loss 0.00214584, acc 1\n",
      "2019-06-29T16:43:41.617035: step 4214, loss 0.00236257, acc 1\n",
      "2019-06-29T16:43:41.905241: step 4215, loss 0.00542855, acc 1\n",
      "2019-06-29T16:43:42.230473: step 4216, loss 0.0102608, acc 1\n",
      "2019-06-29T16:43:42.512674: step 4217, loss 0.00168523, acc 1\n",
      "2019-06-29T16:43:42.801879: step 4218, loss 0.00440578, acc 1\n",
      "2019-06-29T16:43:43.112101: step 4219, loss 0.000522754, acc 1\n",
      "2019-06-29T16:43:43.387295: step 4220, loss 0.0021783, acc 1\n",
      "2019-06-29T16:43:43.665494: step 4221, loss 0.0018596, acc 1\n",
      "2019-06-29T16:43:43.939689: step 4222, loss 0.00663832, acc 1\n",
      "2019-06-29T16:43:44.227895: step 4223, loss 0.00809303, acc 1\n",
      "2019-06-29T16:43:44.493084: step 4224, loss 0.0021038, acc 1\n",
      "2019-06-29T16:43:44.767278: step 4225, loss 0.000442849, acc 1\n",
      "2019-06-29T16:43:45.042474: step 4226, loss 0.00649701, acc 1\n",
      "2019-06-29T16:43:45.337684: step 4227, loss 0.0300144, acc 0.984375\n",
      "2019-06-29T16:43:45.621888: step 4228, loss 0.0135468, acc 1\n",
      "2019-06-29T16:43:45.893362: step 4229, loss 0.00250861, acc 1\n",
      "2019-06-29T16:43:46.207586: step 4230, loss 0.000890063, acc 1\n",
      "2019-06-29T16:43:46.494792: step 4231, loss 0.0163167, acc 1\n",
      "2019-06-29T16:43:46.798007: step 4232, loss 0.00781225, acc 1\n",
      "2019-06-29T16:43:47.112231: step 4233, loss 0.011228, acc 1\n",
      "2019-06-29T16:43:47.426454: step 4234, loss 0.013228, acc 1\n",
      "2019-06-29T16:43:47.713071: step 4235, loss 0.031635, acc 0.984375\n",
      "2019-06-29T16:43:48.003276: step 4236, loss 0.000290535, acc 1\n",
      "2019-06-29T16:43:48.286478: step 4237, loss 0.00479867, acc 1\n",
      "2019-06-29T16:43:48.594699: step 4238, loss 0.00350753, acc 1\n",
      "2019-06-29T16:43:48.879901: step 4239, loss 0.0307225, acc 0.984375\n",
      "2019-06-29T16:43:49.177112: step 4240, loss 0.00279246, acc 1\n",
      "2019-06-29T16:43:49.489337: step 4241, loss 0.0137362, acc 0.984375\n",
      "2019-06-29T16:43:49.822572: step 4242, loss 0.00194231, acc 1\n",
      "2019-06-29T16:43:50.107775: step 4243, loss 0.00901807, acc 1\n",
      "2019-06-29T16:43:50.403986: step 4244, loss 0.0318999, acc 0.984375\n",
      "2019-06-29T16:43:50.701198: step 4245, loss 0.00203792, acc 1\n",
      "2019-06-29T16:43:50.986400: step 4246, loss 0.00203598, acc 1\n",
      "2019-06-29T16:43:51.277609: step 4247, loss 0.00314212, acc 1\n",
      "2019-06-29T16:43:51.572819: step 4248, loss 0.000757154, acc 1\n",
      "2019-06-29T16:43:51.874032: step 4249, loss 0.00684514, acc 1\n",
      "2019-06-29T16:43:52.164239: step 4250, loss 0.00211132, acc 1\n",
      "2019-06-29T16:43:52.470457: step 4251, loss 0.0368416, acc 0.984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-29T16:43:52.770671: step 4252, loss 0.00312327, acc 1\n",
      "2019-06-29T16:43:53.132930: step 4253, loss 0.0084107, acc 1\n",
      "2019-06-29T16:43:54.052585: step 4254, loss 0.00158817, acc 1\n",
      "2019-06-29T16:43:54.423848: step 4255, loss 0.00934736, acc 1\n",
      "2019-06-29T16:43:54.723061: step 4256, loss 0.00167259, acc 1\n",
      "2019-06-29T16:43:55.005262: step 4257, loss 0.00230418, acc 1\n",
      "2019-06-29T16:43:55.287464: step 4258, loss 0.00204756, acc 1\n",
      "2019-06-29T16:43:55.574668: step 4259, loss 0.000723854, acc 1\n",
      "2019-06-29T16:43:55.855868: step 4260, loss 0.0303083, acc 0.984375\n",
      "2019-06-29T16:43:56.152079: step 4261, loss 0.00221511, acc 1\n",
      "2019-06-29T16:43:56.670449: step 4262, loss 0.00190501, acc 1\n",
      "2019-06-29T16:43:56.971662: step 4263, loss 0.00168531, acc 1\n",
      "2019-06-29T16:43:57.258866: step 4264, loss 0.00100231, acc 1\n",
      "2019-06-29T16:43:57.554076: step 4265, loss 0.00318897, acc 1\n",
      "2019-06-29T16:43:57.840281: step 4266, loss 0.00366666, acc 1\n",
      "2019-06-29T16:43:58.121480: step 4267, loss 0.0117894, acc 1\n",
      "2019-06-29T16:43:58.413689: step 4268, loss 0.00100721, acc 1\n",
      "2019-06-29T16:43:58.714903: step 4269, loss 0.0186907, acc 0.984375\n",
      "2019-06-29T16:43:59.005110: step 4270, loss 0.00217684, acc 1\n",
      "2019-06-29T16:43:59.291314: step 4271, loss 0.00577764, acc 1\n",
      "2019-06-29T16:43:59.581520: step 4272, loss 0.00367106, acc 1\n",
      "2019-06-29T16:43:59.886737: step 4273, loss 0.0024962, acc 1\n",
      "2019-06-29T16:44:00.178975: step 4274, loss 0.0152664, acc 0.984375\n",
      "2019-06-29T16:44:00.467179: step 4275, loss 0.00343323, acc 1\n",
      "2019-06-29T16:44:00.753384: step 4276, loss 0.00227554, acc 1\n",
      "2019-06-29T16:44:01.053596: step 4277, loss 0.00152688, acc 1\n",
      "2019-06-29T16:44:01.338799: step 4278, loss 0.00428717, acc 1\n",
      "2019-06-29T16:44:01.631007: step 4279, loss 0.000736964, acc 1\n",
      "2019-06-29T16:44:01.923984: step 4280, loss 0.00155775, acc 1\n",
      "2019-06-29T16:44:02.213697: step 4281, loss 0.0348257, acc 0.984375\n",
      "2019-06-29T16:44:02.512910: step 4282, loss 0.000367959, acc 1\n",
      "2019-06-29T16:44:02.828135: step 4283, loss 0.00333149, acc 1\n",
      "2019-06-29T16:44:03.167375: step 4284, loss 0.00288979, acc 1\n",
      "2019-06-29T16:44:03.487708: step 4285, loss 0.00266414, acc 1\n",
      "2019-06-29T16:44:03.814566: step 4286, loss 0.00364879, acc 1\n",
      "2019-06-29T16:44:04.134794: step 4287, loss 0.00247387, acc 1\n",
      "2019-06-29T16:44:04.444015: step 4288, loss 0.00103022, acc 1\n",
      "2019-06-29T16:44:04.752233: step 4289, loss 0.0026491, acc 1\n",
      "2019-06-29T16:44:05.070460: step 4290, loss 0.00492334, acc 1\n",
      "2019-06-29T16:44:05.402697: step 4291, loss 0.00464968, acc 1\n",
      "2019-06-29T16:44:05.707913: step 4292, loss 0.00232515, acc 1\n",
      "2019-06-29T16:44:06.015132: step 4293, loss 0.00424292, acc 1\n",
      "2019-06-29T16:44:06.305340: step 4294, loss 0.0189263, acc 1\n",
      "2019-06-29T16:44:06.609556: step 4295, loss 0.0103182, acc 1\n",
      "2019-06-29T16:44:06.897761: step 4296, loss 0.00171654, acc 1\n",
      "2019-06-29T16:44:07.298047: step 4297, loss 0.00218543, acc 1\n",
      "2019-06-29T16:44:07.820418: step 4298, loss 0.0114018, acc 1\n",
      "2019-06-29T16:44:08.129638: step 4299, loss 0.0110323, acc 1\n",
      "2019-06-29T16:44:08.424848: step 4300, loss 0.00189805, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-06-29T16:44:09.243433: step 4300, loss 1.31826, acc 0.727955\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\mao\\Desktop\\Tensorflow\\CNN\\1\\cnn-text-classification-tf-master\\runs\\1561796526\\checkpoints\\model-4300\n",
      "\n",
      "2019-06-29T16:44:11.036708: step 4301, loss 0.00108669, acc 1\n",
      "2019-06-29T16:44:11.458007: step 4302, loss 0.00581495, acc 1\n",
      "2019-06-29T16:44:11.727200: step 4303, loss 0.0182463, acc 0.984375\n",
      "2019-06-29T16:44:11.997392: step 4304, loss 0.00719014, acc 1\n",
      "2019-06-29T16:44:12.264582: step 4305, loss 0.00511344, acc 1\n",
      "2019-06-29T16:44:12.554789: step 4306, loss 0.00269244, acc 1\n",
      "2019-06-29T16:44:12.825982: step 4307, loss 0.0163192, acc 1\n",
      "2019-06-29T16:44:13.105180: step 4308, loss 0.00264887, acc 1\n",
      "2019-06-29T16:44:13.374372: step 4309, loss 0.00210489, acc 1\n"
     ]
    }
   ],
   "source": [
    "#! /usr/bin/env python\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import data_helpers\n",
    "from text_cnn import TextCNN\n",
    "from tensorflow.contrib import learn\n",
    "\n",
    "# Parameters\n",
    "# ==================================================\n",
    "\n",
    "# Data loading params\n",
    "tf.flags.DEFINE_float(\"dev_sample_percentage\", .1, \"Percentage of the training data to use for validation\")\n",
    "tf.flags.DEFINE_string(\"positive_data_file\", \"./data/rt-polaritydata/rt-polarity.pos\", \"Data source for the positive data.\")\n",
    "tf.flags.DEFINE_string(\"negative_data_file\", \"./data/rt-polaritydata/rt-polarity.neg\", \"Data source for the negative data.\")\n",
    "\n",
    "# Model Hyperparameters\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", 128, \"Dimensionality of character embedding (default: 128)\")\n",
    "tf.flags.DEFINE_string(\"filter_sizes\", \"3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "tf.flags.DEFINE_integer(\"num_filters\", 128, \"Number of filters per filter size (default: 128)\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.0, \"L2 regularization lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 64, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 200, \"Number of training epochs (default: 200)\")\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "# FLAGS._parse_flags()\n",
    "# print(\"\\nParameters:\")\n",
    "# for attr, value in sorted(FLAGS.__flags.items()):\n",
    "#     print(\"{}={}\".format(attr.upper(), value))\n",
    "# print(\"\")\n",
    "\n",
    "def preprocess():\n",
    "    # Data Preparation\n",
    "    # ==================================================\n",
    "\n",
    "    # Load data\n",
    "    print(\"Loading data...\")\n",
    "    x_text, y = data_helpers.load_data_and_labels(FLAGS.positive_data_file, FLAGS.negative_data_file)\n",
    "\n",
    "    # Build vocabulary\n",
    "    max_document_length = max([len(x.split(\" \")) for x in x_text])\n",
    "    vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "    x = np.array(list(vocab_processor.fit_transform(x_text)))\n",
    "\n",
    "    # Randomly shuffle data\n",
    "    np.random.seed(10)\n",
    "    shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "    x_shuffled = x[shuffle_indices]\n",
    "    y_shuffled = y[shuffle_indices]\n",
    "\n",
    "    # Split train/test set\n",
    "    # TODO: This is very crude, should use cross-validation\n",
    "    dev_sample_index = -1 * int(FLAGS.dev_sample_percentage * float(len(y)))\n",
    "    x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "    y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
    "\n",
    "    del x, y, x_shuffled, y_shuffled\n",
    "\n",
    "    print(\"Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))\n",
    "    print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))\n",
    "    return x_train, y_train, vocab_processor, x_dev, y_dev\n",
    "\n",
    "def train(x_train, y_train, vocab_processor, x_dev, y_dev):\n",
    "    # Training\n",
    "    # ==================================================\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        session_conf = tf.ConfigProto(\n",
    "          allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "          log_device_placement=FLAGS.log_device_placement)\n",
    "        sess = tf.Session(config=session_conf)\n",
    "        with sess.as_default():\n",
    "            cnn = TextCNN(\n",
    "                sequence_length=x_train.shape[1],\n",
    "                num_classes=y_train.shape[1],\n",
    "                vocab_size=len(vocab_processor.vocabulary_),\n",
    "                embedding_size=FLAGS.embedding_dim,\n",
    "                filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "                num_filters=FLAGS.num_filters,\n",
    "                l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
    "\n",
    "            # Define Training procedure\n",
    "            global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "            optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "            grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "            train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "            # Keep track of gradient values and sparsity (optional)\n",
    "            grad_summaries = []\n",
    "            for g, v in grads_and_vars:\n",
    "                if g is not None:\n",
    "                    grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                    sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                    grad_summaries.append(grad_hist_summary)\n",
    "                    grad_summaries.append(sparsity_summary)\n",
    "            grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "            # Output directory for models and summaries\n",
    "            timestamp = str(int(time.time()))\n",
    "            out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "            print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "            # Summaries for loss and accuracy\n",
    "            loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "            acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "            # Train Summaries\n",
    "            train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "            train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "            train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "            # Dev summaries\n",
    "            dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "            dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "            dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "            # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "            checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "            checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "            if not os.path.exists(checkpoint_dir):\n",
    "                os.makedirs(checkpoint_dir)\n",
    "            saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
    "\n",
    "            # Write vocabulary\n",
    "            vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "            # Initialize all variables\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            def train_step(x_batch, y_batch):\n",
    "                \"\"\"\n",
    "                A single training step\n",
    "                \"\"\"\n",
    "                feed_dict = {\n",
    "                  cnn.input_x: x_batch,\n",
    "                  cnn.input_y: y_batch,\n",
    "                  cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
    "                }\n",
    "                _, step, summaries, loss, accuracy = sess.run(\n",
    "                    [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                    feed_dict)\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "                train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "            def dev_step(x_batch, y_batch, writer=None):\n",
    "                \"\"\"\n",
    "                Evaluates model on a dev set\n",
    "                \"\"\"\n",
    "                feed_dict = {\n",
    "                  cnn.input_x: x_batch,\n",
    "                  cnn.input_y: y_batch,\n",
    "                  cnn.dropout_keep_prob: 1.0\n",
    "                }\n",
    "                step, summaries, loss, accuracy = sess.run(\n",
    "                    [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "                    feed_dict)\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "                if writer:\n",
    "                    writer.add_summary(summaries, step)\n",
    "\n",
    "            # Generate batches\n",
    "            batches = data_helpers.batch_iter(\n",
    "                list(zip(x_train, y_train)), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "            # Training loop. For each batch...\n",
    "            for batch in batches:\n",
    "                x_batch, y_batch = zip(*batch)\n",
    "                train_step(x_batch, y_batch)\n",
    "                current_step = tf.train.global_step(sess, global_step)\n",
    "                if current_step % FLAGS.evaluate_every == 0:\n",
    "                    print(\"\\nEvaluation:\")\n",
    "                    dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "                    print(\"\")\n",
    "                if current_step % FLAGS.checkpoint_every == 0:\n",
    "                    path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                    print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "\n",
    "def main(argv=None):\n",
    "    x_train, y_train, vocab_processor, x_dev, y_dev = preprocess()\n",
    "    train(x_train, y_train, vocab_processor, x_dev, y_dev)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
